The Use of Binary Choice Forests to Model and Estimate Discrete Choices Ningyuan Chen*1, Guillermo Gallegoâ€ 2, and Zhuodong Tangâ€¡2 1Department of Management, University of Toronto Mississauga 2Department of Industrial Engineering & Decision Analytics Hong Kong University of Science and Technology Abstract We show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is conrmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets. *ningyuan.chen@utoronto.ca â€ ggallego@ust.hkâ€¡ztangai@connect.ust.hk 1  Electronic copy available at: https://ssrn.com/abstract=3430886 1. Introduction Being able to understand consumersâ€™ choice behavior when they are oered an assortment of products provides rms with unique advantages. It is particularly important in the modern era: online retailers that predict consumersâ€™ choice behavior more accurately can implement more eective strategies and earn higher prots. In turn, they can aord to invest in advanced technologies and infrastructure, and sharpen their prediction of consumersâ€™ behavior. The unstoppable cycle has created a few unprecedented market juggernauts such as Amazon. Firms unwilling or incapable of getting inside the mind of their consumers are left behind. Not surprisingly, discrete choice models (DCM) have become one of the central topics in revenue management and pricing analytics. To understand and predict consumersâ€™ choice behavior, academics and practitioners have proposed several frameworks, some of which are widely adopted in industry, One ubiquitous framework is model-then-estimate. In this framework, a parametric DCM is proposed to explain how a customer chooses a product when oered an assortment. The parameters are then estimated using historical data. Once the model has been estimated properly, it can then be used as a workhorse to predict the choice behavior of future consumers. In the model-then-estimate framework, there is a trade-o between the exibility and accuracy. A exible DCM incorporates a wide range of patterns of consumersâ€™ behavior, but it may be dicult to estimate and may overt training data. A parsimonious model, may fail to capture consumers behavior, and even if estimated correctly it would be misspecied. The goal is to reach a delicate balance between exibility and predictability even relative to assortments never seen before. Not surprisingly, it is not straightforward to nd the â€œsweet spotâ€ when selecting among the large class of parametric DCMs. Another framework favored by data scientists is estimate-without-models. Advanced machine learning algorithms are applied to historical sales data, and used to predict future choice behavior. The framework skips â€œmodelingâ€ entirely and does not attempt to understand the rationality (or irrationality) hidden behind the patterns observed in the training data. With engineering tweaks, the algorithms can be implemented eciently and capture a wide range of choice behavior. For example, neural networks are known to be able to approximate any continuous functions. This approach 2  Electronic copy available at: https://ssrn.com/abstract=3430886 may sound appealing: if the algorithm achieves impressive accuracy when predicting the choice behavior of new consumers, why do we care about the actual rationale behind consumers when they make choices? There are two reasons to care. First, the rm may be interested in not only making accurate predictions, but also other goals such as nding the optimal assortment that maximizes the expected revenue. Without a proper model, it is unclear if the goal can be formulated as an optimization problem. Second, when the market environment or customer preferences change systematically over time, having a reasonable model provide a certain degree of generalizability while black-box algorithms may fail to capture an obvious pattern just because the pattern has not appeared frequently in the past. In this paper, we introduce a data-driven framework which we call estimate-andmodel that combines machine learning with DCMs, and thus retains the strengths of both frameworks mentioned previously. The model we propose, binary choice forests, is a mixture of binary trees, each of which mimics the internal decision-making process of a customer. We show that the binary choice forest can be used to approximate any DCM, and is thus suciently exible, but still identiable with training data of reasonable size. Moreover, it can be eciently estimated using random forests (Breiman, 2001), a popular machine learning technique that has stood the test of time. Random forests are easy to implement using R or Python (Pedregosa et al., 2011; Liaw and Wiener, 2002) and have been shown to have extraordinary predictive power in practice. We provide theoretical guarantees: as the sample size increases, random forests can successfully recover the binary choice forest, and thus any DCM. Moreover, the splitting criterion used by the random forests is intrinsically connected to the preference rank list of customers. As a contribution to the literature, the framework we propose has the following practical advantages: â€¢It can capture various patterns of customer behavior that cannot be easily captured by other models, such as irregularity and sequential searches (Weitzman, 1979). See Section 5.1 for more details. â€¢It can deal with nonstandard formats of historical data, which is a major challenge in practice. See Section 5.2 for more details. â€¢It can return an importance index for all products, based on how frequently 3  Electronic copy available at: https://ssrn.com/abstract=3430886 a random customer would make decisions depending on the presence of the product. â€¢It can incorporate the prices of the products and reect the information in the decision-making of consumers. â€¢It can naturally incorporate customer features and is compatible with personalized online retailing. 1.1. LiteratureReview We rst review DCMs proposed in the literature following the model-then-estimate framework, in the order of increasing exibility and diculty in terms of estimation. The independent demand model and the MNL model (McFadden, 1973) have very few parameters (one per product), which are easy to estimate (Train, 2009). Although the MNL model is still widely used, its inherent property of independence of irrelevant alternatives (IIA) has been criticized for being unrealistic (see Anderson et al. (1992) for more details). The mixed logit model, the nested logit model, the Markov chain DCM, and the rank-based DCM (see, e.g., Williams (1977); Train (2009); Farias et al. (2013); Blanchet et al. (2016)) are able to capture much more complex choice behavior than the MNL model. In fact, the mixed logit model and the rank-based DCM can approximate any random utility model (RUM), encompassing a very general class of DCMs. The estimation of these models is challenging, but there has been exciting progress in recent years (Farias et al., 2013; van Ryzin and Vulcano, 2014, 2017; Simsek and Topaloglu, 2018; Jagabathula et al., 2019). However, the computational feasibility and the susceptibility to overtting remain a challenge in practice. Even the general class of RUM cannot capture certain choice behavioral. A RUM possesses the so-called regularity property: the probability of choosing an alternative cannot increase if the oered set is enlarged. There are a few experimental studies showing strong evidence that regularity may be violated (Simonson and Tversky, 1992). Several models are proposed to capture even more general behavior than RUM (Natarajan et al., 2009; Flores et al., 2017; Berbeglia, 2019; Feng et al., 2017). It is unclear if the estimation for such models can be done eciently. The specications of random forests used in this paper are introduced by Breiman 4  Electronic copy available at: https://ssrn.com/abstract=3430886 (2001), although many of the ideas were discovered even earlier. The readers may refer to Hastie et al. (2009) for a general introduction. Although random forests have been very successful in practice, little is known about their theoretical properties. To date, most studies are focused on isolated setups or simplied versions of the procedure. In a recent study, Scornet et al. (2015) establish the consistency of random forests in regression problems, under less restrictive assumptions. Biau and Scornet (2016) provide an excellent survey of the recent theoretical and methodological developments in the eld. A recent paper by Chen and MiÅ¡ic (2019) proposes a similar tree-based DCM. They show that their â€œdecision forestâ€ can approximate any DCMs with arbitrary precision; a similar result is proved with a dierent approach in this paper. Our studies dier substantially in the estimation step: we focus on random forests, while Chen and MiÅ¡ic (2019) follow an optimization approach based on column generation ideas for estimation. Moreover, we establish the consistency of random forests, and show that the estimation can accommodate the price information and aggregate choice data. In our numerical study, we nd that random forests are quite robust and have a good performance even compared with the Markov chain model estimated using the expectation-maximization (EM) algorithm, which has been shown to have outstanding empirical performance compared to MNL, the nested logit, the mixed logit and rank-based DCM (Berbeglia et al., 2018), especially when the training data is large. Our algorithm runs 17 times faster than the EM algorithm. In contrast, the computational study in Chen and MiÅ¡ic (2019) is limited to the rank-based model estimated by column generation (van Ryzin and Vulcano, 2014), which is shown to be outperformed by the Markov chain model (Berbeglia et al., 2018). 2. ChoiceModelsandMixtureofBinaryTrees Consider a set Â»N ] , f1;:::, N gof N products and dene Â»N Â¼+ , Â»N Â¼[f0gwhere 0 represents the no-purchase option. Let x 2f0, 1gN be a binary vector representing an assortment of products, where xÂ¹i) = 1 indicates product i is in the assortment and xÂ¹iÂº= 0 otherwise. A discrete choice model (DCM) is a non-negative mapping 5  Electronic copy available at: https://ssrn.com/abstract=3430886 Product 2 S0 S1 S2 YN 0 1 Product 1 xÂ¹1Âº0:5 1 xÂ¹2Âº0:5 2 0 Figure 1: A binary tree representation of the partition. pÂ¹i, xÂº, Â»N Â¼+ f0, 1gN 7!Â»0, 1Â¼such that Ã• pÂ¹i, xÂº= 1, pÂ¹i, xÂº= 0 if xÂ¹iÂº= 0. i2Â»N Â¼+ It is clear that pÂ¹i, xÂºrepresents the probability of a random customer choosing product i when presented the assortment x. We refer to a subset S of Â»N Â¼as an assortment associated with x 2f0, 1gN , i.e., i 2S if and only if xÂ¹iÂº= 1. Without ambiguity, we will write pÂ¹i, SÂºinstead of pÂ¹i, xÂº. A binary decision tree tÂ¹xÂºmaps x 2f0, 1gN into Â»N Â¼+. More precisely, it species a partition of the space f0, 1gN , fSi , i 2Â»N Â¼+g, and assigns label i 2Â»N Â¼+ to region Si , so tÂ¹x) = Ã i Ifx2Sig. Some of the regions in the partition may be empty. We i2Â»N Â¼+ refer to the partition as a binary decision tree because any partition of f0, 1gN can be obtained by sequentially splitting the space along N dimensions. For example, a decision tree representation of a partition when N = 2 is demonstrated in Figure 1. A binary decision forest is dened as a convex combination of multiple binary decision trees. More precisely, a binary decision forest can be written as BÃ• f Â¹i, xÂº= wb IftbÂ¹xÂº=igb=1 where the tbÂ¹x) and wb are, respectively decision trees, and non-negative weights 6  Electronic copy available at: https://ssrn.com/abstract=3430886 summing up to one. Notice that a decision forest maps Â»N Â¼+ f0, 1gN 7!Â»0, 1Â¼just like DCMs do. Yet decision forest are not necessarily DCMs because tb Â¹xÂºmay be equal to i even if xÂ¹iÂº= 0. A binary decision tree tÂ¹xÂºis a binary choice tree if tÂ¹x) = i only if xÂ¹iÂº= 1. A binary decision forest is a binary choice forest (BCF) if it is a convex combination of binary choice trees. A BCF can be interpreted as decisions made by B consumer types, with consumers of type b having weight wb and making decisions based on binary choice tree tbÂ¹xÂº. If f Â¹i, xÂºis a BCF, then f is also a DCM. This is because f is non-negative, Ã i2Â»N Â¼+ f Â¹i, xÂº= 1 and f Â¹i, xÂº= 0 if xÂ¹iÂº= 0. To see that the converse is also true, we will rst show that DCMs are closed under convex combinations and that any DCM is in the convex hull of extreme DCMs. We next argue that the extreme DCMs are the deterministic DCMs that assign S to a particular choice iÂ¹SÂº2S+ with probability one for every S Â»N Â¼. The next step is to show that each extreme DCM can be represented by a binary choice tree concluding that every DCM is a convex combination of choice trees and is thus a BCF. Theorem1.Every BCF is a DCM, and every DCM can be represented as a BCF. One way to interpret this result is that for each DCM there exists a set of weights we, e 2E adding to one, such that pÂ¹i, SÂº= Ã e2E wepe Â¹i, SÂºfor all i 2S+, S âŠ‚ N , where the pe â€™s are the extreme deterministic DCMs. Although we can represent every DCM as a BCF, it will be dicult to estimate if we have too many extreme points. The number of extreme points is ÃŽN =1Â¹k + 1ÂºÂ¹NkÂºfork N products, which increases tremendously as N increases, with more than 6:7 10173 extreme points for N = 8. In the next theorem, we will show that any DCM can be represented as a convex combination of much fewer binary choice trees. Theorem2.Every DCM can be represented as a convex combination of a BCF containing 2N ô€€€1at most N Â· + 1 trees. Proof. CarathÃ©odoryâ€™s theorem states that if a point x of Rd lies in the convex hull of a set P, then x can be written as the convex combination of at most d + 1 points in P. To apply CarathÃ©odoryâ€™s theorem to DCM, notice that since the choice probabilities sumÃN ô€€€N to 1, each assortment with cardinality k has dimension of k. We have d == k=1 kkÃNN ! ÃN Â¹N ô€€€1Âº!2N ô€€€1 k=1 k Â· k!Â¹N ô€€€kÂº! = N Â· k=1 Â¹kô€€€1Âº!Â¹N ô€€€kÂº! = N Â· . Therefore, Every DCM can be represented as a convex combination of a BCF containing at most N 2N ô€€€1 + 1. 7  Electronic copy available at: https://ssrn.com/abstract=3430886 As an example, for N = 8, d = 1024, so any DCM with N = 8 can be represented by a convex combination of 1025 trees. A recent working paper by Chen and MiÅ¡ic (2019) has independently shown, by construction, that any choice model can be represented by a decision forest where each of the trees has depth N + 1. While their proof has the virtue of being constructive, our proof is more succinct and insightful as it shows that DCMs and BCFs are equivalent, and the existence of a solution of much lower dimension. Our result implies that choice forests are capable of explaining some of the pathological cases that do not exhibit regularity and are outside the RUM, including the decoy eect (Ariely, 2008) and the comparison-based choice (Huber et al., 1982). Note also that all RUMs can be modelled as convex combinations of permutation lists, which are special cases of decision trees. 3. DataandEstimation The main goal of this paper is to provide a practical method to estimate DCMs using random forests, which are shown to be able to approximate all BCFs. The numerical recipe for random forests is widely available and implementable. Before proceeding we remark that an alternative approach would be to use column generation starting with a collection of trees and adding additional trees to improve the t to data. This approach has been taken, for example by van Ryzin and Vulcano (2014); MiÅ¡ic (2016); Jagabathula and Rusmevichientong (2016) to estimate RUMs by weighted preference rank lists, and a similar approach has been pursued by Chen and MiÅ¡ic (2019) for trees. We remark that the output of our model can be fed into a column generation algorithm to seek further improvements although we have not pursued this in our paper. We will assume that arriving consumers make selections independently based on an unknown DCM pÂ¹i, xÂº, and that a rm collects data of the form Â¹it , xt Âº(or equivalently Â¹it , St Âº) where xt was the assortment oered to the tth consumer and it 2St [f0gis the choice made by consumer t = 1;:::;T . Our goal is to use the data to construct a family of binary choice trees as a means to estimate the underlying DCM pÂ¹i, xÂºrepresented by a BCF. We view the problem as a classication problem: given the predictor x, we would like to provide a classier that maps the predictor to a class label i 2Â»N Â¼+, or the class probabilities. To this end we will use a random forest as a classier. The output of a random forest 8  Electronic copy available at: https://ssrn.com/abstract=3430886 is B individual binary decision trees (CART), ftbÂ¹xÂºgB =1, where B is a tunable parameter. b Although a single tree only outputs a class label in each region, the aggregation of the trees, i.e., the forest, is naturally equipped with the class probabilities. Then the choice probability of item i in the assortment x is estimated as BÃ• 1 IftbÂ¹xÂº=ig, (1)B b=1 which is a special form of BCF. The next result shows that the random forest can still approximate any DCM. Theorem3.If B is suciently large, then a binary choice forest of the form BÃ• 1 f Â¹i, xÂº= IftbÂ¹xÂº=ig;B b=1 can approximate any DCM. The implication of this result is that we donâ€™t have to worry about generating all of the extreme points, or deterministic DCMs, and then nding a set of weights wb for each such tree tb Â¹xÂº. Intuitively, if B is suciently large, then we need approximately Bwb type b customers associated with tree tb with positive weight wb > 0 in the convex combination. We explain how the random forest can be estimated from the historical data by rst reviewing the basic mechanism of CART which preforms recursive binary splitting of the predictor space Â»0, 1Â¼N . In each iteration, it selects a dimension i 2Â»N Â¼and a split point to split the predictor space. More precisely, the split Â¹i, si Âºdivides the observations to fÂ¹it , xt Âº: xt Â¹iÂºsi gand fÂ¹it , xt Âº: xt Â¹iÂº> si g. In our problem, because xt 2f0, 1gN is at the corner of the hypercube, all split points between 0 and 1 create the same partition of the observations and thus we simply set si 0:5. To select the dimension, usually an empirical criterion is optimized to favor splits that create â€œpurerâ€ regions. That is, the resulting region should contain data points that mostly belong to the same class. WeÃ tjÃNuse a common measure called Gini index: =0 pË†jk Â¹1 ô€€€pË†jk Âºwhere tj is the number RjTk of observations in region Rj of the partition and pË†jk is the empirical frequency of class k in Rj . It is not hard to see that the Gini index takes smaller values when the regions 9  Electronic copy available at: https://ssrn.com/abstract=3430886 contain predominantly observations from a single class. In this case, a dimension is selected that minimizes the measures and the partition is further rened by a binary split. This splitting operation is conducted recursively for the regions in the resulting partition until a stopping rule is met. The main drawback of CART is its tendency to overtting the training data. If a deep decision tree is built (having a large number of splits), then it may t the training data well but introduce large variances when applied to test data. If the tree is pruned and only has a few leaves (or regions in the predictor space), then it loses the predictive accuracy. Random forests, by creating a number of decision trees and then aggregating them, signicantly improve the power of single trees and moves the bias-variance tradeo toward the favorable direction. The basically idea behind random forests is to â€œshakeâ€ the original training data in various ways in order to create decision trees that are as uncorrelated as possible. Because the decision trees are deliberately â€œdecorrelatedâ€, they can aord to be deep, as the large variances are remedied by aggregating the â€œalmost independentâ€ trees. Next we explain the details of random forests. To create B randomized trees, for each b = 1;:::, B, we randomly choose z samples with replacement from the T observations (a bootstrap sample). Only the sub-sample of z observations is used to train the bth decision tree. Splits are performed only on a random subset of Â»N Â¼of size m according to one of the criterion of Gini index. The random sub-sample of training data and random directions to split are two key ingredients in creating less correlated decision trees in the random forest. The depth of the tree is controlled by the minimal number of observations, say l, in a region for the tree to keep splitting. These ideas are subsumed in Algorithm 1. We rst remark on the procedure in Algorithm 1 that can be applied to a generic classication problem and then comment on the special properties in our problem. (1) Many machine learning algorithms such as neural networks have numerous parameters to tune and the performance crucially depends on a suitable choice of parameters. Random forests, on the other hand, have only a few interpretable parameters. Even so, in the numerical studies in this paper, we simply choose a set of parameters that are commonly used for classication problems, without cross-validation or tuning, in order to demonstrate the robustness of the palgorithm. In particularly, we mostly use z = T , m = N and l = 50. There are other alternative options when constructing random forests, such as using a bootstrap 10  Electronic copy available at: https://ssrn.com/abstract=3430886 Algorithm 1 Random forests for DCM estimation 1: Data: fÂ¹it , xt ÂºgT t=1 2: Tunable parameters: number of trees B, sub-sample size z 2f1;:::;T g, number of dimensions to split m 2f1;:::, N g, terminal leaf size l 2f1;:::, zg3: for b = 1 to B do 4: Select z observations from the training data with replacement, denoted by Z 5: Initialize the tree tb Â¹xÂº0 with a single root node 6: while some leaf has greater than or equal to l observations belonging to Z and can be split do 7: Select m variables without replacement among f1;:::, N g8: Select the optimal one to split among the m dimensions that minimizes the Gini index 9: Split the leaf node into two 10: end while 11: Denote the partition corresponding to the leaves of the tree by fR1;:::, RM g; let ci be the class label of a randomly chosen observation in RiÃM12: Dene tb Â¹xÂº= i=1 ci Ifx2Rig13: end for 14: The trees ftbÂ¹ÂºgbB =1 are used to estimate the class probabilities as (1) sample Step 4. For the ease of exposition, we stick to the canonical version presented in Algorithm 1. (2) The numerical recipe for the algorithm is implemented in many programming languages such as R and Python and ready to use. In Section B, we provide a demonstration using scikit-learn, a popular machine learning package in Python that implements random forests, to estimate customer choice. As one can see, it takes less than 20 lines to implement the procedure. Because of the structure of the problem, there are three specic observations. (1) Because the entries of x are binary f0, 1g, the split position of decision trees is always 0:5. Therefore, along a branch of a decision tree, there can be at most one split on a particular dimension, and the depth of a decision tree is at most N . (2) The random forest is a binary decision forest instead of a BCF. In particular, the probability of class i, or the choice probability of product i given assortment x, may be positive even when xÂ¹iÂº= 0, i.e., product i is not included in the assortment. To x the issue, we adjust the 11  Electronic copy available at: https://ssrn.com/abstract=3430886 probability of class i by conditioning on the trees that output reasonable class labels: BÃ• ÃÃ1 B IftbÂ¹xÂº=i;xÂ¹iÂº=1gb=1 j:xÂ¹jÂº=1 b=1 IftbÂ¹xÂº=j} (3) When returning the class label of a leaf note in a decision tree, we use a randomly chosen observation instead of taking a majority vote (Step 11 in Algorithm 1). While not being a typical choice, it seems crucial in deriving our consistency result (Theorem 4). Intuitively, unlike other classication problems in which the predictor has a continuous support, in our problem xt are overlapping when an assortment is oered to multiple consumers in the data. A majority vote would favor the choice of product that most consumers make and ignore less attractive products. To correctly recover the choice probability from the data, we randomly choose an observation in the leaf (equivalently, randomly pick a customer t in the data who has been oered the same assortment), which is at least an unbiased estimator for the choice probability. 4. WhyDoRandomForestsWorkWell? Many machine learning algorithms have superb performances in practice, while very few theories can be spelt out on why it is the case. For example, for random forests, even consistency, one of the most fundamental properties a statistician would demand for any classic estimators, was only established recently for regression problems under restrictive assumptions (Scornet et al., 2015). The lack of theoretical understandings can worry practitioners when stakes are high and the failure may have harmful consequences. In this section, we attempt to answer the â€œwhyâ€ question for our setting from two angles. We show that random forests are consistent for any DCM, and the way that random forests split (Gini index) can naturally help to recover the choice model when it can be represented by a tree. 4.1. RandomForestsareConsistentforAnyChoiceModel We now show that with enough data, random forests can recover the choice probability of any DCM. To obtain our theoretical results, we impose mild assumptions on how the data is generated. 12  Electronic copy available at: https://ssrn.com/abstract=3430886 Assumption 1. There is an underlying ground truth DCM from which all T consumers independently make selections from the oered assortments, generating data Â¹it , St Âº, t = 1;::. T . Note that the assumption only requires consumers to make choices independently. On the other hand, we focus on a xed-design experiment, and the sequence of assortment oered xt can be arbitrary. This is dierent from most consistency results of random forests in which random design is used (see (Biau and Scornet, 2016) for references), i.e., xt are i.i.d. In our setting, the assortment is unlikely to be generated randomly, but chosen by the rm, either to maximize the revenue or explore customer preferences by A/B testing. Therefore, a xed design probably reects the reality more than a random design. Since the consistency result requires the sample size T !1, we use the subscript T to emphasize the fact that the parameters may be chosen based on T . For a given ÃTassortment x, let kT Â¹xÂº, t=1 Ifxt=x } be the number of consumers who see assortment x. We are now ready to establish the consistency of random forests. Theorem4.Suppose Assumption 1 holds, then for any x and i, if limT !1kT Â¹xÂºÂT > 0, lT is xed, zT !1, BT !1, then the random forest is consistent: !Ã• lim P BT 1 IftbÂ¹xÂº=igô€€€pÂ¹i, x) >  = 0 T !âˆž BTb=1 for all  > 0. According to Theorem 4, the random forest can accurately predict the choice probability of any DCM, given that the rm oers the assortment for many times. Practically, the result can guide us about the choice of parameters. In fact, we just need to generate many trees in the forest (BT !1), re-sample many observations in a decision tree (zT !1), and keep the terminal leaf small (lT is xed). The requirement is easily metpby the choice of parameters in the remarks following Algorithm 1, i.e., z = T , m = N and l = 50. Theorem 4 guarantees a good performance of the random forest when the seller has collected a large dataset. This is a typical case in online retailing, especially in the era of â€œbig dataâ€. Random forests thus provide a novel data-driven approach to model customer choices. In particular, the model is rst trained from data, and then used to interpret 13  Electronic copy available at: https://ssrn.com/abstract=3430886 the inherent thought process of consumers when they make purchases. By Theorem 4, when the historical data has a large sample size, the model can accurately predict how consumers make decisions in reality. This reects the universality of the model. In this section, we provide concrete examples demonstrating several practical considerations that can hardly be captured by other DCMs and handled well by random forests. 4.2. GiniIndexRecoverstheRankList In Section 2, we have shown that any DCM can be represented by a combination of binary decision trees. Moreover, through numerous experiments, we have found out that random forests perform particularly well when the data is generated by DCMs that can be represented by a few binary decision trees. In this section, we further explore this connection by studying a concrete setting where the DCM is represented by a single regular decision tree. Without loss of generality, we assume that customers always prefer product i to i + 1, for i = 1;:::, N ô€€€1, and product N to the no-purchase option. Equivalently, the DCM is a single rank list (the preferences of all customers form an ordered set). The following nite-sample result demonstrates that the rank list can be recovered from the random forest with high probability. Theorem5.Suppose the actual DCM is a rank list and the assortments in the training data are sampled uniformly. The random forest algorithm with sub-sample size z = T (without replacement), m = N , terminal leaf size l = 1 and B = 1 accurately predicts the choices of at least a fraction 1 ô€€€ of all 2N assortments with probability at least k   Ã• TT1 âˆ’ 13 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp ô€€€164 2iô€€€1 113 2iô€€€1 i=1 where k = dlog2 1 e. Since the bound scales exponentially in T , the predictive accuracy increases tremendously with size. The proof of the theorem reveals an intrinsic connection between the Gini index and the recovery of the rank list. Consider the rst split of the random forest in Step 8. We can show that, in expectation, if the rst split is on product i, then the resulting 14  Electronic copy available at: https://ssrn.com/abstract=3430886 Gini index is 21 1 22N ô€€€2 + OÂ¹1ÂT Âº:3 âˆ’ 3 22iô€€€2 âˆ’ 3 Â· In other words, if the data is generated without randomness (centered at the mean), then the rst split would occur on product 1 because of the ordering of the Gini index when T is large. Therefore, for the data points falling into the right branch of the rst split (having product 1 in the assortment), no more splits are needed as all customers would choose product 1 according to the rank list. Such a split correctly identies roughly half of the assortments. In the proof, we control the randomness by concentration inequalities, and conduct similar computations for the second, third splits and so on. The proof reveals the following insight into why random forests may work well in practice: The Gini index criterion tends to nd the products that are ranked high in the rank lists, because they create â€œpurerâ€ splits that lower Gini index. As a result, the topological structure of the decision trees trained in the random forest is likely to resemble that of the binary choice trees underlying the DCM generating the data. 5. FlexibilityandBenefitsofRandomForests In this section we demonstrate the exibility and benet of using random forests to estimate the choice forest. 5.1. BehavioralIssues Because of Theorem 3 and Theorem 4, random forests can be used to estimated any DCMs. For example, there is empirical evidence showing that behavioral considerations of consumers may distort their choice and thus violate regularity, e.g., the decoy eect (Ariely, 2008) and the comparison-based DCM (Huber et al., 1982; Russo and Dosher, 1983). It is already documented in Chen and MiÅ¡ic (2019) that the decision forest can capture the decoy eect. In this section, we use the choice forest to model consumer search. How consumers search to obtain new information when making purchases, is an important behavioral issue that is not monitored, or â€œunsupervisedâ€ in statistical terms, and hard to estimate by most models (for a few exceptions, see e.g. Wang and Sahin (2017)). Therefore, most DCMs abstract away those thought processes and only capture 15  Electronic copy available at: https://ssrn.com/abstract=3430886 the aggregate eect. Weitzman (1979) proposes a sequential search model with search costs. Prior to initiating the search consumers know only the distribution, say Vj of the net utility of product j 2Â»N Â¼and the cost cj to learn the realization of Vj . Let zj be the root of the equation EÂ»Â¹Vj ô€€€zj Âº+Â¼= cj and sort the products in descending order of zj . Weitzman shows that it is optimal to walk away without making any observations if the realized value of the no-purchase alternative, say W0 = V0 exceeds z1. Otherwise c1 is paid to observe V1 is observed and W1 = maxÂ¹V1;W0Âºis computed. The process stops if W1 exceeds z2 and continued otherwise, stopping the rst time, if ever, that Wi > zi+1. We next show that this search process can be represented by decision trees. Consider three products (N = 3). Suppose that the products are sorted so that z1 > z2 > z3 > 0, and that the valuations of an arriving customer satisfy v2 > v1 > v3. Hence the customer always searches in the order of product one !product two !product three. If in addition we suppose v2 > z3 > v1, then the decision tree can be illustrated in Figure 2. For example, suppose products one and tree are oered. The customer rst searches product one, because the reservation price of product one z1 is the highest. The realized valuation of product one is, however, not satisfactory (v1 < z3). Hence the customer keeps on searching the product with the second highest reservation price in the assortment, which is product three (product two is skipped because it is not in the assortment). However, the search process results in an even lower valuation of product three v3 < v1. As a result, the customer recalls and chooses product one. Clearly, a customer with dierent realized valuations would conduct a dierent search process, and leads to a dierent decision tree. 5.2. AggregatedChoiceData One of the most pressing practical challenges in data analytics is the quality of data. In Section 2, the historical data fÂ¹it , xt ÂºgT =1 is probably the most structured and granular t form of data one can hope to acquire. While most academic papers studying the estimation of DCMs assume this level of granularity, in practice it is frequent to see data in a more aggregate format. As an example, consider an airline oering three service classes E, T and Q of a ight where data is aggregated over a time window during which there may be changes to the assortment, and compiled from dierent sales channels. The company records information at certain time clicks as in Table 1. For each class, the 16  Electronic copy available at: https://ssrn.com/abstract=3430886 Has product 1 Has product 2 Choose 2 Has product 3 Choose 1 Choose 1 Has product 2 Choose 2 Has product 3 Choose 3 No purchase Y Y N Y N N Y N Y N Has product 1 Has product 2 Choose 2 Has product 3 Choose 1 Choose 1 Has product 2 Choose 2 Has product 3 Choose 3 No purchase Y Y N Y N N Y N Y N Figure 2: The sequential search process when N = 3 and the realized valuations and reservation prices satisfy v2 > v1 > v3, z1 > z2 > z3 > 0 and v2 > z3 > v1. Class Closure percentage # Booking E 20% 2 T 0%5 Q 90% 1 Table 1: A sample daily data of oered service classes and number of bookings. 17  Electronic copy available at: https://ssrn.com/abstract=3430886 closure percentage reects the fraction of time that the class is not open for booking, i.e., included in the assortment. Thus, 100% would imply that the corresponding class is not oered during that the time window. In a retail setting, this helps to deal with products that sell-out between review periods. The number of bookings for each class is also recorded. There may be various reasons behind the aggregation of data. The managers may not realize the value of high-quality data or are unwilling to invest in the infrastructure and human resources to reform the data collection process. One of the author has encountered this situation in practice with aggregate datasets as in Table 1. Fortunately, random forests can deal with aggregated choice data naturally, a feat that may be quite dicult to deal with with the column generation approach. Suppose the presented aggregated data has the form fÂ¹ps, bs ÂºgS =1, where ps 2Â»0, 1Â¼N denotess the closure percentage of the N products in day s, bs âˆˆ ZN +1 denotes the number of + bookings1, and the data spans S time windows. We transform the data into the desiredÃNform as follows: for each time window s, we create Ds , k=0 bs Â¹k) observations,  DsÂ¹is;j, xs;j ) =1. The predictor xs;j 1ô€€€ps 2Â»0, 1Â¼N and let bs Â¹kÂºof is;j be valued k, for j k = 0;:::, N . To explain the intuition behind the data transformation, notice that we cannot tell from the data which assortment a customer faced when she made the booking. We simply take an average assortment that the customer may have faced, represented by 1ô€€€ps . In other words, if 1 ô€€€ps Â¹jÂº2Â»0, 1Â¼is large, then it implies that product j is oered most of the time during the day, and the transformation leads to the interpretation that consumers see a larger â€œfractionâ€ of product j. As the closure percentage has a continuous impact on the eventual choice, it is reasonable to transform the predictors into a Euclidean space Â»0, 1Â¼N , and build a smooth transition between the two ends ps Â¹jÂº= 0 (the product is always oered) and ps Â¹jÂº= 1 (the product is never oered). The transformation creates a training dataset for classication with continuous predictors. The random forest can accommodate the data with minimal adaptation. In particular, all the steps in Algorithm 1 can be performed. The tree may have dierent structures: because the predictor x may not be at the corner of the unit hypercube any more, the split points may no longer be at 0.5. 1Again, we do not deal with demand censoring in this paper and assume that bshas an additional dimension to record the number of consumers who do not book any class. 18  Electronic copy available at: https://ssrn.com/abstract=3430886 5.3. ProductImportance Random forests can be used to assign scores to each product and rank the importance of products. A common score, mean decrease impurity (MDI), is based on the total decrease in node impurity from splitting on the variable (product), averaged over all trees (Biau and Scornet, 2016). The score for product m is dened as BÃ•Ã•1MDIÂ¹mÂº= Â¹fraction of data in the parent node of sÂºB b=1 all splits s in the bth tree Â¹reduction in the Gini index caused by sÂºIfs splits on mg. In other words, if consumers make decisions frequently based on the presence of product m (a lot of splits occur on product m), or their decisions are more consistent after observing the presence of product m (the Gini index is reduced signicantly after splitting on m), then the product gains more score in MDI and regarded as important. The identication of important products provides simple yet powerful insights into the behavioral patterns of consumers. Consider the following use cases: (1) An online retailer wants to promote its â€œagshipâ€ products that signicantly increase the conversion rate. By computing the MDI from the historical data, important products can be identied without extensive A/B testing. (2) Due to limited capacity, a rm plans to reduce the available types of products in order to cut costs. It could simply remove the products that have low sales according to the historical data. However, some products, while not looking attractive themselves, serve as decoys or references and boost the demand of other products. Removing these products would distort the choice behavior of consumers and may lead to unfavorable consequences. The importance score provides an ideal solution: if a product is ranked low based on MDI, then it does not strongly inuence the decision making of consumers. It is therefore safe to leave them out. (3) When designing a new product, a rm attempts to decode the impact of various product features on customer choices. Which product feature is drawing most attentions? What do attractive products have in common? To conduct successful product engineering, rst it needs to use the historical data to nail down a set of attractive products. Moreover, to quantify and separate out the contribution of various features, a numerical score of product importance is necessary. The importance 19  Electronic copy available at: https://ssrn.com/abstract=3430886 score is a more reasonable criterion than sales volume, because the latter cannot capture the synergy created between the products. 5.4. IncorporatingPriceInformation Besides the ease of estimation, the other benet of a parametric DCM, such as the MNL or nested logit model, is the ability to account for covariates. For example, in the MNL model, the rm can estimate the price sensitivity of each product, and extrapolate/predict the choice probability when the product is charged a new price that has never been observed in the historical data. Many nonparametric DCMs cannot easily be extended to new prices. In this section, we show that while enjoying the benet of a nonparametric formulation, random forests can also accommodate the price information. Consider the data of the following format: fÂ¹it , pt ÂºgT =1, where pt 2Â»0, +1Â¼N rep-t resent the prices of all products. For product j that is not included in the assortment oered to customer t, we set pt Â¹jÂº=+1. This is because when a product is priced at +1, no customer would be willing to purchase it, and it is equivalent to the scenario that the product is not oered at all. Such view of equivalence is commonly adopted in the literature.2 Therefore, compared to the binary vector xt that only records whether a product is oered, the price vector pt provides more information. However, the predictor p can not be readily used in random forests. The predictor space Â»0, +1Â¼N is unbounded, and the value +1added to the extended real number line is not implementable in practice. To apply Algorithm 1, we introduce link functions that map the predictors into a compact set. Denition 1. A function gÂ¹Âº: Â»0, +1Âº7!Â¹0, 1Â¼is referred to as a link function, if (1) gÂ¹xÂºis strictly decreasing, (2) gÂ¹0Âº= 1, and (3) limx!+1gÂ¹xÂº= 0. The link function can be used to transform a price p â‰¥ 0 into Â¹0, 1Â¼. Moreover, because of property (3), we can naturally dene gÂ¹+1Âº= 0. Thus, if product j is not included in assortment xt , then gÂ¹pt Â¹jÂº) = gÂ¹+1) = 0 = xt Â¹jÂº. If product j is oered at a very low price, then gÂ¹pt Â¹jÂºÂºgÂ¹0) = 1. After the transformation of predictors, 2One may argue that an assortment with a product having an articially high price is not equivalent to the one without such a product, as the product may induce reference eects. We do not consider such behaviors here. 20  Electronic copy available at: https://ssrn.com/abstract=3430886 pt !gÂ¹pt Âº3, we introduce a continuous scale to the problem in Section 2. Instead of binary status (included or not), each product now has a spectrum of presence, depending on the price of the product. Now we can directly apply Algorithm 1 to the training data fÂ¹it , gÂ¹pt ÂºÂºgT =1. As a result, we need to modify Step 7, because the algorithm needs to t nd not only the optimal dimension to split, but also the optimal split location. The slightly modied random forests are demonstrated in Algorithm 2. Because of the Algorithm 2 Random forests for DCM estimation with price information 1: Data: fÂ¹it , pt ÂºgT t=1 2: Tunable parameters: number of trees B, sub-sample size z 2f1;:::;T g, number of dimensions to split m 2f1;:::, N g, terminal leaf size l 2f1;:::, zg, a link function gÂ¹) 3: Transform the training data to fÂ¹it , gÂ¹pt ÂºÂºgT t=1 4: for b = 1 to B do 5: Select z observations from the training data with replacement, denoted by Z 6: Initialize the tree tb Â¹gÂ¹pÂºÂº0 with a single root node 7: while some leaf has greater than or equal to l observations belonging to Z and can be split do 8: Select m variables without replacement among f1;:::, N g9: Select the optimal one among the m dimensions and the optimal position to split that minimize the Gini index 10: Split the leaf node into two 11: end while 12: Denote the partition corresponding to the leaves of the tree by fR1;:::, RM g; let ci be the class label of a randomly chosen observation in RiÃM13: Dene tb Â¹gÂ¹pÂºÂº= i=1 ci IfgÂ¹pÂº2Rig14: end for 15: The choice probability of product i given price vector p is ÃB 1 IftbÂ¹gÂ¹pÂºÂº=igb=1 B nature of the decision trees, the impact of prices on the choice behaviors is piecewise linear. For example, Figure 3 illustrates a possible decision tree with N = 3. It is not surprising that there are numerous link functions to choose from. We give two examples below: ô€€€xâ€¢gÂ¹xÂº= e 2â€¢gÂ¹xÂº= 1 âˆ’ arctanÂ¹xÂº 3When gÂ¹Âºis applied to a vector p, it is interpreted as applied to each component of the vector. 21  Electronic copy available at: https://ssrn.com/abstract=3430886 gÂ¹pÂ¹1ÂºÂº>0:3 gÂ¹pÂ¹1ÂºÂº>0:9 1 gÂ¹pÂ¹2ÂºÂº>0:5 2 0 gÂ¹pÂ¹3ÂºÂº>0:4 gÂ¹pÂ¹3ÂºÂº>0:8 3 0 gÂ¹pÂ¹2ÂºÂº>0:3 2 0 Y Y N Y N N Y Y N N Y N gÂ¹pÂ¹1ÂºÂº>0:3 gÂ¹pÂ¹1ÂºÂº>0:91 gÂ¹pÂ¹2ÂºÂº>0:5Y gÂ¹pÂ¹3ÂºÂº>0:4 gÂ¹pÂ¹3ÂºÂº>0:8 gÂ¹pÂ¹2ÂºÂº>0:3N YN YN YN YN YN 2 0 3 0 2 0 Figure 3: A possible decision tree when the price information is incorporated for N = 3. gÂ¹pÂ¹iÂºÂº> a is equivalent to pÂ¹iÂº< gô€€€1Â¹aÂº, i.e., product i is included in the assortment and its price is less than gô€€€1Â¹aÂº. In fact, the survival function of any non-negative random variables with positive PDF is a candidate for the link function. This extra degree of freedom may concern some academics and practitioners: How sensitive is the estimated DCM to the choice of link functions? What criteria may be used to pick a â€œgoodâ€ link function? Our next result guarantees that the choice of link functions does not aect the estimated DCM. For any two link functions g1Â¹xÂºand g2Â¹xÂº, we can run Algorithm 2 for training data fÂ¹it , g1Â¹pt ÂºÂºgT =1 and fÂ¹it , g2Â¹pt ÂºÂºgT =1. We use tÂ¹jÂºÂ¹xÂºto denote the returned bth tree of t tb the algorithm for link function gj Â¹xÂº, j = 1, 2. Proposition1.If we equalize â€¢the choice of parameters in Step 2 except for the link function â€¢the internal randomizers in Step 5, 8, and 12 in Algorithm 2, then the trees of both link functions return the same class label for Â¹1ÂºÂ¹2Âºan observation in the training data: t Â¹g1Â¹pt Âº) = t Â¹g2Â¹pt ÂºÂºfor all t = 1;:::;T andbb b = 1;:::, B. It is worth pointing out that although the random forests using two link functions output identical class labels for pt in the training data, they may dier for when predicting a new price vector p. This is because the splitting operation that minimizes 22  Electronic copy available at: https://ssrn.com/abstract=3430886 the Gini index in Step 8 is not unique. Any split between two consecutive observations4 results in an identical class composition in the new leaves and thus the same Gini index. Usually the algorithm picks the middle between two consecutive observations to split, which may dier for dierent link functions if they are not locally linear. Nevertheless, these cases are rare and Algorithm 2 is not sensitive to the choice of link functions. The theoretical guarantee in the pricing setting, however, is far more involved than Section 2. The state-of-art theoretical guarantee of random forests is given by Scornet et al. (2015). The authors prove that random forests are consistent for the regression problem, under some mild assumptions. Their setup is the closest to the original algorithm proposed in Breiman (2001), while other papers have proved the consistency for random forests with simplied or special implementations. Our setup diers from Scornet et al. (2015) in that we are focusing on a classication problem. We can recast it into a regression problem by analyzing the class probability of a particular class. However, instead of the Gini index, the sum of squared errors is typically used in regression problems, and the analysis has to be modied substantially. We thus leave the theoretical guarantee for future research. 5.5. IncorporatingCustomerFeatures A growing trend in online retailing and Ecommerce is personalization. Due to the increasing access to personal information and computation power, the retailer is able to device specic policies, including pricing or recommendation, for dierent customers based on his/her observed features. Personalization turns out to be hugely successful. Imagine an arriving customer being labeled as a college student. Then for a fashion retailer, it is a strong indicator that she/he may be interested in aordable brands. Leveraging personal information can greatly increase the garnered revenue of the rm. To oer personalized assortment, the very rst step is to incorporate the feature information into the choice model. One possible model, The mixed logit model assumes that the customers are categorized into discrete types, and customers of the same type behave homogeneously according to an independent MNL model. As one of the main drawbacks, it is not straightforward to connect continuous customers features 4If the algorithm splits on dimension m, then pt1 and pt2 are consecutive if there does not exist pt3 in the same leaf node such that Â¹pt1 Â¹mÂºô€€€pt3 Â¹mÂºÂºÂ¹pt2 Â¹mÂºô€€€pt3 Â¹mÂºÂº< 0. 23  Electronic copy available at: https://ssrn.com/abstract=3430886 Has product 1 Has product 3 Choose 3 Choose 1 Age 30 Married Choose 4 Choose 2 No purchase Y Y N N Y Y N N Has product 1 Has product 3 Choose 3 Choose 1 Age 30 Married Choose 4 Choose 2 No purchase Y Y N N Y Y N N Figure 4: A possible binary choice tree after incorporating customer features. to discrete types, and unsupervised learning algorithms may be needed. Recently, Bernstein et al. (2018) propose a dynamic Bayesian algorithm to address the issue. Another typical approach is built on the MNL model, while replacing the deterministic utility of a product by a linear function of the customer feature. For example, see Cheung and Simchi-Levi (2017) and references therein. In such personalized MNL models, the critics of the MNL model (such as IIA) persist. In this section, we demonstrate that it is natural for random forests to capture customer features and return a binary choice forest that is aware of such information. Suppose the collected data of the rm have the form Â¹it , xt , ft Âºfor customer t, where in addition to Â¹it , xt Âº, the choice made and the oered set, the normalized customer feature ft 2Â»0, 1Â¼M is also recorded. The procedure in Section 3 can be extended naturally. In particular, we may append ft to xt , so that the predictor Â¹x, f Âº2Â»0, 1Â¼M+N . Algorithm 1 can be modied accordingly. The resulting binary choice forest consists of B binary choice trees. The splits of the binary choice tree now encode not only whether a product is oered, but also predictive feature information of the customer. For example, a possible binary choice tree illustrated in Figure 4 may result from the algorithm. Compared with the current personalized choice models, the framework introduced in this paper has the following benets: â€¢The estimation is straightforward (same as the algorithm without customer 24  Electronic copy available at: https://ssrn.com/abstract=3430886 features) and can be implemented eciently. â€¢The nonparametric nature of the model allows to capture complex interaction between products and customer features, and among customer features. For example, â€œoering a high-end handbagâ€ may become a strong predictor when the combination of features â€œfemaleâ€ and â€œageâ‰¥ 30â€ are activated. In a binary choice tree, the eect is captured by three splits (one for the product and two for the customer features) along a branch. It is almost impossible to capture in a parametric (linear) model. â€¢The framework can be combined with aforementioned adjustments, such as pricing and product importance. For example, the measure MDI introduced in Section 5.3 can be used to identify predictive customer features. 6. NumericalExperiments In this section, we conduct a comprehensive numerical study based on both synthetic and real datasets. We nd that (1) random forests are quite robust and the performance does not vary much for underlying DCMs with dierent levels of complexity. In particular, random forests only underperform the correctly specied parametric models by a small margin and do not overt; (2) the standard error of random forests are small compared to other estimation procedures; (3) random forests benet tremendously from increasing sample size compared to other DCMs; (4) the computation time of random forests almost does not scale with the size of the training data; (5) random forests perform well even if the training set only includes 1Â100 of all available assortments; (6) random forests handle training data with nonstandard format reasonably well, such as aggregated data and price information (see Section 5.2 and 5.4 for more details) which cannot be handled easily by other frameworks. We will compare the estimation results of random forests with the MNL model (Train, 2009) and the Markov chain model (Blanchet et al., 2016)5 for both synthetic 5The MNL model is estimated using MLE. The Markov chain model is estimated using the EM algorithm, the same as the implementation in Simsek and Topaloglu (2018). The random forest is estimated using the Python package â€œscikit-learnâ€. The implementation is slightly dierent in that scikitlearn outputs the empirical class probability rather than a random sample in Step 11. The dierence is negligible when B is large. 25  Electronic copy available at: https://ssrn.com/abstract=3430886 and real data sets. We choose the MNL and the Markov Chain models as benchmarks because the MNL model is one of the most widely used DCM and the Markov chain model can exibly approximate RUM (OÂ¹N 2Âº) and has been shown (Berbeglia et al., 2018) to have an outstanding empirical performance compared to MNL, the nested logit, the mixed logit, and rank-based DCM. Note that the actual DCM generating the training data is not necessarily one of the three models mentioned above. When conducting numerical experiments, we set the hyper-parameters of the prandom forest as follows: B = 1000, z = T , m = N , l = 50. Choosing the parameters optimally using cross validation would further improve the performance of random forest. 6.1. TheRandomUtilityModel We rst investigate the performance of random forests when the training data is generated by RUM. The RUM includes a large class of DCMs. Consider N = 10 products. We generate the training set using the MNL model as the ground truth, where the expected utility of each product is generated from a standard normal distribution. Our training data consists of TËœ 2f30, 75, 150, 300, 600} periods. Each period contains a single assortment and 10 transactions so the total number of data points isT = 10TËœ. This is following the setup of Berbeglia et al. (2018). We randomly generate an assortment in each period uniformly randomly among all assortments. The performance is evaluated by root mean squared error, which is also used in Berbeglia et al. (2018): = vuutÃ SÂ»N ] Ã2 PÂ¹jjSÂºô€€€PË†Â¹jjSÂº Ãj2S[f0} SÂ»N Â¼Â¹jS | P, Ë†P , (2)RMSE + 1) where Pdenotes the actual choice probability and PË† denotes the estimated choice probability. The RMSE tests all the assortments and there is no need to generate a test set. For each setting, we generate 100 independent training data sets and compute the average and standard deviation of the RMSEs. The result is shown in Table 2. Not surprisingly, MNL model performs the best among the three because it has very few parameters and correctly species the ground truth. With such a simple DCM, the 26  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 0.084 (0.014) 0.030 (0.007) 0.062 (0.009) 750 0.061 (0.006) 0.019 (0.005) 0.042 (0.005) 1500 0.048 (0.005) 0.014 (0.003) 0.031 (0.004) 3000 0.041 (0.004) 0.009 (0.002) 0.023 (0.003) 6000 0.037 (0.002) 0.006 (0.002) 0.017 (0.002) Table 2: The average and standard deviation of RMSE using random forests, the MNL and the Markov chain model when the training data is generated by the MNL model. random forest does not overt and only slightly underperforms the Markov chain model. As the data size increases, the RMSE of random forest converges to zero. Next we use the rank-based model to generate the training data, which is shown to be equivalent to RUM (Block et al., 1959). Consider N = 10 products. Consumers are divided into k = 4 or k = 10 dierent types, each with a random preference permutation of all the products and the no-purchase alternative. For a given assortment of products, each type of consumer will purchase the product ranked the highest in her preference rank. If the no-purchase option is ranked higher than all the products in the assortment, then the customer does not purchase anything. We also randomly generate the fractions of customer types as follows: draw uniform random variables ui between zero and uione for i = 1, :::, k, and then set Ãk to be the proportion of type i, i = 1, :::, k. The j=1 ujresult is shown in Table 3. We can see that the MNL model underperforms and does not improve signicantly as the data size increases, because of the misspecication error. The Markov chain model performs the best among the three. The performance of the random forest is quite robust, judged from the low standard deviation. Moreover, the performance improves dramatically as T increases; for T = 20000, the RMSE is smaller than the Markov chain model, which is shown in Berbeglia et al. (2018) to outperform other DCM estimators. Predicted by Theorem 4, the RMSE tends to zero when the training set is large. We run our algorithm on iMac with 2.7GHz quad-core Inter Core i5 and 8GB RAM installed. The running time is shown in Table 4. In terms of computation time, both the MNL model and the random forest can be implemented eciently, while the EM algorithm used to estimate the Markov chain model takes much longer. WhenT = 20000, the random forest spends 1/17 of the computation time of the Markov chain model. 27  Electronic copy available at: https://ssrn.com/abstract=3430886 Tk = 4 RF MNL Markov 300 0.115 (0.031) 0.121 (0.034) 0.078 (0.032) 750 0.090 (0.021) 0.118 (0.025) 0.058 (0.024) 1500 0.069 (0.016) 0.114 (0.029) 0.047 (0.020) 3000 0.056 (0.009) 0.118 (0.018) 0.044 (0.017) 6000 0.045 (0.006) 0.116 (0.021) 0.040 (0.017) 20000 0.034 (0.004) 0.115 (0.020) 0.037 (0.017) k = 10 RF MNL Markov 300 0.104 (0.013) 0.097 (0.016) 0.077 (0.016) 750 0.079 (0.009) 0.093 (0.012) 0.057 (0.009) 1500 0.065 (0.008) 0.091 (0.014) 0.048 (0.009) 3000 0.053 (0.005) 0.088 (0.013) 0.042 (0.008) 6000 0.046 (0.004) 0.088 (0.013) 0.040 (0.008) 20000 0.038 (0.003) 0.087 (0.014) 0.037 (0.009) Table 3: The average and standard deviation of RMSE of random forests, the MNL and the Markov chain model when the training data is generated using the rank-based model. 28  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 72.3s 0.7s 25.7s 750 72.5s 1.4s 36.1s 1500 72.3s 3.2s 113.7s 3000 74.0s 6.8s 203.0s 6000 74.5s 17.4s 445.2s 20000 81.8s 55.5s 1460.6s Table 4: The average running time of Random forest, MNL and the Markov chain Model Note that the running time of random forest almost does not increase for larger training set. This makes it useful when dealing with big data. 6.2. GeneralizabilitytoUnseenAssortments One of the major challenges in the estimation of the DCM, compared to other statistical estimation problems, is the limited coverage of the training data, which strongly violates the i.i.d. assumption. In particular, the seller tends to oer a few assortments that they believe are protable. As a result, in the training data fxt gT =1 only makes up a small t fraction of the total 2N available assortments. Any estimation procedure needs to address the following issue: can the DCM estimated from a few assortments generalize to the assortments that have never been oered in the training data? Next we show that random forests perform this task well: theoretically, random forests adaptively choose nearest neighbors, and the choice probability of an assortment can be generalized to â€œneighboringâ€ assortments (those with one more or one less product), as long as the underlying DCM possesses a certain degree of continuity in terms of the oered set x. Consider N = 10 products andT = 6000. We randomly choose TËœ assortments to oer in the training set and thus there are 6000ÂTËœ transactions for each assortment. â€œLargeâ€ assortments refer to those with many products (7 jS j10). The result is shown in Table 5. Note that there are 2N ô€€€1 = 1023 possible available assortments. Therefore, for example, TËœ = 10 implies that only 1Â100 of the total assortments have been oered in the training data. The RMSE is only two to three times larger than the case where most assortments have been oered TËœ = 600. Moreover, a larger assortment helps the estimation of the DCM. When the actual DCM is the MNL 29  Electronic copy available at: https://ssrn.com/abstract=3430886 TËœ Rank-based k = 4 Rank-based k = 10 MNL 5 0.193 (0.064) 0.156 (0.034) 0.133 (0.041) 10 0.158 (0.034) 0.128 (0.026) 0.111 (0.035) 5 (large) 0.181 (0.056) 0.124 (0.028) 0.038 (0.017) 10 (large) 0.150 (0.047) 0.109 (0.027) 0.034 (0.014) 50 0.087 (0.025) 0.073 (0.014) 0.054 (0.008) 100 0.068 (0.014) 0.060 (0.007) 0.042 (0.004) 600 0.045 (0.006) 0.046 (0.004) 0.037 (0.002) Table 5: The average and standard deviation of RMSE using random forests when there are a few assortments in the training data. The column represents dierent ground-truth models. model, training random forests with 10 large assortments performs better than training with 600 randomly chosen assortments. We also remark that the generalizability of random forests does not only depend on the estimator, but also the actual DCM. Some DCMs are more accessible to generalization to unseen assortments. It remains an exciting future research to formalize the statement and theoretically quantify the generalizability of a DCM to unseen data in the framework of random forests. 6.3. BehavioralChoiceModels When the DCM is outside the scope of RUM and the regularity is violated, the Markov chain and MNL model may fail to specify the choice behavior correctly. In this section, we generate choice data using the comparison-based DCM (Huber et al., 1982), described below. Consumers implicitly score various attributes of the products in the assortment. Then they undergo an internal round-robin tournament of all the products. When comparing two products from the assortment, the customer checks their attributes and count the number of preferable attributes of both products. Eventually, the customer count the total number of wins (preferable attributes) in the pairwise comparisons. Here we assume that customers choose with equal probability if there is a tie. In the experiment, we consider N = 10 products. Consumers are divided into k = 2 dierent types, whose proportions are randomly generated between 0 and 1. Each type assigns uniform random variables between 0 and 1 to the ve attributes of all the 30  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 0.157 (0.031) 0.160 (0.033) 0.146 (0.038) 750 0.133 (0.025) 0.156 (0.030) 0.132 (0.036) 1500 0.112 (0.022) 0.152 (0.030) 0.123 (0.033) 3000 0.094 (0.021) 0.155 (0.030) 0.120 (0.037) 6000 0.079 (0.018) 0.152 (0.032) 0.120 (0.036) Table 6: The average and standard deviation of RMSE using Random Forest, MNL and Markov chain Model under the comparison-based DCM products (including the no-purchase option). Again we use the RMSE in (2) to compare the predictive accuracy. Like in the previous experiment, each setting is simulated 100 times. The result is shown in Table 6. Because of the irregularity, both the MNL and the Markov chain DCM are outperformed by the random forest, especially when the data size increases. Note that as T !1, the random forest is able to achieve diminishing RMSE, while the other two models do not improve because of the misspecication error. Like the previous experiment, the random forest achieves stable performances with small standard deviations. 6.4. AggregatedChoiceData In this section, we investigate the performance of random forests when the training data is aggregated as in Section 5.2. To generate the aggregated training data, we rst generate T observations using the rank-based model for N = 10 products and k = 10 customer types, as in Section 6.1. The only dierence is that we only simulate one instead of ten transactions for each oered assortment. Then, we let a be aggregation levels, i.e., we aggregate a data points together. For example, a = 1 is equivalent to the original data. For a = 5, Table 7 illustrates ve observations in the original data set for n = 5. Upon aggregation, the ve transactions are replaced by ve new observations with xt Â»0:6, 0:4, 0:8, 0:4, 0:6Â¼and it = 1, 0, 4, 3, 1 for t = 1, 2, 3, 4, 5. We test the performance for dierent sizes of the training set T 2f500, 5000, 50000gand dierent aggregate levels a 2f1, 5, 10, 50, 100g. The performance is measured in RMSE. We simulate 100 instances for each setting to evaluate the average and standard deviation, shown in Table 8. From the results, random forests handle aggregate data 31  Electronic copy available at: https://ssrn.com/abstract=3430886 Product 1 Product 2 Product 3 Product 4 Product 5 Choices 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 4 3 1 Table 7: Five observations in the unaggregated original data. Upon aggregation, they are replaced by ve new observations with xtÂ»0:6, 0:4, 0:8, 0:4, 0:6Â¼and it= 1, 0, 4, 3, 1 for t = 1, 2, 3, 4, 5. T a = 1 a = 5 a = 10 a = 50 a = 100 500 0.082 (0.009) 0.109 (0.016) 0.114 (0.016) 0.119 (0.015) 0.120 (0.015) 5000 0.047 (0.004) 0.085 (0.010) 0.097 (0.012) 0.111 (0.013) 0.114 (0.013) 50000 0.039 (0.002) 0.068 (0.009) 0.082 (0.011) 0.103 (0.013) 0.108 (0.013) Table 8: Random Forest performance for dierent aggregate levels relatively well. Even with aggregation level a = 100, the RMSE does not seem to deteriorate signicantly. Note that no other DCMs can handle aggregate data to the best of our knowledge, so no benchmark can be provided in this case. 6.5. IncorporatingPricingInformation In this section, we test the performance of random forests when the price information is incorporated. This is a unique feature of random forests as most DCMs canâ€™t estimate the choice probability eciently with prices. We use the MNL model to generate the choice data. Let u denote the expected utility of the products and p their prices. Therefore, for given assortment S, the choice probabilities of product i 2S and the no-purchase option are: expÂ¹ui ô€€€pi ) 1Pi = Ã , P0 = Ã . (3)1 + j2S expÂ¹uj ô€€€pj ) 1 + j2S expÂ¹uj ô€€€pj ) Consider N = 10 products. We generate ui as uniform random variables between 0 and 1 for each product. For each observation, we rst randomly generate an assortment as Section 6.1. Then we generate a price for each product in the assortment as the 32  Electronic copy available at: https://ssrn.com/abstract=3430886 T RMSE 500 0.067 (0.008) 5000 0.040 (0.002) 50000 0.035 (0.002) Table 9: The RMSE of random forests with price information absolute value of a standard normal random variable. As explained in Section 5.4, we use the link function gÂ¹xÂº= expÂ¹ô€€€xÂº. The customerâ€™s choice then follows the choice probability (3). The RMSE in (2) is no longer applicable because the assortments and prices cannot be exhausted. To evaluate the performance, we randomly generate N = 1000 assortments and prices according to the same distribution as the training data. Then we evaluate the RMSE as follows: vuutÃN Ã2 PÂ¹jjSi Âºô€€€PË†Â¹jjSi Âº i=1 j2Si[f0gÃNP, Ë†P , (4)RMSE = =1Â¹jSi j+ 1Âºi where Pdenotes the actual choice probability, and PË† denotes the estimation. We investigate the performance of the random forest for dierent sizes of training data T 2f500, 5000, 50000g. The result is shown in Table 9. The result conrms that random forests can tackle price information well. Although we do not have benchmarks, the RMSE is comparable to the previous experiments, e.g., Table 8. 6.6. RealData:Hotel In this section we apply the random forest algorithm to a public dataset based on Bodea et al. (2009). The dataset includes transient customers (mostly from business travelers) who stayed in one of ve continental U.S. hotels between March 12, 2007, and April 15, 2007. The minimum booking horizon for each check-in date is four weeks. Rate and room type availability and reservation information are collected via the hotel and/or customer relationship ocers (CROs), the hotelâ€™s websites, and oine travel agencies. Since there is no direct competition among these ve hotels, we will process the data 33  Electronic copy available at: https://ssrn.com/abstract=3430886 separately. A product is uniquely dened by the room type (e.g. Suite 1, 2 Double Beds Room 1, etc). For each transaction, the purchased room type and the assortment oered are recorded. When processing the dataset, we have removed the product that has less than 10 transactions. We also removed the transactions whose oered assortments are not available due to technical reasons. For the transactions that none of the products in the available sets are purchased by the customer, we assume customers choose the no-purchase alternative. We do not add dummy transactions with no-purchases to uncensor the data like van Ryzin and Vulcano (2014), Simsek and Topaloglu (2018) and Berbeglia et al. (2018). To compare dierent estimation procedures, we use ve-fold cross validation to examine the out-of-sample performance. Because we no longer know the actual choice model that generates the data, after estimating the model in the training set, we follow Berbeglia et al. (2018) and evaluate the â€œempiricalâ€ version of the RMSE in the validation set. That is, letting T be the validation set, we dene  Ë† Â¹i;SÂº2T Ifj=igâˆ’ P, T vuutÃ2Ã PË†Â¹jjSÂº (5)RMSE =Ãj2S[f0} . Â¹i;SÂº2TÂ¹jS j+ 1) In Table 10 we show the scale of the ve datasets after preprocessing. We show the out-of-sample RMSE data for each hotel (average and standard deviation). In addition, we also show the performance of the independent demand model (ID), which does not incorporate the substitution eect and is expected to perform poorly, in order to provide a lower bound of the performance. Consistent with the insights drawn from the synthetic data, random forest outperforms the parametric methods for larger dataset (Hotel 1, 2 and 3). For smaller data size (Hotel 4 and 5), random forest is on a par with the best parametric estimation procedure (Markov) according to Berbeglia et al. (2018). 6.7. RealData:IRIAcademicDataset In this section we compare several algorithms on the IRI Academic Dataset (Bronnenberg et al., 2008). The IRI Academic Dataset collects weekly transaction data from 47 U.S. 34  Electronic copy available at: https://ssrn.com/abstract=3430886 # products # in-sample # out-of-sample Hotel 1 Hotel 2 Hotel 3 Hotel 4 Hotel 5 10 6 7 4 6 1271 347 1073 240 215 318 87 268 60 54 RF MNL Markov ID Hotel 1 Hotel 2 Hotel 3 Hotel 4 Hotel 5 0.3040 (0.0046) 0.3034 (0.0120) 0.2842 (0.0051) 0.3484 (0.0129) 0.3219 (0.0041) 0.3098 (0.0031) 0.3120 (0.0148) 0.2854 (0.0065) 0.3458 (0.0134) 0.3222 (0.0069) 0.3047 (0.0039) 0.3101 (0.0124) 0.2842 (0.0064) 0.3471 (0.0125) 0.3203 (0.0046) 0.3224 (0.0043) 0.3135 (0.0178) 0.2971 (0.0035) 0.3584 (0.0047) 0.3259 (0.0058) Table 10: Summary statistics of the ve datasets and the average and standard deviation of the out-of-sample RMSE. markets from 2001 to 2012, covering more than 30 product categories. Each transaction includes the week and store of purchase, the universal product code (UPC) of the purchased item, number of units purchased and total paid dollars. The pre-processing steps taken follow those in Jagabathula and Rusmevichientong (2018) and Chen and MiÅ¡ic (2019). In particular, we conduct the analysis for 31 categories separately using the data for the rst two weeks in 2007. Each product is uniquely dened by the vendor code. Each assortment is dened as the set of products that are available in the same store in that week. We only focus on the top nine purchased products from all stores during the two weeks in each category and treat all other products as the no-purchase alternative. However, the sales data for most categories is still too large for the EM algorithm to estimate the Markov chain model. For example, carbonated beverages, milk, soup and yogurt have more than 10 million transactions. For computational eciency, we uniformly sample 1/200 of original data size without replacement. This does not signicantly increase the sampling variability as most transactions in the original data are repeated entries. We use ve-fold cross-validation and RMSE dened in (5) to examine the out-ofsample performance. The result is shown in Table 11. Random forests outperform the 35  Electronic copy available at: https://ssrn.com/abstract=3430886 other two in 24 of 31 categories, especially for larger data size. According to Berbeglia et al. (2018), the Markov chain choice model has already been shown to have superb performance in synthetic and real-world studies. Table 11 fully demonstrates the potential of random forests as a framework to model and estimate consumer behaviors in practice. 7. ConcludingRemarks We hope that this study will encourage more scholars to pursue BRF as a research topic. We believe that addressing the following questions would help us decode the empirical success of random forests and understand the pitfalls: â€¢What type of DCMs can be estimated well by random forests and have higher generalizability to unseen assortments? â€¢As we use the choice forest to approximate DCMs, how can we translate the properties of a DCM to the topological structure of decision trees? â€¢Can we provide nite-sample error bounds for the performance of random forests, with or without the price information? â€¢What properties does the product importance index MDI have? â€¢Given a binary choice forest, possibly estimated by random forests, can we compute the optimal assortment eciently? References Anderson, S. P., A. De Palma, and J.-F. Thisse (1992). Discrete choice theory of product dierentiation. MIT press. Ariely, D. (2008). Predictably irrational. Harper Audio. Berbeglia, G. (2019). The generalized stochastic preference choice model. Working paper. 36  Electronic copy available at: https://ssrn.com/abstract=3430886 Product Category # data RF MNL Markov 1 Beer 10,440 0.2717 (0.0006) 0.2722 (0.0008) 0.2721 (0.0007) 2 Blades 1,085 0.3106 (0.0037) 0.3092 (0.0034) 0.3096 (0.0036) 3 Carbonated Beverages 71,114 0.3279 (0.0004) 0.3299 (0.0004) 0.3295 (0.0004) 4 Cigarettes 6,760 0.2620 (0.0028) 0.2626 (0.0030) 0.2626 (0.0030) Coee 8,135 0.2904 (0.0010) 0.2934 (0.0009) 0.2925 (0.0010) 6 Cold Cereal 30,369 0.2785 (0.0003) 0.2788 (0.0003) 0.2787 (0.0003) 7 Deodorant 2,775 0.2827 (0.0005) 0.2826 (0.0005) 0.2826 (0.0005) 8 Diapers 1,528 0.3581 (0.0024) 0.3583 (0.0020) 0.3583 (0.0022) 9 Facial Tissue 8,956 0.3334 (0.0007) 0.3379 (0.0010) 0.3375 (0.0007) Frozen Dinners/Entrees 48,349 0.2733 (0.0003) 0.2757 (0.0003) 0.2750 (0.0003) 11 Frozen Pizza 16,263 0.3183 (0.0001) 0.3226 (0.0001) 0.3210 (0.0001) 12 Household Cleaners 6,403 0.2799 (0.0010) 0.2798 (0.0009) 0.2798 (0.0009) 13 Hotdogs 7,281 0.3123 (0.0011) 0.3183 (0.0005) 0.3170 (0.0007) 14 Laundry Detergent 7,854 0.2738 (0.0017) 0.2875 (0.0017) 0.2853 (0.0016) Margarine/Butter 9,534 0.2985 (0.0004) 0.2995 (0.0004) 0.2990 (0.0003) 16 Mayonnaise 4,380 0.3212 (0.0024) 0.3242 (0.0010) 0.3230 (0.0006) 17 Milk 56,849 0.2467 (0.0007) 0.2501 (0.0005) 0.2538 (0.0012) 18 Mustard 5,354 0.2844 (0.0008) 0.2856 (0.0006) 0.2852 (0.0006) 19 Paper Towels 9,520 0.2939 (0.0009) 0.2964 (0.0008) 0.2959 (0.0008) Peanut Butter 4,985 0.3113 (0.0017) 0.3160 (0.0006) 0.3146 (0.0009) 21 Photography supplies 189 0.3456 (0.0081) 0.3399 (0.0081) 0.3456 (0.0088) 22 Razors 111 0.3269 (0.0300) 0.3294 (0.0225) 0.3323 (0.0195) 23 Salt Snacks 44,975 0.2830 (0.0006) 0.2844 (0.0007) 0.2840 (0.0007) 24 Shampoo 3,354 0.2859 (0.0006) 0.2855 (0.0071) 0.2856 (0.0009) Soup 68,049 0.2709 (0.0007) 0.2738 (0.0005) 0.2729 (0.0005) 26 Spaghetti/Italian Sauce 12,377 0.2901 (0.0003) 0.2919 (0.0006) 0.2914 (0.0006) 27 Sugar Substitutes 1,269 0.3080 (0.0036) 0.3067 (0.0035) 0.3072 (0.0034) 28 Toilet Tissue 11,154 0.3084 (0.0005) 0.3126 (0.0004) 0.3132 (0.0014) 29 Toothbrushes 2,562 0.2860 (0.0009) 0.2859 (0.0004) 0.2858 (0.0006) Toothpaste 4,258 0.2704 (0.0008) 0.2708 (0.0011) 0.2708 (0.0011) 31 Yogurt 61,671 0.2924 (0.0011) 0.2976 (0.0008) 0.2960 (0.0008) Table 11: The data size after pre-processing and the average and standard deviation of the out-of-sample RMSE (5) for each category. 37  Electronic copy available at: https://ssrn.com/abstract=3430886 Berbeglia, G., A. Garassino, and G. Vulcano (2018). A comparative empirical study of discrete choice models in retail operations. Working paper. Bernstein, F., S. Modaresi, and D. SaurÃ© (2018). A dynamic clustering approach to data-driven assortment personalization. Management Science 65(5), 2095â€“2115. Biau, G. and E. Scornet (2016). A random forest guided tour. Test 25(2), 197â€“227. Blanchet, J., G. Gallego, and V. Goyal (2016). A markov chain approximation to choice modeling. Operations Research 64(4), 886â€“905. Block, H. D., J. Marschak, et al. (1959). Random orderings and stochastic theories of response. Technical report, Cowles Foundation for Research in Economics, Yale University. Bodea, T., M. Ferguson, and L. Garrow (2009). Data setâ€”choice-based revenue management: Data from a major hotel chain. Manufacturing & Service Operations Management 11(2), 356â€“361. Breiman, L. (2001). Random forests. Machine learning 45(1), 5â€“32. Bronnenberg, B. J., M. W. Kruger, and C. F. Mela (2008). Database paperâ€”the iri marketing data set. Marketing science 27(4), 745â€“748. Chen, Y.-C. and V. V. MiÅ¡ic (2019). Decision forest: A nonparametric approach to modeling irrational choice. Working paper. Cheung, W. C. and D. Simchi-Levi (2017). Thompson sampling for online personalized assortment optimization problems with multinomial logit choice models. Working paper. Farias, V. F., S. Jagabathula, and D. Shah (2013). A nonparametric approach to modeling choice with limited data. Management science 59(2), 305â€“322. Feng, G., X. Li, and Z. Wang (2017). On the relation between several discrete choice models. Operations research 65(6), 1516â€“1525. Flores, A., G. Berbeglia, and P. Van Hentenryck (2017). Assortment and price optimization under the two-stage luce model. Working paper. 38  Electronic copy available at: https://ssrn.com/abstract=3430886 Hastie, T., R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics. Springer. Huber, J., J. W. Payne, and C. Puto (1982). Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis. Journal of consumer research 9(1), 90â€“98. Jagabathula, S. and P. Rusmevichientong (2016). A nonparametric joint assortment and price choice model. Management Science 63(9), 3128â€“3145. Jagabathula, S. and P. Rusmevichientong (2018). The limit of rationality in choice modeling: Formulation, computation, and implications. Management Science 65(5), 2196â€“2215. Jagabathula, S., L. Subramanian, and A. Venkataraman (2019). A conditional gradient approach for nonparametric estimation of mixing distributions. Management Science. Liaw, A. and M. Wiener (2002). Classication and regression by randomforest. R News 2(3), 18â€“22. McFadden, D. (1973). Conditional logit analysis of qualitative choice behaviour. In P. Zarembka (Ed.), Frontiers in Econometrics, pp. 105â€“142. New York, NY, USA: Academic Press New York. MiÅ¡ic, V. V. (2016). Data, models and decisions for large-scale stochastic optimization problems. Ph. D. thesis, Massachusetts Institute of Technology. Natarajan, K., M. Song, and C.-P. Teo (2009). Persistency model and its applications in choice modeling. Management Science 55(3), 453â€“469. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay (2011). Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research 12, 2825â€“2830. Russo, J. E. and B. A. Dosher (1983). Strategies for multiattribute binary choice. Journal of Experimental Psychology: Learning, Memory, and Cognition 9(4), 676. 39  Electronic copy available at: https://ssrn.com/abstract=3430886 Scornet, E., G. Biau, J.-P. Vert, et al. (2015). Consistency of random forests. The Annals of Statistics 43(4), 1716â€“1741. Simonson, I. and A. Tversky (1992). Choice in context: Tradeo contrast and extremeness aversion. Journal of marketing research 29(3), 281â€“295. Simsek, A. S. and H. Topaloglu (2018). An expectation-maximization algorithm to estimate the parameters of the markov chain choice model. Operations Research 66(3), 748â€“760. Train, K. E. (2009). Discrete choice methods with simulation. Cambridge university press. van Ryzin, G. and G. Vulcano (2014). A market discovery algorithm to estimate a general class of nonparametric choice models. Management Science 61(2), 281â€“300. van Ryzin, G. and G. Vulcano (2017). An expectation-maximization method to estimate a rank-based choice model of demand. Operations Research 65(2), 396â€“407. Wang, R. and O. Sahin (2017). The impact of consumer search cost on assortment planning and pricing. Management Science 64(8), 3649â€“3666. Weitzman, M. L. (1979). Optimal search for the best alternative. Econometrica, 641â€“654. Williams, H. C. (1977). On the formation of travel demand models and economic evaluation measures of user benet. Environment and planning A 9(3), 285â€“344. A. Proofs Proof of Theorem 1: It is easy to see that a BCF is a DCM. To show the converse, consider a collection of DCMs pc Â¹i, SÂº, c âˆˆ C. Let â‰¥ 0 with Ã = 1, Then pÂ¹i, S) = cc2CcÃ cpc Â¹i, SÂºis clearly a DCM, so a convex combination of DCM is a DCM and thus c2C all DCMs form a convex set. Consider the extreme points, i.e., DCMs that cannot be written as a non-trivial convex combination of two or more DCMs. Let E be the collection of all extreme DCMs, pe Â¹i, SÂº, e âˆˆ E. Then any DCM pÂ¹i, SÂº, i âˆˆ S+, S âŠ‚ N is in the convex hull of pe, e âˆˆ E. A deterministic DCM is a DCM such that pÂ¹i, SÂº2f0, 1gn for every i 2S+ and every 40  Electronic copy available at: https://ssrn.com/abstract=3430886 S âŠ‚ N . Next we show that a DCM is an extreme point if and only if it is deterministic. Given a deterministic DCM, say d, let id Â¹SÂºâˆˆ S+ be the choice made by d, so that pd Â¹i, SÂº= 1 only if i = id Â¹SÂº. It is clear that a deterministic DCM is an extreme point. Conversely, for an extreme DCM, if it is not deterministic, then we can always split the probability between 0 and 1 and makes it a convex combination of two dierent DCMs. Therefore, extreme points are equivalent to deterministic DCMs. It is sucient to show that all deterministic DCMs can be represented as a BCF. This follows directly because every deterministic DCM is the binary choice tree which can be explicitly constructed tb Â¹SÂº, id Â¹SÂºfor all S N . We can now formally state the connection between DCMs and BCFs. Proof of Theorem 3. Let pÂ¹i, xÂºbe an arbitrary DCM. Construct B trees tb, b = 1;:::, B, with 2n leaves associated with each of the 2n possible subsets of Â»N Â¼. For any given ÃB x 2f0, 1gN , we let =1 IftbÂ¹xÂº=i} = bpÂ¹i, xÂºBcfor i = 1;:::, N . It is easy to see that b N 1 jf Â¹0, xÂºô€€€pÂ¹0, xÂº| â‰¤ jf Â¹i, xÂºô€€€pÂ¹i, xÂº| â‰¤ , i = 1;:::, N . BB Since the error bound holds for all x and i, the choice forest can approximate any DCM for a suciently large B.  Proof of Theorem 4: We rst prove that for a single decision tree, there is a high probability that the number of observations chosen in Step 4 in which x is oered is large. ÃTMore precisely, let Xt = Ifxt=x g. It is easy to see that =1 Xt = kT . Step 4 randomly t selects zT observations out of the T with replacement. Denote the bootstrap sample of fX1;:::, XT } by Y1;:::, YzT . By Hoedingâ€™s inequality, we have the following concentration inequality Ã! zT j=1 Yj kT ô€€€ P ô€€€ 2 exp ô€€€2zT2 (6) zT T for any  > 0. In other words, the bootstrap sample in Step 4 does not deviate too far from the population as long as zT is large. As we choose  < limT !1kT ÂT , it implies 41  Electronic copy available at: https://ssrn.com/abstract=3430886 zTthat Ã j=1 Yj !1and in particular zTÃ• lim P( Yj > lT Âº= 1. (7)T !âˆž j=1 zTNext we show that given Ã j=1 Yj > lT for a decision tree, the leaf node that contains x only contains observations with Yj = 1. That is, the terminal leaf containing x is a single corner of the unit hypercube. If the terminal leaf node containing an observation zT zTwith predictor x, then it has no less than Ã j=1 Yj observations, because all the Ã j=1 Yj samples used to train the tree fall on the same corner in the predictor space. If another observation with a dierent predictor is in the same leaf node, then it contradicts Step 6 in the algorithm, because it would imply that another split could be performed. Suppose fR1;:::, RM gis the nal partition corresponding to the decision tree. As a result, in the region Rj such that x 2Rj , we must have that tbÂ¹xÂºis a random sample from the ÃzT =1 Yj customer choices, according to Step 11. j Now consider the estimated choice probability from the random forest: ÃBT 1 IftbÂ¹xÂº=ig.b=1 BT Note that ftbÂ¹xÂºgbB = T 1 are i.i.d. given the training set. By Hoedingâ€™s inequality, conditional on fÂ¹it , xt ÂºgT t=1, !Ã•BT 1 ô€€€2BT2P IftbÂ¹xÂº=igô€€€PÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgT =1) > 1 fÂ¹it , xt ÂºgT 2e 1 , (8)tt=1BTb=1 for all 1 > 0. Next we analyze the probability PÂ¹tb Â¹x) = ijfÂ¹it , xt ÂºgT =1Âºfor a singlet zTdecision tree. By the previous paragraph, conditional Ã j=1 Yj > lT , the output of a zTsingle tree tbÂ¹xÂºis randomly chosen from the class labels of Ã =1 Yj observations whose j predictor is x. Let Zj be the class label of the jth chosen observation in Step 4. Therefore, zTconditional on the event Ã j=1 Yj > lT and the training data, we have zT zTÃ•Ã• Yj IfZj=igPÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgTt=1 , Yj > lT Âº= Ã . (9)zT j=1 j=1 j=1 Yj 42  Electronic copy available at: https://ssrn.com/abstract=3430886 Because fYj IfZj=iggz = T 1 is a bootstrap sample, having i.i.d. distribution j ÃT t=1 Ifit=i;xt=x gPÂ¹Yj IfZj=i} = 1Âº= T given the training data, we apply Hoedingâ€™s inequality again ÃzT ÃT ! j=1 Yj IfZj=i} t=1 Ifit=i;xt=xgP âˆ’ > 2 fÂ¹it , xt ÂºgTt=1 2 expÂ¹ô€€€2zT22) (10) zT T ÃTfor all 2 > 0. Now applying Hoedingâ€™s inequality to t=1 Ifit=i;xt=x } again, and because of Assumption 1, we have that ÃT ! t=1 Ifit=i;xt=xgP ô€€€PÂ¹ijx) > 3 2 expÂ¹ô€€€2kT32) (11)kT for all 3 > 0. With the above results, we can bound the target quantity !Ã•BT 1P IftbÂ¹xÂº=igô€€€PÂ¹ijx) >  BTb=1" !# Ã•BT 1 = E IftbÂ¹xÂº=igô€€€PÂ¹ijx) >  fÂ¹it , xt ÂºgTP t=1BTb=1" !# Ã• EP BT 1 IftbÂ¹xÂº=igô€€€PÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â2 fÂ¹it , xt ÂºgT tt=1BTb=1  + EPPÂ¹ijxÂºô€€€PÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â2 fÂ¹it , xt ÂºgT tt=1 By (8), the rst term is bounded by 2 expÂ¹ô€€€BT2Â2Âºwhich converges to zero as BT !1. 43  Electronic copy available at: https://ssrn.com/abstract=3430886 To bound the second term, note that  PPÂ¹ijxÂºô€€€PÂ¹tbÂ¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â2 fÂ¹it , xt ÂºgT tt=1 ÃT ! t=1 Ifit=i;xt=xgPPÂ¹ijxÂºâˆ’ > Â6 fÂ¹it , xt ÂºgT t=1kT ÃTzT ! t=1 Ifit=i;xt=x } Ã• Yj IfZj=ig+ P âˆ’ Ã > Â6 fÂ¹it , xt ÂºgT zT t=1kT j=1 Yjj=1 ! zTÃ• Yj IfZj=ig+ P Ã ô€€€PÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â6 fÂ¹it , xt ÂºgT (12)zT tt=1 j=1 j=1 Yj The expected value of the rst term in (12) is bounded by 2 expÂ¹ô€€€kT2Â18) by (11), which converges to zero as kT !1. For the second term of (12), we have that ÃTzT ! t=1 Ifit=i;xt=x} Ã• Yj IfZj=igP âˆ’ Ã > Â6 fÂ¹it , xt ÂºgT t=1kT j=1 zj= T 1 Yj !ÃTzT t=1 Ifit=i;xt=x } Ã• TYjIfZj=igP âˆ’ > Â12 fÂ¹it , xt ÂºgT t=1kT zTkTj=1 ! zT zTÃ• TYj IfZj=i} Ã• Yj IfZj=ig+ P âˆ’ Ã > Â12 fÂ¹it , xt ÂºgT (13)zT t=1 zTkT =1 Yjj=1 j=1 j For the rst term in (13), note that by (9) ÃTzT ! t=1 Ifit=i;xt=x } Ã• TYjIfZj=igP âˆ’ > Â12 fÂ¹it , xt ÂºgT t=1kT zTkTj=1ÃTzT ! t=1 Ifit=i;xt=x} Ã• Yj IfZj=ig= P âˆ’ > kTÂ12 fÂ¹it , xt ÂºgT 2 expÂ¹ô€€€zTk22Â72Âº!0t=1 TT zTj=1 44  Electronic copy available at: https://ssrn.com/abstract=3430886 as T !1. For the second term in (13), we have ! zT zTÃ• TYj IfZj=i} Ã• Yj IfZj=igP âˆ’ Ã > Â12 fÂ¹it , xt ÂºgT zT t=1 zTkTj=1 j=1 j=1 YjÃ!zT j=1 Yj IfZj=i} T ÃzTP âˆ’ > Â12 fÂ¹it , xt ÂºgT zT t=1 zTkT j=1 Yj ! T ÃzTP âˆ’ > Â12 fÂ¹it , xt ÂºgT zT t=1kT j=1 Yj Ã! zT TzTkT j=1 Yj = P Ã âˆ’ > Â12 fÂ¹it , xt ÂºgT zT t=1kT =1 YjT zTj TzTIt is easy to see that ÃzT converges almost surely to a constant asT !1. Therefore, kT =1 Yjjby (6) the last term converges to zero. Finally we move on to the third term of (12). By (9), we have ! zTÃ• YjIfZj=igP Ã ô€€€PÂ¹tbÂ¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â6 fÂ¹it , xt ÂºgT zT tt=1 j=1 j=1 Yj ! zTÃ• = PPÂ¹tbÂ¹xÂº= ijfÂ¹it , xt ÂºgT =1, Yj > lT Âºô€€€PÂ¹tb Â¹xÂº= ijfÂ¹it , xt ÂºgT =1) > Â6 fÂ¹it , xt ÂºgT t tt=1 j=1!! zTÃ• P2P Yj lT fÂ¹it , xt ÂºgT > Â6 fÂ¹it , xt ÂºgT :t=1 t=1 j=1 Note that we are focusing on a xed-design case, and fYj gand fit gare independent given Assumption 1. Therefore, !! zT zTÃ•Ã• P Yj lT fÂ¹it , xt ÂºgTt=1 = P Yj lT !0 j=1 j=1 by (7). This completes the proof. Proof of Proposition 1: We need to show that the bth tree constructed by the algorithm of both link functions returns the same partition (in the sense that each region contains the same set of observations in the training data) of the predictor space Â»0, 1Â¼N and the 45  Electronic copy available at: https://ssrn.com/abstract=3430886 same class labels in each region/leaf. The class labels are guaranteed to be the same because we control the internal randomizer in Step 12. To show the partitions are the same, it suces to show that each split creates regions that are identical for the two link functions in the sense that the resulting regions contain the same set of observations. We will use induction to prove this claim. Before the construction of the bth tree, because the internal randomizers in Step 5 are equalized, the root node Â»0, 1Â¼N for both link functions contains the same set of observations. Now focusing on a leaf node in the middle of constructing the bth tree Â¹jÂºÂ¹jÂºÂ¹jÂºÂ¹jÂºfor both link functions. We use Â»l , u1 Â¼Â»lN , u Â¼Â»0, 1Â¼N to denote the region 1 N of the leaf node for link functions j = 1, 2. By the inductive hypothesis, both regions contain the same set of observations. Without loss of generality, we assume that the regions contain fg1Â¹pt ÂºgT = 1 1 and fg2Â¹pt ÂºgT = 1 1, respectively. After Step 8, the same set of tt candidate splitting dimensions are selected. To show that Step 9 results in the same split in the two regions, consider a given split direction m and split point xj for j = 1, 2. If Â¹1ÂºÂ¹1ÂºÂ¹1ÂºÂ¹1ÂºÂ¹1ÂºÂ¹2ÂºÂ¹2ÂºÂ¹2ÂºÂ¹2ÂºÂ¹2ÂºÂ»l , u Â¼::. Â»l , xÂ¹1ÂºÂ¼Â»l , u Â¼and Â»l , u Â¼::. Â»l , xÂ¹2ÂºÂ¼Â»l , u Â¼m11 NN 11 m NN contain the same set of observations, i.e., for t = 1;:::;T1 Â¹1ÂºÂ¹1ÂºÂ¹1ÂºÂ¹1ÂºÂ¹1Âºg1Â¹pt Âº2Â»l , u Â¼::. Â»l , xÂ¹1ÂºÂ¼Â»l , u Â¼11 m NN Â¹2ÂºÂ¹2ÂºÂ¹2ÂºÂ¹2ÂºÂ¹2Âº()g2Â¹pt Âº2Â»l , u Â¼::. Â»l , xÂ¹2ÂºÂ¼Â»l , u Â¼;11 m NN then the Gini indices resulting from the splits are equal for the two link functions. This is because the Gini index only depends on the class composition in a region instead of the locations of the predictors, and the splits above lead to the same class composition in the sub-regions. This implies that in Step 8, both trees are going to nd the optimal splits that lead to the same division of training data in the sub-regions. By induction and the recursive nature of the tree construction, Algorithm 2 outputs the same partition in the bth tree for both link functions, i.e., the training data is partitioned equally. This completes the proof. Proof of Theorem 5: To simplify the notation, we use product N + 1 to denote the no-purchase option. Let nj denote the number of assortments in the training data where product j is in the assortment. Clearly nj has binomial distribution BÂ¹T , 1Â2Âº. Let nj denote the number of assortments in the training set where product j is in the k 46  Electronic copy available at: https://ssrn.com/abstract=3430886 assortment and product k is chosen. Let n ô€€€j denote the number of assortments in thek training set where product j is not in the assortment and product k is chosen. The variable we dened above follows the following binomial distribution when 1 j N : 8 BÂ¹T , 1Â2k+1) 1 k < j8 BÂ¹T , 1Â2k+1) 1 k < j j >< ô€€€j >< = 0 k = j n BÂ¹T , 1Â2j) k = j , n :kk BÂ¹T , 1Â2k ) j < k â‰¤ N> = 0 j < k N + 1 >: BÂ¹T , 1Â2N ) k = N + 1: For example, a data point counts as nj when products 1 to k ô€€€1 are not in the assortmentk while product k and j are in the assortment. So the probability is 1Â2k+1. Note thatÃjj Ãjô€€€1 ô€€€j ÃN +1 ô€€€jjwe have = nj and += T ô€€€n . Next we compute thek=1 nk k=1 nk k=j+1 nk Gini index Gj if the rst split is on product j. Recall the denition of the Gini index:Ã tjÃN =0 pË†jk Â¹1 ô€€€pË†jk Âº. If the split is on product j, then the left node (assortmentsRjTk without product j) has nj data points while the right node has T ô€€€nj . In the left node, the empirical frequency of label k, i.e., the fraction of assortments resulting in a purchase ô€€€jof product k, is n ÂÂ¹T ô€€€nj Âº. Similarly, in the right node, the empirical frequency isk jn Ânj for label k â‰¤ j and 0 for label k > j. Therefore, we have thatk jj ô€€€j ô€€€j ô€€€j ô€€€j nj Ã•jn nT ô€€€nj Ã•jô€€€1 n nT ô€€€nj NÃ•+1 nnkkkk kkGj = Â¹1 ô€€€Âº+ Â¹1 ô€€€Âº+ Â¹1 ô€€€Âºjjjj jjTnn T T ô€€€nT ô€€€nT T ô€€€nT ô€€€n k=1 k=1 k=j+1Ãjj Âº2 Ãjô€€€1 ô€€€j Âº2 + ÃN +1 ô€€€j Âº2 ! 1 j âˆ’ k=1Â¹nk j âˆ’ k=1Â¹nk k=j+1Â¹nk = n + T ô€€€njjTn T ô€€€n Ãjj Âº2 Ãjâˆ’ = 11Â¹n ô€€€jÂº2 + ÃN =+ j 1 +1Â¹n ô€€€j Âº2 k=1Â¹nkkk kk = 1 ô€€€âˆ’ (14)Tnj T Â¹T ô€€€nj ) Note that The second item of above equation only depends on assortments with product j, and the third item of above equation only depends on assortments without product j. Next we will show that G1 < Gj with high probability. We dene Hj for the quantity in 47  Electronic copy available at: https://ssrn.com/abstract=3430886 (14) for simplicity: Ã kj =1Â¹nkj Âº2 Ã kjâˆ’ = 11Â¹nk ô€€€j Âº2 + Ã kN =+ j 1 +1Â¹nk ô€€€j Âº2 Hj , + :jjnT ô€€€n Itâ€™s easy to see that G1 < Gj if and only if H1 > Hj . To bound its probability, we introduce the Cherno inequality. The Cherno inequality: Let X1, X2, :::, Xn be independent Bernoulli random nvariables. Denote X = Ã with Î¼ = EÂ»X Â¼. For all 0 <  < 1, we have thati=1 Xi PÂ¹X < Â¹1 ô€€€ÂºÎ¼Âº< expÂ¹ô€€€Î¼22 Âº. Applying the Cherno inequality to the Binomial random variable nj BÂ¹T , 1Â2Âºjjfor 1 â‰¤ j â‰¤ N , we have PÂ¹n < Â¹1 ô€€€ÂºT Â2) = PÂ¹n > Â¹1 + ÂºT Â2) < expÂ¹ô€€€T2Â4Âº. Therefore,  1 2TP nj âˆ’ >< 2 exp âˆ’ (15)2T 2T 4 Next we bound the probability that H1 is small compared to its mean. We canÃN+1 ô€€€1Âº2 rewrite H1 as H1 = n1 + k=2 Â¹n 1 k . Conditional on n1, nô€€€1 âˆ¼ BÂ¹T ô€€€n1 , 1Â2) andT ô€€€n 2âˆš ô€€€11n BÂ¹T ô€€€n , 1Â4Âº. Dene 1 , / 1 ô€€€. By the Cherno inequality we have: 3   1 âˆš 1 âˆš 12Â¹T ô€€€n1Âºô€€€11 ô€€€11PÂ¹n2 Âº2 < 4Â¹1 âˆ’ 21Âº2Â¹T ô€€€n 1Âº2 n = Pn < 2Â¹1 âˆ’ 21ÂºÂ¹T ô€€€n 1Âºn < exp ô€€€2 2   11Âº21 12Â¹T ô€€€n1Âºô€€€11 ô€€€11PÂ¹n3 Âº2 < n = Pn < 4Â¹1 ô€€€21ÂºÂ¹T ô€€€n 1Âºn < exp ô€€€316Â¹1 ô€€€21Âº2Â¹T ô€€€n 2 ô€€€1Since Â¹n Âº2 0 for all k 4, we have: k !NÃ•+1 1 âˆš 1ô€€€11P Â¹nk Âº2 < 4Â¹1 âˆ’ 21Âº2 + Â¹T ô€€€n 1Âº2 n16Â¹1 ô€€€21Âº2 k=2 1 âˆš 1ô€€€1 ô€€€11< PÂ¹n2 Âº2 + Â¹n3 Âº2 < 4Â¹1 âˆ’ 21Âº2 + Â¹T ô€€€n 1Âº2 n16Â¹1 ô€€€21Âº2  1 âˆš 1ô€€€11 ô€€€11< PÂ¹n2 Âº2 < 4Â¹1 âˆ’ 21Âº2Â¹T ô€€€n 1Âº2 n + PÂ¹n3 Âº2 < 1Âº2 n16Â¹1 ô€€€21Âº2Â¹T ô€€€n  12Â¹T ô€€€n1Âº< 2 exp âˆ’ 2 48  Electronic copy available at: https://ssrn.com/abstract=3430886 Therefore,  1 âˆš 111PH1 < n + 4Â¹1 âˆ’ 21Âº2 + Â¹T ô€€€n 1Âºn16Â¹1 ô€€€21Âº2 !NÃ•+1 1 âˆš 1ô€€€11 = P Â¹nk Âº2 < 4Â¹1 âˆ’ 21Âº2 + Â¹T ô€€€n 1Âº2 n16Â¹1 ô€€€21Âº2 k=2 12Â¹T ô€€€n1Âº< 2 exp âˆ’ (16)2 Combining with (15), we have that the unconditional probability can also be bounded:   Â¹1 ô€€€ÂºT 1 âˆš 1 Â¹1 + ÂºTPH1 < + 4Â¹1 âˆ’ 21Âº2 +2 16Â¹1 ô€€€21Âº221  < P n 1 âˆ’ >2T 2T  Ã• Â¹1 ô€€€ÂºT 1 âˆš 1 Â¹1 + ÂºT11+ PÂ¹n = kÂºPH1 < + 4Â¹1 âˆ’ 21Âº2 + n = k2 16Â¹1 ô€€€21Âº22 k:jkô€€€T Â2j<T Â2  2T Ã• 12Â¹T ô€€€kÂº1< 2 exp âˆ’ + PÂ¹n = kÂº2 exp ô€€€42 k:jkô€€€T Â2j<T Â2  2T12Â¹1 ô€€€ÂºT2T < 2 exp âˆ’ + 2 exp âˆ’ = 4 exp âˆ’ . (17)44 4 Next we will bound the probability that H2 is large compared to its mean. Recall that Â¹n21Âº2 + Â¹n22Âº2 Â¹nâˆ’ 12Âº2 + ÃN =+ 31Â¹nô€€€2Âº2 kkH2 = 2 + 2 (18) nT ô€€€n 222 2For the rst item in (18), conditional on n2, both n1 and n = n2 ô€€€n1 follows BÂ¹n , 1Â2Âº.2 Therefore, by the Cherno bound, we have !âˆš  22 212 222 1n P n1 âˆ’ 2n > 2 1nn < 2 exp âˆ’ 2 49  Electronic copy available at: https://ssrn.com/abstract=3430886 Moreover, we have 122 2PÂ¹n1Âº2 + Â¹n 2 ô€€€n1Âº2 > 2Â¹1 + 212ÂºÂ¹n 2Âº2 n 1 1122 = P 2Â¹n 2Âº2 + 2Â¹n1 âˆ’ 2n 2Âº2 > 2Â¹1 + 212ÂºÂ¹n 2Âº2 n !âˆš  22 212 222 1n = P n1 âˆ’ 2n > 2 1nn < 2 exp âˆ’ . (19)2 where the rst equality above follows from Â¹n21Âº2 + Â¹n2 ô€€€n21Âº2 = 12 Â¹n2Âº2 + 2Â¹n21 âˆ’ 12n2Âº2. ô€€€22For the second item in (18), conditional on n2, we have n BÂ¹T ô€€€n , 1Â2Âº, T ô€€€1 ô€€€22 ô€€€22 ô€€€2 ô€€€22n2 ô€€€n BÂ¹T ô€€€n , 1Â2Âº, n BÂ¹T ô€€€n , 1Â4Âºand T ô€€€n2 ô€€€n ô€€€n BÂ¹T ô€€€n , 1Â4Âº.13 13 By the Cherno bound we have:  1 12Â¹T ô€€€n2Âºô€€€22Pn < 4Â¹1 ô€€€21ÂºÂ¹T ô€€€n 2Âºn < exp ô€€€3 2  1 12Â¹T ô€€€n2Âºô€€€2 ô€€€22PT ô€€€n 2 ô€€€n ô€€€n < 4Â¹1 ô€€€21ÂºÂ¹T ô€€€n 2Âºn < exp ô€€€13 2 From the above two equations we have  1 12Â¹T ô€€€n2Âºô€€€2 ô€€€2 ô€€€22Pn3 Â¹T ô€€€n 2 ô€€€n ô€€€n3 Âº< 2Âº2 n < 2 exp ô€€€1 16Â¹1 ô€€€21Âº2Â¹T ô€€€n 2 Similar to (19) we also have  12Âº2 12Â¹T ô€€€n2Âºô€€€2 ô€€€22PÂ¹n1 Âº2 + Â¹T ô€€€n 2 ô€€€n1 Âº2 > 2Â¹1 + 212ÂºÂ¹T ô€€€nn < 2 exp âˆ’ 2 50  Electronic copy available at: https://ssrn.com/abstract=3430886 Combining the above two inequalities we have !NÃ•+1 11ô€€€2 ô€€€2 ô€€€22PÂ¹n1 Âº2 + Â¹n3 Âº2 + Â¹n Âº2 > 2Â¹1 + 212Âºâˆ’ 8Â¹1 ô€€€21Âº2 Â¹T ô€€€n 2Âº2 nk k=4 11ô€€€2 ô€€€2 ô€€€2 ô€€€22< PÂ¹n1 Âº2 + Â¹n3 Âº2 + Â¹T ô€€€n 2 ô€€€n ô€€€n3 Âº2 > 2Â¹1 + 212Âºâˆ’ 8Â¹1 ô€€€21Âº2 Â¹T ô€€€n 2Âº2 n1  11ô€€€2 ô€€€2 ô€€€2 ô€€€2 ô€€€22 = PÂ¹n1 Âº2 + Â¹T ô€€€n 2 ô€€€n1 Âº2 ô€€€2n3 Â¹T ô€€€n 2 ô€€€n ô€€€n3 Âº> 2Â¹1 + 212Âºâˆ’ 8Â¹1 ô€€€21Âº2 Â¹T ô€€€n 2Âº2 n1 1ô€€€2 ô€€€22< PÂ¹n1 Âº2 + Â¹T ô€€€n 2 ô€€€n1 Âº2 > 2Â¹1 + 212ÂºÂ¹T ô€€€n 2Âº2 n 1ô€€€2 ô€€€2 ô€€€22+ Pn3 Â¹T ô€€€n 2 ô€€€n ô€€€n3 Âº< 2Âº2 n1 16Â¹1 ô€€€21Âº2Â¹T ô€€€n    12Â¹T ô€€€n2) 12Â¹T ô€€€n2Âº< 2 exp âˆ’ + 2 exp ô€€€22  12Â¹T ô€€€n2Âº= 4 exp âˆ’ (20)2 The rst inequality follows from Ã kN =+ 41 nk ô€€€2 = T ô€€€n2 ô€€€n1 ô€€€2 ô€€€nâˆ’ 3 2 and thus Ã kN =+ 41Â¹nk ô€€€2Âº2 â‰¤ ô€€€2 ô€€€2 ô€€€2 ô€€€2 ô€€€2Â¹T ô€€€n2 ô€€€n ô€€€n3 Âº2. The rst equality follows from Â¹n3 Âº2 + Â¹T ô€€€n2 ô€€€n ô€€€n3 Âº2 =11 Â¹T ô€€€n2 ô€€€nâˆ’ 12Âº2 ô€€€2nâˆ’ 32Â¹T ô€€€n2 ô€€€nô€€€2 ô€€€nâˆ’ 32Âº.1 Combine (19) and (20) we have 11 2PH2 > 2Â¹1 + 212ÂºT âˆ’ 8Â¹1 ô€€€21Âº2Â¹T ô€€€n 2Âºn 22Â¹n1Âº2 + Â¹n2 ô€€€n1Âº21 22< P > 2Â¹1 + 212Âºnn n2 ! Â¹n1 ô€€€2Âº2 + Â¹nâˆ’ 32Âº2 + Ã kN =+ 41Â¹nk ô€€€2Âº2 11  2+ P > 2Â¹1 + 212Âºâˆ’ 8Â¹1 ô€€€21Âº2 Â¹T ô€€€n 2Âºn2T ô€€€n 122 2 = PÂ¹n1Âº2 + Â¹n 2 ô€€€n1Âº2 > 2Â¹1 + 212ÂºÂ¹n 2Âº2 n !NÃ•+1 11ô€€€2 ô€€€2 ô€€€22+ PÂ¹n1 Âº2 + Â¹n3 Âº2 + Â¹nk Âº2 > 2Â¹1 + 212Âºâˆ’ 8Â¹1 ô€€€21Âº2 Â¹T ô€€€n 2Âº2 n k=4  22 22Âº1n 1 Â¹T ô€€€n < 2 exp âˆ’ + 4 exp âˆ’ . (21)22 51  Electronic copy available at: https://ssrn.com/abstract=3430886 Next we bound the unconditional probability based on the conditional probability. We have 11PH2 > 2Â¹1 + 212ÂºT âˆ’ 8Â¹1 ô€€€21Âº2 Â¹1 ô€€€ÂºT 21  < P n 2 âˆ’ >2T 2T Ã• 11 + PÂ¹n 2 = kÂºPH2 > 2Â¹1 + 212ÂºT âˆ’ 8Â¹1 ô€€€21Âº2 Â¹1 ô€€€ÂºTn 2 = k2 k:jkô€€€T Â2jT Â2   2T Ã• 12k12Â¹T ô€€€kÂº2< 2 exp âˆ’ + PÂ¹n = k) 2 exp âˆ’ + 4 exp ô€€€4 22 k:jkô€€€T Â2jT Â2 Ã•2T12Â¹1 ô€€€ÂºT22 exp âˆ’ + PÂ¹n = kÂº6 exp ô€€€44 k:jkô€€€T Â2jT Â2  2T12Â¹1 ô€€€ÂºT2T = 2 exp âˆ’ + 6 exp âˆ’ = 8 exp âˆ’ . (22)44 4 Next we choose a proper value for . By inequality (17) and (22), we want to nd  such that with high probability, we have pÂ¹1 ô€€€ÂºT 11 Â¹1 + ÂºT H1 â‰¥ + 4Â¹1 âˆ’ 21Âº2 +2 16Â¹1 ô€€€21Âº22 11 > 2Â¹1 + 212ÂºT âˆ’ 8Â¹1 ô€€€21Âº2 Â¹1 ô€€€ÂºT H22 pwhere 1 = / 1 ô€€€. We also have the constraint that 0 < 21 < 1, which is equivalent âˆš to 0 <  < 17ô€€€1 â‰ˆ 0:39. Solving the above inequality for 0 <  < 0:39 we have 2 0 <  0:166185. Let  = 0:166185. Then 4Â2 145. Plugging into (17) and (22), we have  TPÂ¹H1 < 0:512041T Âº< 4 exp âˆ’ (23)145 TPÂ¹H2 > 0:512041T Âº< 8 exp âˆ’ (24)145 52  Electronic copy available at: https://ssrn.com/abstract=3430886 Therefore TPÂ¹H1 < H2Âº< 12 expÂ¹ô€€€145Âº. This implies that G1 < G2 with high probability. Note that the probability bound in above equation doesnâ€™t depend on N . Next we consider j 3. Recall that Ãjj Ãjô€€€1 ô€€€j ÃN +1 ô€€€j k=1Â¹nk Âº2 k=1Â¹nk Âº2 + k=j+1Â¹nk Âº2 Hj = j + j (25) nT ô€€€n Consider some 2 > 0. From (15) we have   21 22T P nj âˆ’ >< 2 exp âˆ’ (26)2T 2 T 4 ô€€€jjWe investigate the second item of (25). Conditional nj , we have n BÂ¹T ô€€€n , 1Â2Âº,1 ô€€€jj ô€€€jj ô€€€j ô€€€jjT ô€€€nj ô€€€n BÂ¹T ô€€€n , 1Â2Âº, n BÂ¹T ô€€€n , 1Â4Âºand T ô€€€nj ô€€€n ô€€€n BÂ¹T ô€€€n , 1Â4Âº.12 12pDene 3 , 2/ 1 ô€€€2. Then similar to (20), we have jô€€€1 NÃ•+1 Ã•Â© ô€€€j ô€€€j ô€€€j ô€€€j 11 j Âª PÂ­Â¹n1 Âº2 + Â¹n2 Âº2 + Â¹n Âº2 + Â¹n Âº2 > 2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2 Â¹T ô€€€nj Âº2 n Â®kk k=3 k=j+1Â«Â¬  ô€€€j ô€€€j ô€€€j ô€€€j 11 j< PÂ¹n1 Âº2 + Â¹n2 Âº2 + Â¹T ô€€€nj ô€€€n ô€€€n2 Âº2 > 2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2 Â¹T ô€€€nj Âº2 n1  32Â¹T ô€€€nj Âº< 4 exp âˆ’ . (27)2 jj jThen similarly we can bound the rst term of (25) since n1 âˆ¼ BÂ¹n , 1Â2Âº, nj ô€€€n1 âˆ¼ jjj jjjBÂ¹n , 1Â2Âº, n2 BÂ¹n , 1Â4Âºand nj ô€€€n1 ô€€€n2 BÂ¹T ô€€€n , 1Â4Âº. j  !Ã• j Âº21 1 j Âº2 jP Â¹n > 2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2 Â¹nnk k=1  jj jj 11 j< PÂ¹n1Âº2 + Â¹n2Âº2 + Â¹nj ô€€€n1 ô€€€n2Âº2 > 2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2 Â¹njÂº2 n  2 j < 4 exp âˆ’ 3n . (28)2 53  Electronic copy available at: https://ssrn.com/abstract=3430886 Combining (27) and (28), we have 11 jPHj > Â»2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2Â¼Tn !jÃ• j 11 j< P Â¹n Âº2 > Â»2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2Â¼Â¹nj Âº2 nk k=1 jô€€€1Ã• NÃ•+1 + PÂ©Â­Â¹n ô€€€jÂº2 + Â¹n ô€€€j Âº2 > [ 12Â¹1 + 232Âºâˆ’ 18Â¹1 ô€€€23Âº2Â¼Â¹T ô€€€njÂº2 nj ÂªÂ®kk k=1 k=j+1Â«Â¬   2 j2 j Âº3n 3 Â¹T ô€€€n < 4 exp âˆ’ + 4 exp âˆ’ (29)22 Using a similar argument, we can bound the unconditional probability:  11 22T PHj > Â»2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2Â¼T < 10 exp âˆ’ . (30)4 Similarly we can calculate 2. By (30) and (23), we have the following condition for 2 11 2Â¹1 + 232Âºâˆ’ 8Â¹1 ô€€€23Âº2 < 0:512041, (31) pwhere 3 = 2/ 1 ô€€€2. Again when we consider 0 < 2 < 0:39, the above inequality is equivalent to 0 < 2 < 0:200261. Let 2 = 0:200261, then 4Â22 99:74, so we have for all j 3 ô€€€ TPHj > 0:512041T < 10 exp âˆ’ (32)100 Now note that ô€€€ PÂ¹rst split not on product 1Âº= PG1 > minfGj j2 â‰¤ j N gÃ•ô€€€ N ô€€€ = PH1 < maxfHj j2 â‰¤ j N } < PÂ¹H1 < 0:512041T Âº+ PHj > 0:512041T j=2  2T2T 2T < 4 exp âˆ’ + 8 exp âˆ’ + Â¹N ô€€€2Âº10 exp âˆ’ 2 44 4  TT = 12 exp âˆ’ + 10Â¹N ô€€€2Âºexp âˆ’ (33)145 100 54  Electronic copy available at: https://ssrn.com/abstract=3430886 Next we are going to show that the probability of the second split on product 2, third split on product 3 and so on, can be bounded by union bound. Let T denote the training set. Dene a sequence of subsets of T as follows: Ti , fS 2Tj1, 2, :::, i ô€€€1 < Sg. That is, Ti only contains assortments that include a subset of fi, i + 1;:::, N g. Let Ti , jTi jdenote the cardinality of set Ti . Note that T1 = T and T1 = T . for 2 i N , since Ti BÂ¹T , 1Â2iô€€€1Âº, by the Cherno inequality we have  2 PTi < Â¹1 ô€€€0Âº2i 1 ô€€€1T < expÂ¹âˆ’ 0T Âº, (34)2i where 0 < 0 < 1. Â¯Dene the event Ai , fproduct i is the best split for training set Ti g, and let Ai denote the complement of event Ai . Then conditional onTi , we can bound the probability of event A Â¯ i from (33) (here we only have N ô€€€i + 1 products):  ô€€€ Â¯ Ti TiPAi Ti Âº< 12 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp âˆ’ (35)145 100 The unconditional probability can be bounded by combining (34) and (35):  Ã• 1  PAi < PTi > Â¹1 ô€€€0Âº2iô€€€1T + PÂ¹Ti = kÂºPAi jTi = k k:kÂ¹1ô€€€0ÂºT Â2iô€€€1    2 Ã•0T kk < expÂ¹âˆ’ Âº+ PÂ¹Ti = k) 12 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp ô€€€2i 145 100 k:k<Â¹1ô€€€0ÂºT Â2iô€€€1   02T Â¹1 ô€€€0ÂºT Â¹1 ô€€€0ÂºT â‰¤ expÂ¹âˆ’ Âº+ 12 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp âˆ’ (36)2i 145 2iô€€€1 100 2iô€€€1 0T Â¹1ô€€€0ÂºTSolving the equation  22 i = 1452iô€€€1, we get 0 0:1107. Then we have  ô€€€ Â¯  TTPAi 13 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp ô€€€164 2iô€€€1 113 2iô€€€1 If all the events A1, A2, :::, Am happen, we can get right split for the rst m step. That is, the rst split is on product 1, the second split is on product 2, ..., the mth split is on 55  Electronic copy available at: https://ssrn.com/abstract=3430886 product m. We can bound the probability by the union bound: ô€€€ ô€€€ P\mi=1Ai = 1 ô€€€P[mi=1A Â¯ i mÃ• Â¯1 âˆ’ PÂ¹Ai Âºi=1   mÃ• TT 1 âˆ’ 13 exp âˆ’ + 10Â¹N ô€€€i ô€€€1Âºexp âˆ’ (37)164 2iô€€€1 113 2iô€€€1 i=1 If the rst m splits match the products, then the assortments including at least one product among f1;:::, mgcan be correctly classied. Therefore, with probability at ô€€€least P\mi=1Ai , we can accurately predict at least a fraction 1 ô€€€1Â2m of assortments. Given  > 0, letting m = dlog2 1 ecompletes the proof.  B. SampleCode 56  Electronic copy available at: https://ssrn.com/abstract=3430886 