The Use of Binary Choice Forests to Model and Estimate Discrete Choices Ningyuan Chen*1, Guillermo Gallego‚Ä†2, and Zhuodong Tang‚Ä°2 1Department of Management, University of Toronto Mississauga 2Department of Industrial Engineering & Decision Analytics Hong Kong University of Science and Technology Abstract We show the equivalence of discrete choice models and the class of binary choice forests, which are random forests based on binary choice trees. This suggests that standard machine learning techniques based on random forests can serve to estimate discrete choice models with an interpretable output. This is conrmed by our data-driven theoretical results which show that random forests can predict the choice probability of any discrete choice model consistently, with its splitting criterion capable of recovering preference rank lists. The framework has unique advantages: it can capture behavioral patterns such as irrationality or sequential searches; it handles nonstandard formats of training data that result from aggregation; it can measure product importance based on how frequently a random customer would make decisions depending on the presence of the product; it can also incorporate price information and customer features. Our numerical results show that using random forests to estimate customer choices represented by binary choice forests can outperform the best parametric models in synthetic and real datasets. *ningyuan.chen@utoronto.ca ‚Ä†ggallego@ust.hk‚Ä°ztangai@connect.ust.hk 1  Electronic copy available at: https://ssrn.com/abstract=3430886 1. Introduction Being able to understand consumers‚Äô choice behavior when they are oered an assortment of products provides rms with unique advantages. It is particularly important in the modern era: online retailers that predict consumers‚Äô choice behavior more accurately can implement more eective strategies and earn higher prots. In turn, they can aord to invest in advanced technologies and infrastructure, and sharpen their prediction of consumers‚Äô behavior. The unstoppable cycle has created a few unprecedented market juggernauts such as Amazon. Firms unwilling or incapable of getting inside the mind of their consumers are left behind. Not surprisingly, discrete choice models (DCM) have become one of the central topics in revenue management and pricing analytics. To understand and predict consumers‚Äô choice behavior, academics and practitioners have proposed several frameworks, some of which are widely adopted in industry, One ubiquitous framework is model-then-estimate. In this framework, a parametric DCM is proposed to explain how a customer chooses a product when oered an assortment. The parameters are then estimated using historical data. Once the model has been estimated properly, it can then be used as a workhorse to predict the choice behavior of future consumers. In the model-then-estimate framework, there is a trade-o between the exibility and accuracy. A exible DCM incorporates a wide range of patterns of consumers‚Äô behavior, but it may be dicult to estimate and may overt training data. A parsimonious model, may fail to capture consumers behavior, and even if estimated correctly it would be misspecied. The goal is to reach a delicate balance between exibility and predictability even relative to assortments never seen before. Not surprisingly, it is not straightforward to nd the ‚Äúsweet spot‚Äù when selecting among the large class of parametric DCMs. Another framework favored by data scientists is estimate-without-models. Advanced machine learning algorithms are applied to historical sales data, and used to predict future choice behavior. The framework skips ‚Äúmodeling‚Äù entirely and does not attempt to understand the rationality (or irrationality) hidden behind the patterns observed in the training data. With engineering tweaks, the algorithms can be implemented eciently and capture a wide range of choice behavior. For example, neural networks are known to be able to approximate any continuous functions. This approach 2  Electronic copy available at: https://ssrn.com/abstract=3430886 may sound appealing: if the algorithm achieves impressive accuracy when predicting the choice behavior of new consumers, why do we care about the actual rationale behind consumers when they make choices? There are two reasons to care. First, the rm may be interested in not only making accurate predictions, but also other goals such as nding the optimal assortment that maximizes the expected revenue. Without a proper model, it is unclear if the goal can be formulated as an optimization problem. Second, when the market environment or customer preferences change systematically over time, having a reasonable model provide a certain degree of generalizability while black-box algorithms may fail to capture an obvious pattern just because the pattern has not appeared frequently in the past. In this paper, we introduce a data-driven framework which we call estimate-andmodel that combines machine learning with DCMs, and thus retains the strengths of both frameworks mentioned previously. The model we propose, binary choice forests, is a mixture of binary trees, each of which mimics the internal decision-making process of a customer. We show that the binary choice forest can be used to approximate any DCM, and is thus suciently exible, but still identiable with training data of reasonable size. Moreover, it can be eciently estimated using random forests (Breiman, 2001), a popular machine learning technique that has stood the test of time. Random forests are easy to implement using R or Python (Pedregosa et al., 2011; Liaw and Wiener, 2002) and have been shown to have extraordinary predictive power in practice. We provide theoretical guarantees: as the sample size increases, random forests can successfully recover the binary choice forest, and thus any DCM. Moreover, the splitting criterion used by the random forests is intrinsically connected to the preference rank list of customers. As a contribution to the literature, the framework we propose has the following practical advantages: ‚Ä¢It can capture various patterns of customer behavior that cannot be easily captured by other models, such as irregularity and sequential searches (Weitzman, 1979). See Section 5.1 for more details. ‚Ä¢It can deal with nonstandard formats of historical data, which is a major challenge in practice. See Section 5.2 for more details. ‚Ä¢It can return an importance index for all products, based on how frequently 3  Electronic copy available at: https://ssrn.com/abstract=3430886 a random customer would make decisions depending on the presence of the product. ‚Ä¢It can incorporate the prices of the products and reect the information in the decision-making of consumers. ‚Ä¢It can naturally incorporate customer features and is compatible with personalized online retailing. 1.1. LiteratureReview We rst review DCMs proposed in the literature following the model-then-estimate framework, in the order of increasing exibility and diculty in terms of estimation. The independent demand model and the MNL model (McFadden, 1973) have very few parameters (one per product), which are easy to estimate (Train, 2009). Although the MNL model is still widely used, its inherent property of independence of irrelevant alternatives (IIA) has been criticized for being unrealistic (see Anderson et al. (1992) for more details). The mixed logit model, the nested logit model, the Markov chain DCM, and the rank-based DCM (see, e.g., Williams (1977); Train (2009); Farias et al. (2013); Blanchet et al. (2016)) are able to capture much more complex choice behavior than the MNL model. In fact, the mixed logit model and the rank-based DCM can approximate any random utility model (RUM), encompassing a very general class of DCMs. The estimation of these models is challenging, but there has been exciting progress in recent years (Farias et al., 2013; van Ryzin and Vulcano, 2014, 2017; Simsek and Topaloglu, 2018; Jagabathula et al., 2019). However, the computational feasibility and the susceptibility to overtting remain a challenge in practice. Even the general class of RUM cannot capture certain choice behavioral. A RUM possesses the so-called regularity property: the probability of choosing an alternative cannot increase if the oered set is enlarged. There are a few experimental studies showing strong evidence that regularity may be violated (Simonson and Tversky, 1992). Several models are proposed to capture even more general behavior than RUM (Natarajan et al., 2009; Flores et al., 2017; Berbeglia, 2019; Feng et al., 2017). It is unclear if the estimation for such models can be done eciently. The specications of random forests used in this paper are introduced by Breiman 4  Electronic copy available at: https://ssrn.com/abstract=3430886 (2001), although many of the ideas were discovered even earlier. The readers may refer to Hastie et al. (2009) for a general introduction. Although random forests have been very successful in practice, little is known about their theoretical properties. To date, most studies are focused on isolated setups or simplied versions of the procedure. In a recent study, Scornet et al. (2015) establish the consistency of random forests in regression problems, under less restrictive assumptions. Biau and Scornet (2016) provide an excellent survey of the recent theoretical and methodological developments in the eld. A recent paper by Chen and Mi≈°ic (2019) proposes a similar tree-based DCM. They show that their ‚Äúdecision forest‚Äù can approximate any DCMs with arbitrary precision; a similar result is proved with a dierent approach in this paper. Our studies dier substantially in the estimation step: we focus on random forests, while Chen and Mi≈°ic (2019) follow an optimization approach based on column generation ideas for estimation. Moreover, we establish the consistency of random forests, and show that the estimation can accommodate the price information and aggregate choice data. In our numerical study, we nd that random forests are quite robust and have a good performance even compared with the Markov chain model estimated using the expectation-maximization (EM) algorithm, which has been shown to have outstanding empirical performance compared to MNL, the nested logit, the mixed logit and rank-based DCM (Berbeglia et al., 2018), especially when the training data is large. Our algorithm runs 17 times faster than the EM algorithm. In contrast, the computational study in Chen and Mi≈°ic (2019) is limited to the rank-based model estimated by column generation (van Ryzin and Vulcano, 2014), which is shown to be outperformed by the Markov chain model (Berbeglia et al., 2018). 2. ChoiceModelsandMixtureofBinaryTrees Consider a set ¬ªN ] , f1;:::, N gof N products and dene ¬ªN ¬º+ , ¬ªN ¬º[f0gwhere 0 represents the no-purchase option. Let x 2f0, 1gN be a binary vector representing an assortment of products, where x¬πi) = 1 indicates product i is in the assortment and x¬πi¬∫= 0 otherwise. A discrete choice model (DCM) is a non-negative mapping 5  Electronic copy available at: https://ssrn.com/abstract=3430886 Product 2 S0 S1 S2 YN 0 1 Product 1 x¬π1¬∫0:5 1 x¬π2¬∫0:5 2 0 Figure 1: A binary tree representation of the partition. p¬πi, x¬∫, ¬ªN ¬º+ f0, 1gN 7!¬ª0, 1¬ºsuch that √ï p¬πi, x¬∫= 1, p¬πi, x¬∫= 0 if x¬πi¬∫= 0. i2¬ªN ¬º+ It is clear that p¬πi, x¬∫represents the probability of a random customer choosing product i when presented the assortment x. We refer to a subset S of ¬ªN ¬ºas an assortment associated with x 2f0, 1gN , i.e., i 2S if and only if x¬πi¬∫= 1. Without ambiguity, we will write p¬πi, S¬∫instead of p¬πi, x¬∫. A binary decision tree t¬πx¬∫maps x 2f0, 1gN into ¬ªN ¬º+. More precisely, it species a partition of the space f0, 1gN , fSi , i 2¬ªN ¬º+g, and assigns label i 2¬ªN ¬º+ to region Si , so t¬πx) = √ç i Ifx2Sig. Some of the regions in the partition may be empty. We i2¬ªN ¬º+ refer to the partition as a binary decision tree because any partition of f0, 1gN can be obtained by sequentially splitting the space along N dimensions. For example, a decision tree representation of a partition when N = 2 is demonstrated in Figure 1. A binary decision forest is dened as a convex combination of multiple binary decision trees. More precisely, a binary decision forest can be written as B√ï f ¬πi, x¬∫= wb Iftb¬πx¬∫=igb=1 where the tb¬πx) and wb are, respectively decision trees, and non-negative weights 6  Electronic copy available at: https://ssrn.com/abstract=3430886 summing up to one. Notice that a decision forest maps ¬ªN ¬º+ f0, 1gN 7!¬ª0, 1¬ºjust like DCMs do. Yet decision forest are not necessarily DCMs because tb ¬πx¬∫may be equal to i even if x¬πi¬∫= 0. A binary decision tree t¬πx¬∫is a binary choice tree if t¬πx) = i only if x¬πi¬∫= 1. A binary decision forest is a binary choice forest (BCF) if it is a convex combination of binary choice trees. A BCF can be interpreted as decisions made by B consumer types, with consumers of type b having weight wb and making decisions based on binary choice tree tb¬πx¬∫. If f ¬πi, x¬∫is a BCF, then f is also a DCM. This is because f is non-negative, √ç i2¬ªN ¬º+ f ¬πi, x¬∫= 1 and f ¬πi, x¬∫= 0 if x¬πi¬∫= 0. To see that the converse is also true, we will rst show that DCMs are closed under convex combinations and that any DCM is in the convex hull of extreme DCMs. We next argue that the extreme DCMs are the deterministic DCMs that assign S to a particular choice i¬πS¬∫2S+ with probability one for every S ¬ªN ¬º. The next step is to show that each extreme DCM can be represented by a binary choice tree concluding that every DCM is a convex combination of choice trees and is thus a BCF. Theorem1.Every BCF is a DCM, and every DCM can be represented as a BCF. One way to interpret this result is that for each DCM there exists a set of weights we, e 2E adding to one, such that p¬πi, S¬∫= √ç e2E wepe ¬πi, S¬∫for all i 2S+, S ‚äÇ N , where the pe ‚Äôs are the extreme deterministic DCMs. Although we can represent every DCM as a BCF, it will be dicult to estimate if we have too many extreme points. The number of extreme points is √éN =1¬πk + 1¬∫¬πNk¬∫fork N products, which increases tremendously as N increases, with more than 6:7 10173 extreme points for N = 8. In the next theorem, we will show that any DCM can be represented as a convex combination of much fewer binary choice trees. Theorem2.Every DCM can be represented as a convex combination of a BCF containing 2N ÙÄÄÄ1at most N ¬∑ + 1 trees. Proof. Carath√©odory‚Äôs theorem states that if a point x of Rd lies in the convex hull of a set P, then x can be written as the convex combination of at most d + 1 points in P. To apply Carath√©odory‚Äôs theorem to DCM, notice that since the choice probabilities sum√çN ÙÄÄÄN to 1, each assortment with cardinality k has dimension of k. We have d == k=1 kk√çNN ! √çN ¬πN ÙÄÄÄ1¬∫!2N ÙÄÄÄ1 k=1 k ¬∑ k!¬πN ÙÄÄÄk¬∫! = N ¬∑ k=1 ¬πkÙÄÄÄ1¬∫!¬πN ÙÄÄÄk¬∫! = N ¬∑ . Therefore, Every DCM can be represented as a convex combination of a BCF containing at most N 2N ÙÄÄÄ1 + 1. 7  Electronic copy available at: https://ssrn.com/abstract=3430886 As an example, for N = 8, d = 1024, so any DCM with N = 8 can be represented by a convex combination of 1025 trees. A recent working paper by Chen and Mi≈°ic (2019) has independently shown, by construction, that any choice model can be represented by a decision forest where each of the trees has depth N + 1. While their proof has the virtue of being constructive, our proof is more succinct and insightful as it shows that DCMs and BCFs are equivalent, and the existence of a solution of much lower dimension. Our result implies that choice forests are capable of explaining some of the pathological cases that do not exhibit regularity and are outside the RUM, including the decoy eect (Ariely, 2008) and the comparison-based choice (Huber et al., 1982). Note also that all RUMs can be modelled as convex combinations of permutation lists, which are special cases of decision trees. 3. DataandEstimation The main goal of this paper is to provide a practical method to estimate DCMs using random forests, which are shown to be able to approximate all BCFs. The numerical recipe for random forests is widely available and implementable. Before proceeding we remark that an alternative approach would be to use column generation starting with a collection of trees and adding additional trees to improve the t to data. This approach has been taken, for example by van Ryzin and Vulcano (2014); Mi≈°ic (2016); Jagabathula and Rusmevichientong (2016) to estimate RUMs by weighted preference rank lists, and a similar approach has been pursued by Chen and Mi≈°ic (2019) for trees. We remark that the output of our model can be fed into a column generation algorithm to seek further improvements although we have not pursued this in our paper. We will assume that arriving consumers make selections independently based on an unknown DCM p¬πi, x¬∫, and that a rm collects data of the form ¬πit , xt ¬∫(or equivalently ¬πit , St ¬∫) where xt was the assortment oered to the tth consumer and it 2St [f0gis the choice made by consumer t = 1;:::;T . Our goal is to use the data to construct a family of binary choice trees as a means to estimate the underlying DCM p¬πi, x¬∫represented by a BCF. We view the problem as a classication problem: given the predictor x, we would like to provide a classier that maps the predictor to a class label i 2¬ªN ¬º+, or the class probabilities. To this end we will use a random forest as a classier. The output of a random forest 8  Electronic copy available at: https://ssrn.com/abstract=3430886 is B individual binary decision trees (CART), ftb¬πx¬∫gB =1, where B is a tunable parameter. b Although a single tree only outputs a class label in each region, the aggregation of the trees, i.e., the forest, is naturally equipped with the class probabilities. Then the choice probability of item i in the assortment x is estimated as B√ï 1 Iftb¬πx¬∫=ig, (1)B b=1 which is a special form of BCF. The next result shows that the random forest can still approximate any DCM. Theorem3.If B is suciently large, then a binary choice forest of the form B√ï 1 f ¬πi, x¬∫= Iftb¬πx¬∫=ig;B b=1 can approximate any DCM. The implication of this result is that we don‚Äôt have to worry about generating all of the extreme points, or deterministic DCMs, and then nding a set of weights wb for each such tree tb ¬πx¬∫. Intuitively, if B is suciently large, then we need approximately Bwb type b customers associated with tree tb with positive weight wb > 0 in the convex combination. We explain how the random forest can be estimated from the historical data by rst reviewing the basic mechanism of CART which preforms recursive binary splitting of the predictor space ¬ª0, 1¬ºN . In each iteration, it selects a dimension i 2¬ªN ¬ºand a split point to split the predictor space. More precisely, the split ¬πi, si ¬∫divides the observations to f¬πit , xt ¬∫: xt ¬πi¬∫si gand f¬πit , xt ¬∫: xt ¬πi¬∫> si g. In our problem, because xt 2f0, 1gN is at the corner of the hypercube, all split points between 0 and 1 create the same partition of the observations and thus we simply set si 0:5. To select the dimension, usually an empirical criterion is optimized to favor splits that create ‚Äúpurer‚Äù regions. That is, the resulting region should contain data points that mostly belong to the same class. We√ç tj√çNuse a common measure called Gini index: =0 pÀÜjk ¬π1 ÙÄÄÄpÀÜjk ¬∫where tj is the number RjTk of observations in region Rj of the partition and pÀÜjk is the empirical frequency of class k in Rj . It is not hard to see that the Gini index takes smaller values when the regions 9  Electronic copy available at: https://ssrn.com/abstract=3430886 contain predominantly observations from a single class. In this case, a dimension is selected that minimizes the measures and the partition is further rened by a binary split. This splitting operation is conducted recursively for the regions in the resulting partition until a stopping rule is met. The main drawback of CART is its tendency to overtting the training data. If a deep decision tree is built (having a large number of splits), then it may t the training data well but introduce large variances when applied to test data. If the tree is pruned and only has a few leaves (or regions in the predictor space), then it loses the predictive accuracy. Random forests, by creating a number of decision trees and then aggregating them, signicantly improve the power of single trees and moves the bias-variance tradeo toward the favorable direction. The basically idea behind random forests is to ‚Äúshake‚Äù the original training data in various ways in order to create decision trees that are as uncorrelated as possible. Because the decision trees are deliberately ‚Äúdecorrelated‚Äù, they can aord to be deep, as the large variances are remedied by aggregating the ‚Äúalmost independent‚Äù trees. Next we explain the details of random forests. To create B randomized trees, for each b = 1;:::, B, we randomly choose z samples with replacement from the T observations (a bootstrap sample). Only the sub-sample of z observations is used to train the bth decision tree. Splits are performed only on a random subset of ¬ªN ¬ºof size m according to one of the criterion of Gini index. The random sub-sample of training data and random directions to split are two key ingredients in creating less correlated decision trees in the random forest. The depth of the tree is controlled by the minimal number of observations, say l, in a region for the tree to keep splitting. These ideas are subsumed in Algorithm 1. We rst remark on the procedure in Algorithm 1 that can be applied to a generic classication problem and then comment on the special properties in our problem. (1) Many machine learning algorithms such as neural networks have numerous parameters to tune and the performance crucially depends on a suitable choice of parameters. Random forests, on the other hand, have only a few interpretable parameters. Even so, in the numerical studies in this paper, we simply choose a set of parameters that are commonly used for classication problems, without cross-validation or tuning, in order to demonstrate the robustness of the palgorithm. In particularly, we mostly use z = T , m = N and l = 50. There are other alternative options when constructing random forests, such as using a bootstrap 10  Electronic copy available at: https://ssrn.com/abstract=3430886 Algorithm 1 Random forests for DCM estimation 1: Data: f¬πit , xt ¬∫gT t=1 2: Tunable parameters: number of trees B, sub-sample size z 2f1;:::;T g, number of dimensions to split m 2f1;:::, N g, terminal leaf size l 2f1;:::, zg3: for b = 1 to B do 4: Select z observations from the training data with replacement, denoted by Z 5: Initialize the tree tb ¬πx¬∫0 with a single root node 6: while some leaf has greater than or equal to l observations belonging to Z and can be split do 7: Select m variables without replacement among f1;:::, N g8: Select the optimal one to split among the m dimensions that minimizes the Gini index 9: Split the leaf node into two 10: end while 11: Denote the partition corresponding to the leaves of the tree by fR1;:::, RM g; let ci be the class label of a randomly chosen observation in Ri√çM12: Dene tb ¬πx¬∫= i=1 ci Ifx2Rig13: end for 14: The trees ftb¬π¬∫gbB =1 are used to estimate the class probabilities as (1) sample Step 4. For the ease of exposition, we stick to the canonical version presented in Algorithm 1. (2) The numerical recipe for the algorithm is implemented in many programming languages such as R and Python and ready to use. In Section B, we provide a demonstration using scikit-learn, a popular machine learning package in Python that implements random forests, to estimate customer choice. As one can see, it takes less than 20 lines to implement the procedure. Because of the structure of the problem, there are three specic observations. (1) Because the entries of x are binary f0, 1g, the split position of decision trees is always 0:5. Therefore, along a branch of a decision tree, there can be at most one split on a particular dimension, and the depth of a decision tree is at most N . (2) The random forest is a binary decision forest instead of a BCF. In particular, the probability of class i, or the choice probability of product i given assortment x, may be positive even when x¬πi¬∫= 0, i.e., product i is not included in the assortment. To x the issue, we adjust the 11  Electronic copy available at: https://ssrn.com/abstract=3430886 probability of class i by conditioning on the trees that output reasonable class labels: B√ï √ç√ç1 B Iftb¬πx¬∫=i;x¬πi¬∫=1gb=1 j:x¬πj¬∫=1 b=1 Iftb¬πx¬∫=j} (3) When returning the class label of a leaf note in a decision tree, we use a randomly chosen observation instead of taking a majority vote (Step 11 in Algorithm 1). While not being a typical choice, it seems crucial in deriving our consistency result (Theorem 4). Intuitively, unlike other classication problems in which the predictor has a continuous support, in our problem xt are overlapping when an assortment is oered to multiple consumers in the data. A majority vote would favor the choice of product that most consumers make and ignore less attractive products. To correctly recover the choice probability from the data, we randomly choose an observation in the leaf (equivalently, randomly pick a customer t in the data who has been oered the same assortment), which is at least an unbiased estimator for the choice probability. 4. WhyDoRandomForestsWorkWell? Many machine learning algorithms have superb performances in practice, while very few theories can be spelt out on why it is the case. For example, for random forests, even consistency, one of the most fundamental properties a statistician would demand for any classic estimators, was only established recently for regression problems under restrictive assumptions (Scornet et al., 2015). The lack of theoretical understandings can worry practitioners when stakes are high and the failure may have harmful consequences. In this section, we attempt to answer the ‚Äúwhy‚Äù question for our setting from two angles. We show that random forests are consistent for any DCM, and the way that random forests split (Gini index) can naturally help to recover the choice model when it can be represented by a tree. 4.1. RandomForestsareConsistentforAnyChoiceModel We now show that with enough data, random forests can recover the choice probability of any DCM. To obtain our theoretical results, we impose mild assumptions on how the data is generated. 12  Electronic copy available at: https://ssrn.com/abstract=3430886 Assumption 1. There is an underlying ground truth DCM from which all T consumers independently make selections from the oered assortments, generating data ¬πit , St ¬∫, t = 1;::. T . Note that the assumption only requires consumers to make choices independently. On the other hand, we focus on a xed-design experiment, and the sequence of assortment oered xt can be arbitrary. This is dierent from most consistency results of random forests in which random design is used (see (Biau and Scornet, 2016) for references), i.e., xt are i.i.d. In our setting, the assortment is unlikely to be generated randomly, but chosen by the rm, either to maximize the revenue or explore customer preferences by A/B testing. Therefore, a xed design probably reects the reality more than a random design. Since the consistency result requires the sample size T !1, we use the subscript T to emphasize the fact that the parameters may be chosen based on T . For a given √çTassortment x, let kT ¬πx¬∫, t=1 Ifxt=x } be the number of consumers who see assortment x. We are now ready to establish the consistency of random forests. Theorem4.Suppose Assumption 1 holds, then for any x and i, if limT !1kT ¬πx¬∫¬ùT > 0, lT is xed, zT !1, BT !1, then the random forest is consistent: !√ï lim P BT 1 Iftb¬πx¬∫=igÙÄÄÄp¬πi, x) >  = 0 T !‚àû BTb=1 for all  > 0. According to Theorem 4, the random forest can accurately predict the choice probability of any DCM, given that the rm oers the assortment for many times. Practically, the result can guide us about the choice of parameters. In fact, we just need to generate many trees in the forest (BT !1), re-sample many observations in a decision tree (zT !1), and keep the terminal leaf small (lT is xed). The requirement is easily metpby the choice of parameters in the remarks following Algorithm 1, i.e., z = T , m = N and l = 50. Theorem 4 guarantees a good performance of the random forest when the seller has collected a large dataset. This is a typical case in online retailing, especially in the era of ‚Äúbig data‚Äù. Random forests thus provide a novel data-driven approach to model customer choices. In particular, the model is rst trained from data, and then used to interpret 13  Electronic copy available at: https://ssrn.com/abstract=3430886 the inherent thought process of consumers when they make purchases. By Theorem 4, when the historical data has a large sample size, the model can accurately predict how consumers make decisions in reality. This reects the universality of the model. In this section, we provide concrete examples demonstrating several practical considerations that can hardly be captured by other DCMs and handled well by random forests. 4.2. GiniIndexRecoverstheRankList In Section 2, we have shown that any DCM can be represented by a combination of binary decision trees. Moreover, through numerous experiments, we have found out that random forests perform particularly well when the data is generated by DCMs that can be represented by a few binary decision trees. In this section, we further explore this connection by studying a concrete setting where the DCM is represented by a single regular decision tree. Without loss of generality, we assume that customers always prefer product i to i + 1, for i = 1;:::, N ÙÄÄÄ1, and product N to the no-purchase option. Equivalently, the DCM is a single rank list (the preferences of all customers form an ordered set). The following nite-sample result demonstrates that the rank list can be recovered from the random forest with high probability. Theorem5.Suppose the actual DCM is a rank list and the assortments in the training data are sampled uniformly. The random forest algorithm with sub-sample size z = T (without replacement), m = N , terminal leaf size l = 1 and B = 1 accurately predicts the choices of at least a fraction 1 ÙÄÄÄ of all 2N assortments with probability at least k   √ï TT1 ‚àí 13 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ÙÄÄÄ164 2iÙÄÄÄ1 113 2iÙÄÄÄ1 i=1 where k = dlog2 1 e. Since the bound scales exponentially in T , the predictive accuracy increases tremendously with size. The proof of the theorem reveals an intrinsic connection between the Gini index and the recovery of the rank list. Consider the rst split of the random forest in Step 8. We can show that, in expectation, if the rst split is on product i, then the resulting 14  Electronic copy available at: https://ssrn.com/abstract=3430886 Gini index is 21 1 22N ÙÄÄÄ2 + O¬π1¬ùT ¬∫:3 ‚àí 3 22iÙÄÄÄ2 ‚àí 3 ¬∑ In other words, if the data is generated without randomness (centered at the mean), then the rst split would occur on product 1 because of the ordering of the Gini index when T is large. Therefore, for the data points falling into the right branch of the rst split (having product 1 in the assortment), no more splits are needed as all customers would choose product 1 according to the rank list. Such a split correctly identies roughly half of the assortments. In the proof, we control the randomness by concentration inequalities, and conduct similar computations for the second, third splits and so on. The proof reveals the following insight into why random forests may work well in practice: The Gini index criterion tends to nd the products that are ranked high in the rank lists, because they create ‚Äúpurer‚Äù splits that lower Gini index. As a result, the topological structure of the decision trees trained in the random forest is likely to resemble that of the binary choice trees underlying the DCM generating the data. 5. FlexibilityandBenefitsofRandomForests In this section we demonstrate the exibility and benet of using random forests to estimate the choice forest. 5.1. BehavioralIssues Because of Theorem 3 and Theorem 4, random forests can be used to estimated any DCMs. For example, there is empirical evidence showing that behavioral considerations of consumers may distort their choice and thus violate regularity, e.g., the decoy eect (Ariely, 2008) and the comparison-based DCM (Huber et al., 1982; Russo and Dosher, 1983). It is already documented in Chen and Mi≈°ic (2019) that the decision forest can capture the decoy eect. In this section, we use the choice forest to model consumer search. How consumers search to obtain new information when making purchases, is an important behavioral issue that is not monitored, or ‚Äúunsupervised‚Äù in statistical terms, and hard to estimate by most models (for a few exceptions, see e.g. Wang and Sahin (2017)). Therefore, most DCMs abstract away those thought processes and only capture 15  Electronic copy available at: https://ssrn.com/abstract=3430886 the aggregate eect. Weitzman (1979) proposes a sequential search model with search costs. Prior to initiating the search consumers know only the distribution, say Vj of the net utility of product j 2¬ªN ¬ºand the cost cj to learn the realization of Vj . Let zj be the root of the equation E¬ª¬πVj ÙÄÄÄzj ¬∫+¬º= cj and sort the products in descending order of zj . Weitzman shows that it is optimal to walk away without making any observations if the realized value of the no-purchase alternative, say W0 = V0 exceeds z1. Otherwise c1 is paid to observe V1 is observed and W1 = max¬πV1;W0¬∫is computed. The process stops if W1 exceeds z2 and continued otherwise, stopping the rst time, if ever, that Wi > zi+1. We next show that this search process can be represented by decision trees. Consider three products (N = 3). Suppose that the products are sorted so that z1 > z2 > z3 > 0, and that the valuations of an arriving customer satisfy v2 > v1 > v3. Hence the customer always searches in the order of product one !product two !product three. If in addition we suppose v2 > z3 > v1, then the decision tree can be illustrated in Figure 2. For example, suppose products one and tree are oered. The customer rst searches product one, because the reservation price of product one z1 is the highest. The realized valuation of product one is, however, not satisfactory (v1 < z3). Hence the customer keeps on searching the product with the second highest reservation price in the assortment, which is product three (product two is skipped because it is not in the assortment). However, the search process results in an even lower valuation of product three v3 < v1. As a result, the customer recalls and chooses product one. Clearly, a customer with dierent realized valuations would conduct a dierent search process, and leads to a dierent decision tree. 5.2. AggregatedChoiceData One of the most pressing practical challenges in data analytics is the quality of data. In Section 2, the historical data f¬πit , xt ¬∫gT =1 is probably the most structured and granular t form of data one can hope to acquire. While most academic papers studying the estimation of DCMs assume this level of granularity, in practice it is frequent to see data in a more aggregate format. As an example, consider an airline oering three service classes E, T and Q of a ight where data is aggregated over a time window during which there may be changes to the assortment, and compiled from dierent sales channels. The company records information at certain time clicks as in Table 1. For each class, the 16  Electronic copy available at: https://ssrn.com/abstract=3430886 Has product 1 Has product 2 Choose 2 Has product 3 Choose 1 Choose 1 Has product 2 Choose 2 Has product 3 Choose 3 No purchase Y Y N Y N N Y N Y N Has product 1 Has product 2 Choose 2 Has product 3 Choose 1 Choose 1 Has product 2 Choose 2 Has product 3 Choose 3 No purchase Y Y N Y N N Y N Y N Figure 2: The sequential search process when N = 3 and the realized valuations and reservation prices satisfy v2 > v1 > v3, z1 > z2 > z3 > 0 and v2 > z3 > v1. Class Closure percentage # Booking E 20% 2 T 0%5 Q 90% 1 Table 1: A sample daily data of oered service classes and number of bookings. 17  Electronic copy available at: https://ssrn.com/abstract=3430886 closure percentage reects the fraction of time that the class is not open for booking, i.e., included in the assortment. Thus, 100% would imply that the corresponding class is not oered during that the time window. In a retail setting, this helps to deal with products that sell-out between review periods. The number of bookings for each class is also recorded. There may be various reasons behind the aggregation of data. The managers may not realize the value of high-quality data or are unwilling to invest in the infrastructure and human resources to reform the data collection process. One of the author has encountered this situation in practice with aggregate datasets as in Table 1. Fortunately, random forests can deal with aggregated choice data naturally, a feat that may be quite dicult to deal with with the column generation approach. Suppose the presented aggregated data has the form f¬πps, bs ¬∫gS =1, where ps 2¬ª0, 1¬ºN denotess the closure percentage of the N products in day s, bs ‚àà ZN +1 denotes the number of + bookings1, and the data spans S time windows. We transform the data into the desired√çNform as follows: for each time window s, we create Ds , k=0 bs ¬πk) observations,  Ds¬πis;j, xs;j ) =1. The predictor xs;j 1ÙÄÄÄps 2¬ª0, 1¬ºN and let bs ¬πk¬∫of is;j be valued k, for j k = 0;:::, N . To explain the intuition behind the data transformation, notice that we cannot tell from the data which assortment a customer faced when she made the booking. We simply take an average assortment that the customer may have faced, represented by 1ÙÄÄÄps . In other words, if 1 ÙÄÄÄps ¬πj¬∫2¬ª0, 1¬ºis large, then it implies that product j is oered most of the time during the day, and the transformation leads to the interpretation that consumers see a larger ‚Äúfraction‚Äù of product j. As the closure percentage has a continuous impact on the eventual choice, it is reasonable to transform the predictors into a Euclidean space ¬ª0, 1¬ºN , and build a smooth transition between the two ends ps ¬πj¬∫= 0 (the product is always oered) and ps ¬πj¬∫= 1 (the product is never oered). The transformation creates a training dataset for classication with continuous predictors. The random forest can accommodate the data with minimal adaptation. In particular, all the steps in Algorithm 1 can be performed. The tree may have dierent structures: because the predictor x may not be at the corner of the unit hypercube any more, the split points may no longer be at 0.5. 1Again, we do not deal with demand censoring in this paper and assume that bshas an additional dimension to record the number of consumers who do not book any class. 18  Electronic copy available at: https://ssrn.com/abstract=3430886 5.3. ProductImportance Random forests can be used to assign scores to each product and rank the importance of products. A common score, mean decrease impurity (MDI), is based on the total decrease in node impurity from splitting on the variable (product), averaged over all trees (Biau and Scornet, 2016). The score for product m is dened as B√ï√ï1MDI¬πm¬∫= ¬πfraction of data in the parent node of s¬∫B b=1 all splits s in the bth tree ¬πreduction in the Gini index caused by s¬∫Ifs splits on mg. In other words, if consumers make decisions frequently based on the presence of product m (a lot of splits occur on product m), or their decisions are more consistent after observing the presence of product m (the Gini index is reduced signicantly after splitting on m), then the product gains more score in MDI and regarded as important. The identication of important products provides simple yet powerful insights into the behavioral patterns of consumers. Consider the following use cases: (1) An online retailer wants to promote its ‚Äúagship‚Äù products that signicantly increase the conversion rate. By computing the MDI from the historical data, important products can be identied without extensive A/B testing. (2) Due to limited capacity, a rm plans to reduce the available types of products in order to cut costs. It could simply remove the products that have low sales according to the historical data. However, some products, while not looking attractive themselves, serve as decoys or references and boost the demand of other products. Removing these products would distort the choice behavior of consumers and may lead to unfavorable consequences. The importance score provides an ideal solution: if a product is ranked low based on MDI, then it does not strongly inuence the decision making of consumers. It is therefore safe to leave them out. (3) When designing a new product, a rm attempts to decode the impact of various product features on customer choices. Which product feature is drawing most attentions? What do attractive products have in common? To conduct successful product engineering, rst it needs to use the historical data to nail down a set of attractive products. Moreover, to quantify and separate out the contribution of various features, a numerical score of product importance is necessary. The importance 19  Electronic copy available at: https://ssrn.com/abstract=3430886 score is a more reasonable criterion than sales volume, because the latter cannot capture the synergy created between the products. 5.4. IncorporatingPriceInformation Besides the ease of estimation, the other benet of a parametric DCM, such as the MNL or nested logit model, is the ability to account for covariates. For example, in the MNL model, the rm can estimate the price sensitivity of each product, and extrapolate/predict the choice probability when the product is charged a new price that has never been observed in the historical data. Many nonparametric DCMs cannot easily be extended to new prices. In this section, we show that while enjoying the benet of a nonparametric formulation, random forests can also accommodate the price information. Consider the data of the following format: f¬πit , pt ¬∫gT =1, where pt 2¬ª0, +1¬ºN rep-t resent the prices of all products. For product j that is not included in the assortment oered to customer t, we set pt ¬πj¬∫=+1. This is because when a product is priced at +1, no customer would be willing to purchase it, and it is equivalent to the scenario that the product is not oered at all. Such view of equivalence is commonly adopted in the literature.2 Therefore, compared to the binary vector xt that only records whether a product is oered, the price vector pt provides more information. However, the predictor p can not be readily used in random forests. The predictor space ¬ª0, +1¬ºN is unbounded, and the value +1added to the extended real number line is not implementable in practice. To apply Algorithm 1, we introduce link functions that map the predictors into a compact set. Denition 1. A function g¬π¬∫: ¬ª0, +1¬∫7!¬π0, 1¬ºis referred to as a link function, if (1) g¬πx¬∫is strictly decreasing, (2) g¬π0¬∫= 1, and (3) limx!+1g¬πx¬∫= 0. The link function can be used to transform a price p ‚â• 0 into ¬π0, 1¬º. Moreover, because of property (3), we can naturally dene g¬π+1¬∫= 0. Thus, if product j is not included in assortment xt , then g¬πpt ¬πj¬∫) = g¬π+1) = 0 = xt ¬πj¬∫. If product j is oered at a very low price, then g¬πpt ¬πj¬∫¬∫g¬π0) = 1. After the transformation of predictors, 2One may argue that an assortment with a product having an articially high price is not equivalent to the one without such a product, as the product may induce reference eects. We do not consider such behaviors here. 20  Electronic copy available at: https://ssrn.com/abstract=3430886 pt !g¬πpt ¬∫3, we introduce a continuous scale to the problem in Section 2. Instead of binary status (included or not), each product now has a spectrum of presence, depending on the price of the product. Now we can directly apply Algorithm 1 to the training data f¬πit , g¬πpt ¬∫¬∫gT =1. As a result, we need to modify Step 7, because the algorithm needs to t nd not only the optimal dimension to split, but also the optimal split location. The slightly modied random forests are demonstrated in Algorithm 2. Because of the Algorithm 2 Random forests for DCM estimation with price information 1: Data: f¬πit , pt ¬∫gT t=1 2: Tunable parameters: number of trees B, sub-sample size z 2f1;:::;T g, number of dimensions to split m 2f1;:::, N g, terminal leaf size l 2f1;:::, zg, a link function g¬π) 3: Transform the training data to f¬πit , g¬πpt ¬∫¬∫gT t=1 4: for b = 1 to B do 5: Select z observations from the training data with replacement, denoted by Z 6: Initialize the tree tb ¬πg¬πp¬∫¬∫0 with a single root node 7: while some leaf has greater than or equal to l observations belonging to Z and can be split do 8: Select m variables without replacement among f1;:::, N g9: Select the optimal one among the m dimensions and the optimal position to split that minimize the Gini index 10: Split the leaf node into two 11: end while 12: Denote the partition corresponding to the leaves of the tree by fR1;:::, RM g; let ci be the class label of a randomly chosen observation in Ri√çM13: Dene tb ¬πg¬πp¬∫¬∫= i=1 ci Ifg¬πp¬∫2Rig14: end for 15: The choice probability of product i given price vector p is √çB 1 Iftb¬πg¬πp¬∫¬∫=igb=1 B nature of the decision trees, the impact of prices on the choice behaviors is piecewise linear. For example, Figure 3 illustrates a possible decision tree with N = 3. It is not surprising that there are numerous link functions to choose from. We give two examples below: ÙÄÄÄx‚Ä¢g¬πx¬∫= e 2‚Ä¢g¬πx¬∫= 1 ‚àí arctan¬πx¬∫ 3When g¬π¬∫is applied to a vector p, it is interpreted as applied to each component of the vector. 21  Electronic copy available at: https://ssrn.com/abstract=3430886 g¬πp¬π1¬∫¬∫>0:3 g¬πp¬π1¬∫¬∫>0:9 1 g¬πp¬π2¬∫¬∫>0:5 2 0 g¬πp¬π3¬∫¬∫>0:4 g¬πp¬π3¬∫¬∫>0:8 3 0 g¬πp¬π2¬∫¬∫>0:3 2 0 Y Y N Y N N Y Y N N Y N g¬πp¬π1¬∫¬∫>0:3 g¬πp¬π1¬∫¬∫>0:91 g¬πp¬π2¬∫¬∫>0:5Y g¬πp¬π3¬∫¬∫>0:4 g¬πp¬π3¬∫¬∫>0:8 g¬πp¬π2¬∫¬∫>0:3N YN YN YN YN YN 2 0 3 0 2 0 Figure 3: A possible decision tree when the price information is incorporated for N = 3. g¬πp¬πi¬∫¬∫> a is equivalent to p¬πi¬∫< gÙÄÄÄ1¬πa¬∫, i.e., product i is included in the assortment and its price is less than gÙÄÄÄ1¬πa¬∫. In fact, the survival function of any non-negative random variables with positive PDF is a candidate for the link function. This extra degree of freedom may concern some academics and practitioners: How sensitive is the estimated DCM to the choice of link functions? What criteria may be used to pick a ‚Äúgood‚Äù link function? Our next result guarantees that the choice of link functions does not aect the estimated DCM. For any two link functions g1¬πx¬∫and g2¬πx¬∫, we can run Algorithm 2 for training data f¬πit , g1¬πpt ¬∫¬∫gT =1 and f¬πit , g2¬πpt ¬∫¬∫gT =1. We use t¬πj¬∫¬πx¬∫to denote the returned bth tree of t tb the algorithm for link function gj ¬πx¬∫, j = 1, 2. Proposition1.If we equalize ‚Ä¢the choice of parameters in Step 2 except for the link function ‚Ä¢the internal randomizers in Step 5, 8, and 12 in Algorithm 2, then the trees of both link functions return the same class label for ¬π1¬∫¬π2¬∫an observation in the training data: t ¬πg1¬πpt ¬∫) = t ¬πg2¬πpt ¬∫¬∫for all t = 1;:::;T andbb b = 1;:::, B. It is worth pointing out that although the random forests using two link functions output identical class labels for pt in the training data, they may dier for when predicting a new price vector p. This is because the splitting operation that minimizes 22  Electronic copy available at: https://ssrn.com/abstract=3430886 the Gini index in Step 8 is not unique. Any split between two consecutive observations4 results in an identical class composition in the new leaves and thus the same Gini index. Usually the algorithm picks the middle between two consecutive observations to split, which may dier for dierent link functions if they are not locally linear. Nevertheless, these cases are rare and Algorithm 2 is not sensitive to the choice of link functions. The theoretical guarantee in the pricing setting, however, is far more involved than Section 2. The state-of-art theoretical guarantee of random forests is given by Scornet et al. (2015). The authors prove that random forests are consistent for the regression problem, under some mild assumptions. Their setup is the closest to the original algorithm proposed in Breiman (2001), while other papers have proved the consistency for random forests with simplied or special implementations. Our setup diers from Scornet et al. (2015) in that we are focusing on a classication problem. We can recast it into a regression problem by analyzing the class probability of a particular class. However, instead of the Gini index, the sum of squared errors is typically used in regression problems, and the analysis has to be modied substantially. We thus leave the theoretical guarantee for future research. 5.5. IncorporatingCustomerFeatures A growing trend in online retailing and Ecommerce is personalization. Due to the increasing access to personal information and computation power, the retailer is able to device specic policies, including pricing or recommendation, for dierent customers based on his/her observed features. Personalization turns out to be hugely successful. Imagine an arriving customer being labeled as a college student. Then for a fashion retailer, it is a strong indicator that she/he may be interested in aordable brands. Leveraging personal information can greatly increase the garnered revenue of the rm. To oer personalized assortment, the very rst step is to incorporate the feature information into the choice model. One possible model, The mixed logit model assumes that the customers are categorized into discrete types, and customers of the same type behave homogeneously according to an independent MNL model. As one of the main drawbacks, it is not straightforward to connect continuous customers features 4If the algorithm splits on dimension m, then pt1 and pt2 are consecutive if there does not exist pt3 in the same leaf node such that ¬πpt1 ¬πm¬∫ÙÄÄÄpt3 ¬πm¬∫¬∫¬πpt2 ¬πm¬∫ÙÄÄÄpt3 ¬πm¬∫¬∫< 0. 23  Electronic copy available at: https://ssrn.com/abstract=3430886 Has product 1 Has product 3 Choose 3 Choose 1 Age 30 Married Choose 4 Choose 2 No purchase Y Y N N Y Y N N Has product 1 Has product 3 Choose 3 Choose 1 Age 30 Married Choose 4 Choose 2 No purchase Y Y N N Y Y N N Figure 4: A possible binary choice tree after incorporating customer features. to discrete types, and unsupervised learning algorithms may be needed. Recently, Bernstein et al. (2018) propose a dynamic Bayesian algorithm to address the issue. Another typical approach is built on the MNL model, while replacing the deterministic utility of a product by a linear function of the customer feature. For example, see Cheung and Simchi-Levi (2017) and references therein. In such personalized MNL models, the critics of the MNL model (such as IIA) persist. In this section, we demonstrate that it is natural for random forests to capture customer features and return a binary choice forest that is aware of such information. Suppose the collected data of the rm have the form ¬πit , xt , ft ¬∫for customer t, where in addition to ¬πit , xt ¬∫, the choice made and the oered set, the normalized customer feature ft 2¬ª0, 1¬ºM is also recorded. The procedure in Section 3 can be extended naturally. In particular, we may append ft to xt , so that the predictor ¬πx, f ¬∫2¬ª0, 1¬ºM+N . Algorithm 1 can be modied accordingly. The resulting binary choice forest consists of B binary choice trees. The splits of the binary choice tree now encode not only whether a product is oered, but also predictive feature information of the customer. For example, a possible binary choice tree illustrated in Figure 4 may result from the algorithm. Compared with the current personalized choice models, the framework introduced in this paper has the following benets: ‚Ä¢The estimation is straightforward (same as the algorithm without customer 24  Electronic copy available at: https://ssrn.com/abstract=3430886 features) and can be implemented eciently. ‚Ä¢The nonparametric nature of the model allows to capture complex interaction between products and customer features, and among customer features. For example, ‚Äúoering a high-end handbag‚Äù may become a strong predictor when the combination of features ‚Äúfemale‚Äù and ‚Äúage‚â• 30‚Äù are activated. In a binary choice tree, the eect is captured by three splits (one for the product and two for the customer features) along a branch. It is almost impossible to capture in a parametric (linear) model. ‚Ä¢The framework can be combined with aforementioned adjustments, such as pricing and product importance. For example, the measure MDI introduced in Section 5.3 can be used to identify predictive customer features. 6. NumericalExperiments In this section, we conduct a comprehensive numerical study based on both synthetic and real datasets. We nd that (1) random forests are quite robust and the performance does not vary much for underlying DCMs with dierent levels of complexity. In particular, random forests only underperform the correctly specied parametric models by a small margin and do not overt; (2) the standard error of random forests are small compared to other estimation procedures; (3) random forests benet tremendously from increasing sample size compared to other DCMs; (4) the computation time of random forests almost does not scale with the size of the training data; (5) random forests perform well even if the training set only includes 1¬ù100 of all available assortments; (6) random forests handle training data with nonstandard format reasonably well, such as aggregated data and price information (see Section 5.2 and 5.4 for more details) which cannot be handled easily by other frameworks. We will compare the estimation results of random forests with the MNL model (Train, 2009) and the Markov chain model (Blanchet et al., 2016)5 for both synthetic 5The MNL model is estimated using MLE. The Markov chain model is estimated using the EM algorithm, the same as the implementation in Simsek and Topaloglu (2018). The random forest is estimated using the Python package ‚Äúscikit-learn‚Äù. The implementation is slightly dierent in that scikitlearn outputs the empirical class probability rather than a random sample in Step 11. The dierence is negligible when B is large. 25  Electronic copy available at: https://ssrn.com/abstract=3430886 and real data sets. We choose the MNL and the Markov Chain models as benchmarks because the MNL model is one of the most widely used DCM and the Markov chain model can exibly approximate RUM (O¬πN 2¬∫) and has been shown (Berbeglia et al., 2018) to have an outstanding empirical performance compared to MNL, the nested logit, the mixed logit, and rank-based DCM. Note that the actual DCM generating the training data is not necessarily one of the three models mentioned above. When conducting numerical experiments, we set the hyper-parameters of the prandom forest as follows: B = 1000, z = T , m = N , l = 50. Choosing the parameters optimally using cross validation would further improve the performance of random forest. 6.1. TheRandomUtilityModel We rst investigate the performance of random forests when the training data is generated by RUM. The RUM includes a large class of DCMs. Consider N = 10 products. We generate the training set using the MNL model as the ground truth, where the expected utility of each product is generated from a standard normal distribution. Our training data consists of TÀú 2f30, 75, 150, 300, 600} periods. Each period contains a single assortment and 10 transactions so the total number of data points isT = 10TÀú. This is following the setup of Berbeglia et al. (2018). We randomly generate an assortment in each period uniformly randomly among all assortments. The performance is evaluated by root mean squared error, which is also used in Berbeglia et al. (2018): = vuut√ç S¬ªN ] √ç2 P¬πjjS¬∫ÙÄÄÄPÀÜ¬πjjS¬∫ √çj2S[f0} S¬ªN ¬º¬πjS | P, ÀÜP , (2)RMSE + 1) where Pdenotes the actual choice probability and PÀÜ denotes the estimated choice probability. The RMSE tests all the assortments and there is no need to generate a test set. For each setting, we generate 100 independent training data sets and compute the average and standard deviation of the RMSEs. The result is shown in Table 2. Not surprisingly, MNL model performs the best among the three because it has very few parameters and correctly species the ground truth. With such a simple DCM, the 26  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 0.084 (0.014) 0.030 (0.007) 0.062 (0.009) 750 0.061 (0.006) 0.019 (0.005) 0.042 (0.005) 1500 0.048 (0.005) 0.014 (0.003) 0.031 (0.004) 3000 0.041 (0.004) 0.009 (0.002) 0.023 (0.003) 6000 0.037 (0.002) 0.006 (0.002) 0.017 (0.002) Table 2: The average and standard deviation of RMSE using random forests, the MNL and the Markov chain model when the training data is generated by the MNL model. random forest does not overt and only slightly underperforms the Markov chain model. As the data size increases, the RMSE of random forest converges to zero. Next we use the rank-based model to generate the training data, which is shown to be equivalent to RUM (Block et al., 1959). Consider N = 10 products. Consumers are divided into k = 4 or k = 10 dierent types, each with a random preference permutation of all the products and the no-purchase alternative. For a given assortment of products, each type of consumer will purchase the product ranked the highest in her preference rank. If the no-purchase option is ranked higher than all the products in the assortment, then the customer does not purchase anything. We also randomly generate the fractions of customer types as follows: draw uniform random variables ui between zero and uione for i = 1, :::, k, and then set √çk to be the proportion of type i, i = 1, :::, k. The j=1 ujresult is shown in Table 3. We can see that the MNL model underperforms and does not improve signicantly as the data size increases, because of the misspecication error. The Markov chain model performs the best among the three. The performance of the random forest is quite robust, judged from the low standard deviation. Moreover, the performance improves dramatically as T increases; for T = 20000, the RMSE is smaller than the Markov chain model, which is shown in Berbeglia et al. (2018) to outperform other DCM estimators. Predicted by Theorem 4, the RMSE tends to zero when the training set is large. We run our algorithm on iMac with 2.7GHz quad-core Inter Core i5 and 8GB RAM installed. The running time is shown in Table 4. In terms of computation time, both the MNL model and the random forest can be implemented eciently, while the EM algorithm used to estimate the Markov chain model takes much longer. WhenT = 20000, the random forest spends 1/17 of the computation time of the Markov chain model. 27  Electronic copy available at: https://ssrn.com/abstract=3430886 Tk = 4 RF MNL Markov 300 0.115 (0.031) 0.121 (0.034) 0.078 (0.032) 750 0.090 (0.021) 0.118 (0.025) 0.058 (0.024) 1500 0.069 (0.016) 0.114 (0.029) 0.047 (0.020) 3000 0.056 (0.009) 0.118 (0.018) 0.044 (0.017) 6000 0.045 (0.006) 0.116 (0.021) 0.040 (0.017) 20000 0.034 (0.004) 0.115 (0.020) 0.037 (0.017) k = 10 RF MNL Markov 300 0.104 (0.013) 0.097 (0.016) 0.077 (0.016) 750 0.079 (0.009) 0.093 (0.012) 0.057 (0.009) 1500 0.065 (0.008) 0.091 (0.014) 0.048 (0.009) 3000 0.053 (0.005) 0.088 (0.013) 0.042 (0.008) 6000 0.046 (0.004) 0.088 (0.013) 0.040 (0.008) 20000 0.038 (0.003) 0.087 (0.014) 0.037 (0.009) Table 3: The average and standard deviation of RMSE of random forests, the MNL and the Markov chain model when the training data is generated using the rank-based model. 28  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 72.3s 0.7s 25.7s 750 72.5s 1.4s 36.1s 1500 72.3s 3.2s 113.7s 3000 74.0s 6.8s 203.0s 6000 74.5s 17.4s 445.2s 20000 81.8s 55.5s 1460.6s Table 4: The average running time of Random forest, MNL and the Markov chain Model Note that the running time of random forest almost does not increase for larger training set. This makes it useful when dealing with big data. 6.2. GeneralizabilitytoUnseenAssortments One of the major challenges in the estimation of the DCM, compared to other statistical estimation problems, is the limited coverage of the training data, which strongly violates the i.i.d. assumption. In particular, the seller tends to oer a few assortments that they believe are protable. As a result, in the training data fxt gT =1 only makes up a small t fraction of the total 2N available assortments. Any estimation procedure needs to address the following issue: can the DCM estimated from a few assortments generalize to the assortments that have never been oered in the training data? Next we show that random forests perform this task well: theoretically, random forests adaptively choose nearest neighbors, and the choice probability of an assortment can be generalized to ‚Äúneighboring‚Äù assortments (those with one more or one less product), as long as the underlying DCM possesses a certain degree of continuity in terms of the oered set x. Consider N = 10 products andT = 6000. We randomly choose TÀú assortments to oer in the training set and thus there are 6000¬ùTÀú transactions for each assortment. ‚ÄúLarge‚Äù assortments refer to those with many products (7 jS j10). The result is shown in Table 5. Note that there are 2N ÙÄÄÄ1 = 1023 possible available assortments. Therefore, for example, TÀú = 10 implies that only 1¬ù100 of the total assortments have been oered in the training data. The RMSE is only two to three times larger than the case where most assortments have been oered TÀú = 600. Moreover, a larger assortment helps the estimation of the DCM. When the actual DCM is the MNL 29  Electronic copy available at: https://ssrn.com/abstract=3430886 TÀú Rank-based k = 4 Rank-based k = 10 MNL 5 0.193 (0.064) 0.156 (0.034) 0.133 (0.041) 10 0.158 (0.034) 0.128 (0.026) 0.111 (0.035) 5 (large) 0.181 (0.056) 0.124 (0.028) 0.038 (0.017) 10 (large) 0.150 (0.047) 0.109 (0.027) 0.034 (0.014) 50 0.087 (0.025) 0.073 (0.014) 0.054 (0.008) 100 0.068 (0.014) 0.060 (0.007) 0.042 (0.004) 600 0.045 (0.006) 0.046 (0.004) 0.037 (0.002) Table 5: The average and standard deviation of RMSE using random forests when there are a few assortments in the training data. The column represents dierent ground-truth models. model, training random forests with 10 large assortments performs better than training with 600 randomly chosen assortments. We also remark that the generalizability of random forests does not only depend on the estimator, but also the actual DCM. Some DCMs are more accessible to generalization to unseen assortments. It remains an exciting future research to formalize the statement and theoretically quantify the generalizability of a DCM to unseen data in the framework of random forests. 6.3. BehavioralChoiceModels When the DCM is outside the scope of RUM and the regularity is violated, the Markov chain and MNL model may fail to specify the choice behavior correctly. In this section, we generate choice data using the comparison-based DCM (Huber et al., 1982), described below. Consumers implicitly score various attributes of the products in the assortment. Then they undergo an internal round-robin tournament of all the products. When comparing two products from the assortment, the customer checks their attributes and count the number of preferable attributes of both products. Eventually, the customer count the total number of wins (preferable attributes) in the pairwise comparisons. Here we assume that customers choose with equal probability if there is a tie. In the experiment, we consider N = 10 products. Consumers are divided into k = 2 dierent types, whose proportions are randomly generated between 0 and 1. Each type assigns uniform random variables between 0 and 1 to the ve attributes of all the 30  Electronic copy available at: https://ssrn.com/abstract=3430886 T RF MNL Markov 300 0.157 (0.031) 0.160 (0.033) 0.146 (0.038) 750 0.133 (0.025) 0.156 (0.030) 0.132 (0.036) 1500 0.112 (0.022) 0.152 (0.030) 0.123 (0.033) 3000 0.094 (0.021) 0.155 (0.030) 0.120 (0.037) 6000 0.079 (0.018) 0.152 (0.032) 0.120 (0.036) Table 6: The average and standard deviation of RMSE using Random Forest, MNL and Markov chain Model under the comparison-based DCM products (including the no-purchase option). Again we use the RMSE in (2) to compare the predictive accuracy. Like in the previous experiment, each setting is simulated 100 times. The result is shown in Table 6. Because of the irregularity, both the MNL and the Markov chain DCM are outperformed by the random forest, especially when the data size increases. Note that as T !1, the random forest is able to achieve diminishing RMSE, while the other two models do not improve because of the misspecication error. Like the previous experiment, the random forest achieves stable performances with small standard deviations. 6.4. AggregatedChoiceData In this section, we investigate the performance of random forests when the training data is aggregated as in Section 5.2. To generate the aggregated training data, we rst generate T observations using the rank-based model for N = 10 products and k = 10 customer types, as in Section 6.1. The only dierence is that we only simulate one instead of ten transactions for each oered assortment. Then, we let a be aggregation levels, i.e., we aggregate a data points together. For example, a = 1 is equivalent to the original data. For a = 5, Table 7 illustrates ve observations in the original data set for n = 5. Upon aggregation, the ve transactions are replaced by ve new observations with xt ¬ª0:6, 0:4, 0:8, 0:4, 0:6¬ºand it = 1, 0, 4, 3, 1 for t = 1, 2, 3, 4, 5. We test the performance for dierent sizes of the training set T 2f500, 5000, 50000gand dierent aggregate levels a 2f1, 5, 10, 50, 100g. The performance is measured in RMSE. We simulate 100 instances for each setting to evaluate the average and standard deviation, shown in Table 8. From the results, random forests handle aggregate data 31  Electronic copy available at: https://ssrn.com/abstract=3430886 Product 1 Product 2 Product 3 Product 4 Product 5 Choices 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 4 3 1 Table 7: Five observations in the unaggregated original data. Upon aggregation, they are replaced by ve new observations with xt¬ª0:6, 0:4, 0:8, 0:4, 0:6¬ºand it= 1, 0, 4, 3, 1 for t = 1, 2, 3, 4, 5. T a = 1 a = 5 a = 10 a = 50 a = 100 500 0.082 (0.009) 0.109 (0.016) 0.114 (0.016) 0.119 (0.015) 0.120 (0.015) 5000 0.047 (0.004) 0.085 (0.010) 0.097 (0.012) 0.111 (0.013) 0.114 (0.013) 50000 0.039 (0.002) 0.068 (0.009) 0.082 (0.011) 0.103 (0.013) 0.108 (0.013) Table 8: Random Forest performance for dierent aggregate levels relatively well. Even with aggregation level a = 100, the RMSE does not seem to deteriorate signicantly. Note that no other DCMs can handle aggregate data to the best of our knowledge, so no benchmark can be provided in this case. 6.5. IncorporatingPricingInformation In this section, we test the performance of random forests when the price information is incorporated. This is a unique feature of random forests as most DCMs can‚Äôt estimate the choice probability eciently with prices. We use the MNL model to generate the choice data. Let u denote the expected utility of the products and p their prices. Therefore, for given assortment S, the choice probabilities of product i 2S and the no-purchase option are: exp¬πui ÙÄÄÄpi ) 1Pi = √ç , P0 = √ç . (3)1 + j2S exp¬πuj ÙÄÄÄpj ) 1 + j2S exp¬πuj ÙÄÄÄpj ) Consider N = 10 products. We generate ui as uniform random variables between 0 and 1 for each product. For each observation, we rst randomly generate an assortment as Section 6.1. Then we generate a price for each product in the assortment as the 32  Electronic copy available at: https://ssrn.com/abstract=3430886 T RMSE 500 0.067 (0.008) 5000 0.040 (0.002) 50000 0.035 (0.002) Table 9: The RMSE of random forests with price information absolute value of a standard normal random variable. As explained in Section 5.4, we use the link function g¬πx¬∫= exp¬πÙÄÄÄx¬∫. The customer‚Äôs choice then follows the choice probability (3). The RMSE in (2) is no longer applicable because the assortments and prices cannot be exhausted. To evaluate the performance, we randomly generate N = 1000 assortments and prices according to the same distribution as the training data. Then we evaluate the RMSE as follows: vuut√çN √ç2 P¬πjjSi ¬∫ÙÄÄÄPÀÜ¬πjjSi ¬∫ i=1 j2Si[f0g√çNP, ÀÜP , (4)RMSE = =1¬πjSi j+ 1¬∫i where Pdenotes the actual choice probability, and PÀÜ denotes the estimation. We investigate the performance of the random forest for dierent sizes of training data T 2f500, 5000, 50000g. The result is shown in Table 9. The result conrms that random forests can tackle price information well. Although we do not have benchmarks, the RMSE is comparable to the previous experiments, e.g., Table 8. 6.6. RealData:Hotel In this section we apply the random forest algorithm to a public dataset based on Bodea et al. (2009). The dataset includes transient customers (mostly from business travelers) who stayed in one of ve continental U.S. hotels between March 12, 2007, and April 15, 2007. The minimum booking horizon for each check-in date is four weeks. Rate and room type availability and reservation information are collected via the hotel and/or customer relationship ocers (CROs), the hotel‚Äôs websites, and oine travel agencies. Since there is no direct competition among these ve hotels, we will process the data 33  Electronic copy available at: https://ssrn.com/abstract=3430886 separately. A product is uniquely dened by the room type (e.g. Suite 1, 2 Double Beds Room 1, etc). For each transaction, the purchased room type and the assortment oered are recorded. When processing the dataset, we have removed the product that has less than 10 transactions. We also removed the transactions whose oered assortments are not available due to technical reasons. For the transactions that none of the products in the available sets are purchased by the customer, we assume customers choose the no-purchase alternative. We do not add dummy transactions with no-purchases to uncensor the data like van Ryzin and Vulcano (2014), Simsek and Topaloglu (2018) and Berbeglia et al. (2018). To compare dierent estimation procedures, we use ve-fold cross validation to examine the out-of-sample performance. Because we no longer know the actual choice model that generates the data, after estimating the model in the training set, we follow Berbeglia et al. (2018) and evaluate the ‚Äúempirical‚Äù version of the RMSE in the validation set. That is, letting T be the validation set, we dene  ÀÜ ¬πi;S¬∫2T Ifj=ig‚àí P, T vuut√ç2√ç PÀÜ¬πjjS¬∫ (5)RMSE =√çj2S[f0} . ¬πi;S¬∫2T¬πjS j+ 1) In Table 10 we show the scale of the ve datasets after preprocessing. We show the out-of-sample RMSE data for each hotel (average and standard deviation). In addition, we also show the performance of the independent demand model (ID), which does not incorporate the substitution eect and is expected to perform poorly, in order to provide a lower bound of the performance. Consistent with the insights drawn from the synthetic data, random forest outperforms the parametric methods for larger dataset (Hotel 1, 2 and 3). For smaller data size (Hotel 4 and 5), random forest is on a par with the best parametric estimation procedure (Markov) according to Berbeglia et al. (2018). 6.7. RealData:IRIAcademicDataset In this section we compare several algorithms on the IRI Academic Dataset (Bronnenberg et al., 2008). The IRI Academic Dataset collects weekly transaction data from 47 U.S. 34  Electronic copy available at: https://ssrn.com/abstract=3430886 # products # in-sample # out-of-sample Hotel 1 Hotel 2 Hotel 3 Hotel 4 Hotel 5 10 6 7 4 6 1271 347 1073 240 215 318 87 268 60 54 RF MNL Markov ID Hotel 1 Hotel 2 Hotel 3 Hotel 4 Hotel 5 0.3040 (0.0046) 0.3034 (0.0120) 0.2842 (0.0051) 0.3484 (0.0129) 0.3219 (0.0041) 0.3098 (0.0031) 0.3120 (0.0148) 0.2854 (0.0065) 0.3458 (0.0134) 0.3222 (0.0069) 0.3047 (0.0039) 0.3101 (0.0124) 0.2842 (0.0064) 0.3471 (0.0125) 0.3203 (0.0046) 0.3224 (0.0043) 0.3135 (0.0178) 0.2971 (0.0035) 0.3584 (0.0047) 0.3259 (0.0058) Table 10: Summary statistics of the ve datasets and the average and standard deviation of the out-of-sample RMSE. markets from 2001 to 2012, covering more than 30 product categories. Each transaction includes the week and store of purchase, the universal product code (UPC) of the purchased item, number of units purchased and total paid dollars. The pre-processing steps taken follow those in Jagabathula and Rusmevichientong (2018) and Chen and Mi≈°ic (2019). In particular, we conduct the analysis for 31 categories separately using the data for the rst two weeks in 2007. Each product is uniquely dened by the vendor code. Each assortment is dened as the set of products that are available in the same store in that week. We only focus on the top nine purchased products from all stores during the two weeks in each category and treat all other products as the no-purchase alternative. However, the sales data for most categories is still too large for the EM algorithm to estimate the Markov chain model. For example, carbonated beverages, milk, soup and yogurt have more than 10 million transactions. For computational eciency, we uniformly sample 1/200 of original data size without replacement. This does not signicantly increase the sampling variability as most transactions in the original data are repeated entries. We use ve-fold cross-validation and RMSE dened in (5) to examine the out-ofsample performance. The result is shown in Table 11. Random forests outperform the 35  Electronic copy available at: https://ssrn.com/abstract=3430886 other two in 24 of 31 categories, especially for larger data size. According to Berbeglia et al. (2018), the Markov chain choice model has already been shown to have superb performance in synthetic and real-world studies. Table 11 fully demonstrates the potential of random forests as a framework to model and estimate consumer behaviors in practice. 7. ConcludingRemarks We hope that this study will encourage more scholars to pursue BRF as a research topic. We believe that addressing the following questions would help us decode the empirical success of random forests and understand the pitfalls: ‚Ä¢What type of DCMs can be estimated well by random forests and have higher generalizability to unseen assortments? ‚Ä¢As we use the choice forest to approximate DCMs, how can we translate the properties of a DCM to the topological structure of decision trees? ‚Ä¢Can we provide nite-sample error bounds for the performance of random forests, with or without the price information? ‚Ä¢What properties does the product importance index MDI have? ‚Ä¢Given a binary choice forest, possibly estimated by random forests, can we compute the optimal assortment eciently? References Anderson, S. P., A. De Palma, and J.-F. Thisse (1992). Discrete choice theory of product dierentiation. MIT press. Ariely, D. (2008). Predictably irrational. Harper Audio. Berbeglia, G. (2019). The generalized stochastic preference choice model. Working paper. 36  Electronic copy available at: https://ssrn.com/abstract=3430886 Product Category # data RF MNL Markov 1 Beer 10,440 0.2717 (0.0006) 0.2722 (0.0008) 0.2721 (0.0007) 2 Blades 1,085 0.3106 (0.0037) 0.3092 (0.0034) 0.3096 (0.0036) 3 Carbonated Beverages 71,114 0.3279 (0.0004) 0.3299 (0.0004) 0.3295 (0.0004) 4 Cigarettes 6,760 0.2620 (0.0028) 0.2626 (0.0030) 0.2626 (0.0030) Coee 8,135 0.2904 (0.0010) 0.2934 (0.0009) 0.2925 (0.0010) 6 Cold Cereal 30,369 0.2785 (0.0003) 0.2788 (0.0003) 0.2787 (0.0003) 7 Deodorant 2,775 0.2827 (0.0005) 0.2826 (0.0005) 0.2826 (0.0005) 8 Diapers 1,528 0.3581 (0.0024) 0.3583 (0.0020) 0.3583 (0.0022) 9 Facial Tissue 8,956 0.3334 (0.0007) 0.3379 (0.0010) 0.3375 (0.0007) Frozen Dinners/Entrees 48,349 0.2733 (0.0003) 0.2757 (0.0003) 0.2750 (0.0003) 11 Frozen Pizza 16,263 0.3183 (0.0001) 0.3226 (0.0001) 0.3210 (0.0001) 12 Household Cleaners 6,403 0.2799 (0.0010) 0.2798 (0.0009) 0.2798 (0.0009) 13 Hotdogs 7,281 0.3123 (0.0011) 0.3183 (0.0005) 0.3170 (0.0007) 14 Laundry Detergent 7,854 0.2738 (0.0017) 0.2875 (0.0017) 0.2853 (0.0016) Margarine/Butter 9,534 0.2985 (0.0004) 0.2995 (0.0004) 0.2990 (0.0003) 16 Mayonnaise 4,380 0.3212 (0.0024) 0.3242 (0.0010) 0.3230 (0.0006) 17 Milk 56,849 0.2467 (0.0007) 0.2501 (0.0005) 0.2538 (0.0012) 18 Mustard 5,354 0.2844 (0.0008) 0.2856 (0.0006) 0.2852 (0.0006) 19 Paper Towels 9,520 0.2939 (0.0009) 0.2964 (0.0008) 0.2959 (0.0008) Peanut Butter 4,985 0.3113 (0.0017) 0.3160 (0.0006) 0.3146 (0.0009) 21 Photography supplies 189 0.3456 (0.0081) 0.3399 (0.0081) 0.3456 (0.0088) 22 Razors 111 0.3269 (0.0300) 0.3294 (0.0225) 0.3323 (0.0195) 23 Salt Snacks 44,975 0.2830 (0.0006) 0.2844 (0.0007) 0.2840 (0.0007) 24 Shampoo 3,354 0.2859 (0.0006) 0.2855 (0.0071) 0.2856 (0.0009) Soup 68,049 0.2709 (0.0007) 0.2738 (0.0005) 0.2729 (0.0005) 26 Spaghetti/Italian Sauce 12,377 0.2901 (0.0003) 0.2919 (0.0006) 0.2914 (0.0006) 27 Sugar Substitutes 1,269 0.3080 (0.0036) 0.3067 (0.0035) 0.3072 (0.0034) 28 Toilet Tissue 11,154 0.3084 (0.0005) 0.3126 (0.0004) 0.3132 (0.0014) 29 Toothbrushes 2,562 0.2860 (0.0009) 0.2859 (0.0004) 0.2858 (0.0006) Toothpaste 4,258 0.2704 (0.0008) 0.2708 (0.0011) 0.2708 (0.0011) 31 Yogurt 61,671 0.2924 (0.0011) 0.2976 (0.0008) 0.2960 (0.0008) Table 11: The data size after pre-processing and the average and standard deviation of the out-of-sample RMSE (5) for each category. 37  Electronic copy available at: https://ssrn.com/abstract=3430886 Berbeglia, G., A. Garassino, and G. Vulcano (2018). A comparative empirical study of discrete choice models in retail operations. Working paper. Bernstein, F., S. Modaresi, and D. Saur√© (2018). A dynamic clustering approach to data-driven assortment personalization. Management Science 65(5), 2095‚Äì2115. Biau, G. and E. Scornet (2016). A random forest guided tour. Test 25(2), 197‚Äì227. Blanchet, J., G. Gallego, and V. Goyal (2016). A markov chain approximation to choice modeling. Operations Research 64(4), 886‚Äì905. Block, H. D., J. Marschak, et al. (1959). Random orderings and stochastic theories of response. Technical report, Cowles Foundation for Research in Economics, Yale University. Bodea, T., M. Ferguson, and L. Garrow (2009). Data set‚Äîchoice-based revenue management: Data from a major hotel chain. Manufacturing & Service Operations Management 11(2), 356‚Äì361. Breiman, L. (2001). Random forests. Machine learning 45(1), 5‚Äì32. Bronnenberg, B. J., M. W. Kruger, and C. F. Mela (2008). Database paper‚Äîthe iri marketing data set. Marketing science 27(4), 745‚Äì748. Chen, Y.-C. and V. V. Mi≈°ic (2019). Decision forest: A nonparametric approach to modeling irrational choice. Working paper. Cheung, W. C. and D. Simchi-Levi (2017). Thompson sampling for online personalized assortment optimization problems with multinomial logit choice models. Working paper. Farias, V. F., S. Jagabathula, and D. Shah (2013). A nonparametric approach to modeling choice with limited data. Management science 59(2), 305‚Äì322. Feng, G., X. Li, and Z. Wang (2017). On the relation between several discrete choice models. Operations research 65(6), 1516‚Äì1525. Flores, A., G. Berbeglia, and P. Van Hentenryck (2017). Assortment and price optimization under the two-stage luce model. Working paper. 38  Electronic copy available at: https://ssrn.com/abstract=3430886 Hastie, T., R. Tibshirani, and J. Friedman (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics. Springer. Huber, J., J. W. Payne, and C. Puto (1982). Adding asymmetrically dominated alternatives: Violations of regularity and the similarity hypothesis. Journal of consumer research 9(1), 90‚Äì98. Jagabathula, S. and P. Rusmevichientong (2016). A nonparametric joint assortment and price choice model. Management Science 63(9), 3128‚Äì3145. Jagabathula, S. and P. Rusmevichientong (2018). The limit of rationality in choice modeling: Formulation, computation, and implications. Management Science 65(5), 2196‚Äì2215. Jagabathula, S., L. Subramanian, and A. Venkataraman (2019). A conditional gradient approach for nonparametric estimation of mixing distributions. Management Science. Liaw, A. and M. Wiener (2002). Classication and regression by randomforest. R News 2(3), 18‚Äì22. McFadden, D. (1973). Conditional logit analysis of qualitative choice behaviour. In P. Zarembka (Ed.), Frontiers in Econometrics, pp. 105‚Äì142. New York, NY, USA: Academic Press New York. Mi≈°ic, V. V. (2016). Data, models and decisions for large-scale stochastic optimization problems. Ph. D. thesis, Massachusetts Institute of Technology. Natarajan, K., M. Song, and C.-P. Teo (2009). Persistency model and its applications in choice modeling. Management Science 55(3), 453‚Äì469. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay (2011). Scikit-learn: Machine Learning in Python . Journal of Machine Learning Research 12, 2825‚Äì2830. Russo, J. E. and B. A. Dosher (1983). Strategies for multiattribute binary choice. Journal of Experimental Psychology: Learning, Memory, and Cognition 9(4), 676. 39  Electronic copy available at: https://ssrn.com/abstract=3430886 Scornet, E., G. Biau, J.-P. Vert, et al. (2015). Consistency of random forests. The Annals of Statistics 43(4), 1716‚Äì1741. Simonson, I. and A. Tversky (1992). Choice in context: Tradeo contrast and extremeness aversion. Journal of marketing research 29(3), 281‚Äì295. Simsek, A. S. and H. Topaloglu (2018). An expectation-maximization algorithm to estimate the parameters of the markov chain choice model. Operations Research 66(3), 748‚Äì760. Train, K. E. (2009). Discrete choice methods with simulation. Cambridge university press. van Ryzin, G. and G. Vulcano (2014). A market discovery algorithm to estimate a general class of nonparametric choice models. Management Science 61(2), 281‚Äì300. van Ryzin, G. and G. Vulcano (2017). An expectation-maximization method to estimate a rank-based choice model of demand. Operations Research 65(2), 396‚Äì407. Wang, R. and O. Sahin (2017). The impact of consumer search cost on assortment planning and pricing. Management Science 64(8), 3649‚Äì3666. Weitzman, M. L. (1979). Optimal search for the best alternative. Econometrica, 641‚Äì654. Williams, H. C. (1977). On the formation of travel demand models and economic evaluation measures of user benet. Environment and planning A 9(3), 285‚Äì344. A. Proofs Proof of Theorem 1: It is easy to see that a BCF is a DCM. To show the converse, consider a collection of DCMs pc ¬πi, S¬∫, c ‚àà C. Let ‚â• 0 with √ç = 1, Then p¬πi, S) = cc2Cc√ç cpc ¬πi, S¬∫is clearly a DCM, so a convex combination of DCM is a DCM and thus c2C all DCMs form a convex set. Consider the extreme points, i.e., DCMs that cannot be written as a non-trivial convex combination of two or more DCMs. Let E be the collection of all extreme DCMs, pe ¬πi, S¬∫, e ‚àà E. Then any DCM p¬πi, S¬∫, i ‚àà S+, S ‚äÇ N is in the convex hull of pe, e ‚àà E. A deterministic DCM is a DCM such that p¬πi, S¬∫2f0, 1gn for every i 2S+ and every 40  Electronic copy available at: https://ssrn.com/abstract=3430886 S ‚äÇ N . Next we show that a DCM is an extreme point if and only if it is deterministic. Given a deterministic DCM, say d, let id ¬πS¬∫‚àà S+ be the choice made by d, so that pd ¬πi, S¬∫= 1 only if i = id ¬πS¬∫. It is clear that a deterministic DCM is an extreme point. Conversely, for an extreme DCM, if it is not deterministic, then we can always split the probability between 0 and 1 and makes it a convex combination of two dierent DCMs. Therefore, extreme points are equivalent to deterministic DCMs. It is sucient to show that all deterministic DCMs can be represented as a BCF. This follows directly because every deterministic DCM is the binary choice tree which can be explicitly constructed tb ¬πS¬∫, id ¬πS¬∫for all S N . We can now formally state the connection between DCMs and BCFs. Proof of Theorem 3. Let p¬πi, x¬∫be an arbitrary DCM. Construct B trees tb, b = 1;:::, B, with 2n leaves associated with each of the 2n possible subsets of ¬ªN ¬º. For any given √çB x 2f0, 1gN , we let =1 Iftb¬πx¬∫=i} = bp¬πi, x¬∫Bcfor i = 1;:::, N . It is easy to see that b N 1 jf ¬π0, x¬∫ÙÄÄÄp¬π0, x¬∫| ‚â§ jf ¬πi, x¬∫ÙÄÄÄp¬πi, x¬∫| ‚â§ , i = 1;:::, N . BB Since the error bound holds for all x and i, the choice forest can approximate any DCM for a suciently large B.  Proof of Theorem 4: We rst prove that for a single decision tree, there is a high probability that the number of observations chosen in Step 4 in which x is oered is large. √çTMore precisely, let Xt = Ifxt=x g. It is easy to see that =1 Xt = kT . Step 4 randomly t selects zT observations out of the T with replacement. Denote the bootstrap sample of fX1;:::, XT } by Y1;:::, YzT . By Hoeding‚Äôs inequality, we have the following concentration inequality √ç! zT j=1 Yj kT ÙÄÄÄ P ÙÄÄÄ 2 exp ÙÄÄÄ2zT2 (6) zT T for any  > 0. In other words, the bootstrap sample in Step 4 does not deviate too far from the population as long as zT is large. As we choose  < limT !1kT ¬ùT , it implies 41  Electronic copy available at: https://ssrn.com/abstract=3430886 zTthat √ç j=1 Yj !1and in particular zT√ï lim P( Yj > lT ¬∫= 1. (7)T !‚àû j=1 zTNext we show that given √ç j=1 Yj > lT for a decision tree, the leaf node that contains x only contains observations with Yj = 1. That is, the terminal leaf containing x is a single corner of the unit hypercube. If the terminal leaf node containing an observation zT zTwith predictor x, then it has no less than √ç j=1 Yj observations, because all the √ç j=1 Yj samples used to train the tree fall on the same corner in the predictor space. If another observation with a dierent predictor is in the same leaf node, then it contradicts Step 6 in the algorithm, because it would imply that another split could be performed. Suppose fR1;:::, RM gis the nal partition corresponding to the decision tree. As a result, in the region Rj such that x 2Rj , we must have that tb¬πx¬∫is a random sample from the √çzT =1 Yj customer choices, according to Step 11. j Now consider the estimated choice probability from the random forest: √çBT 1 Iftb¬πx¬∫=ig.b=1 BT Note that ftb¬πx¬∫gbB = T 1 are i.i.d. given the training set. By Hoeding‚Äôs inequality, conditional on f¬πit , xt ¬∫gT t=1, !√ïBT 1 ÙÄÄÄ2BT2P Iftb¬πx¬∫=igÙÄÄÄP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > 1 f¬πit , xt ¬∫gT 2e 1 , (8)tt=1BTb=1 for all 1 > 0. Next we analyze the probability P¬πtb ¬πx) = ijf¬πit , xt ¬∫gT =1¬∫for a singlet zTdecision tree. By the previous paragraph, conditional √ç j=1 Yj > lT , the output of a zTsingle tree tb¬πx¬∫is randomly chosen from the class labels of √ç =1 Yj observations whose j predictor is x. Let Zj be the class label of the jth chosen observation in Step 4. Therefore, zTconditional on the event √ç j=1 Yj > lT and the training data, we have zT zT√ï√ï Yj IfZj=igP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gTt=1 , Yj > lT ¬∫= √ç . (9)zT j=1 j=1 j=1 Yj 42  Electronic copy available at: https://ssrn.com/abstract=3430886 Because fYj IfZj=iggz = T 1 is a bootstrap sample, having i.i.d. distribution j √çT t=1 Ifit=i;xt=x gP¬πYj IfZj=i} = 1¬∫= T given the training data, we apply Hoeding‚Äôs inequality again √çzT √çT ! j=1 Yj IfZj=i} t=1 Ifit=i;xt=xgP ‚àí > 2 f¬πit , xt ¬∫gTt=1 2 exp¬πÙÄÄÄ2zT22) (10) zT T √çTfor all 2 > 0. Now applying Hoeding‚Äôs inequality to t=1 Ifit=i;xt=x } again, and because of Assumption 1, we have that √çT ! t=1 Ifit=i;xt=xgP ÙÄÄÄP¬πijx) > 3 2 exp¬πÙÄÄÄ2kT32) (11)kT for all 3 > 0. With the above results, we can bound the target quantity !√ïBT 1P Iftb¬πx¬∫=igÙÄÄÄP¬πijx) >  BTb=1" !# √ïBT 1 = E Iftb¬πx¬∫=igÙÄÄÄP¬πijx) >  f¬πit , xt ¬∫gTP t=1BTb=1" !# √ï EP BT 1 Iftb¬πx¬∫=igÙÄÄÄP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù2 f¬πit , xt ¬∫gT tt=1BTb=1  + EPP¬πijx¬∫ÙÄÄÄP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù2 f¬πit , xt ¬∫gT tt=1 By (8), the rst term is bounded by 2 exp¬πÙÄÄÄBT2¬ù2¬∫which converges to zero as BT !1. 43  Electronic copy available at: https://ssrn.com/abstract=3430886 To bound the second term, note that  PP¬πijx¬∫ÙÄÄÄP¬πtb¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù2 f¬πit , xt ¬∫gT tt=1 √çT ! t=1 Ifit=i;xt=xgPP¬πijx¬∫‚àí > ¬ù6 f¬πit , xt ¬∫gT t=1kT √çTzT ! t=1 Ifit=i;xt=x } √ï Yj IfZj=ig+ P ‚àí √ç > ¬ù6 f¬πit , xt ¬∫gT zT t=1kT j=1 Yjj=1 ! zT√ï Yj IfZj=ig+ P √ç ÙÄÄÄP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù6 f¬πit , xt ¬∫gT (12)zT tt=1 j=1 j=1 Yj The expected value of the rst term in (12) is bounded by 2 exp¬πÙÄÄÄkT2¬ù18) by (11), which converges to zero as kT !1. For the second term of (12), we have that √çTzT ! t=1 Ifit=i;xt=x} √ï Yj IfZj=igP ‚àí √ç > ¬ù6 f¬πit , xt ¬∫gT t=1kT j=1 zj= T 1 Yj !√çTzT t=1 Ifit=i;xt=x } √ï TYjIfZj=igP ‚àí > ¬ù12 f¬πit , xt ¬∫gT t=1kT zTkTj=1 ! zT zT√ï TYj IfZj=i} √ï Yj IfZj=ig+ P ‚àí √ç > ¬ù12 f¬πit , xt ¬∫gT (13)zT t=1 zTkT =1 Yjj=1 j=1 j For the rst term in (13), note that by (9) √çTzT ! t=1 Ifit=i;xt=x } √ï TYjIfZj=igP ‚àí > ¬ù12 f¬πit , xt ¬∫gT t=1kT zTkTj=1√çTzT ! t=1 Ifit=i;xt=x} √ï Yj IfZj=ig= P ‚àí > kT¬ù12 f¬πit , xt ¬∫gT 2 exp¬πÙÄÄÄzTk22¬ù72¬∫!0t=1 TT zTj=1 44  Electronic copy available at: https://ssrn.com/abstract=3430886 as T !1. For the second term in (13), we have ! zT zT√ï TYj IfZj=i} √ï Yj IfZj=igP ‚àí √ç > ¬ù12 f¬πit , xt ¬∫gT zT t=1 zTkTj=1 j=1 j=1 Yj√ç!zT j=1 Yj IfZj=i} T √çzTP ‚àí > ¬ù12 f¬πit , xt ¬∫gT zT t=1 zTkT j=1 Yj ! T √çzTP ‚àí > ¬ù12 f¬πit , xt ¬∫gT zT t=1kT j=1 Yj √ç! zT TzTkT j=1 Yj = P √ç ‚àí > ¬ù12 f¬πit , xt ¬∫gT zT t=1kT =1 YjT zTj TzTIt is easy to see that √çzT converges almost surely to a constant asT !1. Therefore, kT =1 Yjjby (6) the last term converges to zero. Finally we move on to the third term of (12). By (9), we have ! zT√ï YjIfZj=igP √ç ÙÄÄÄP¬πtb¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù6 f¬πit , xt ¬∫gT zT tt=1 j=1 j=1 Yj ! zT√ï = PP¬πtb¬πx¬∫= ijf¬πit , xt ¬∫gT =1, Yj > lT ¬∫ÙÄÄÄP¬πtb ¬πx¬∫= ijf¬πit , xt ¬∫gT =1) > ¬ù6 f¬πit , xt ¬∫gT t tt=1 j=1!! zT√ï P2P Yj lT f¬πit , xt ¬∫gT > ¬ù6 f¬πit , xt ¬∫gT :t=1 t=1 j=1 Note that we are focusing on a xed-design case, and fYj gand fit gare independent given Assumption 1. Therefore, !! zT zT√ï√ï P Yj lT f¬πit , xt ¬∫gTt=1 = P Yj lT !0 j=1 j=1 by (7). This completes the proof. Proof of Proposition 1: We need to show that the bth tree constructed by the algorithm of both link functions returns the same partition (in the sense that each region contains the same set of observations in the training data) of the predictor space ¬ª0, 1¬ºN and the 45  Electronic copy available at: https://ssrn.com/abstract=3430886 same class labels in each region/leaf. The class labels are guaranteed to be the same because we control the internal randomizer in Step 12. To show the partitions are the same, it suces to show that each split creates regions that are identical for the two link functions in the sense that the resulting regions contain the same set of observations. We will use induction to prove this claim. Before the construction of the bth tree, because the internal randomizers in Step 5 are equalized, the root node ¬ª0, 1¬ºN for both link functions contains the same set of observations. Now focusing on a leaf node in the middle of constructing the bth tree ¬πj¬∫¬πj¬∫¬πj¬∫¬πj¬∫for both link functions. We use ¬ªl , u1 ¬º¬ªlN , u ¬º¬ª0, 1¬ºN to denote the region 1 N of the leaf node for link functions j = 1, 2. By the inductive hypothesis, both regions contain the same set of observations. Without loss of generality, we assume that the regions contain fg1¬πpt ¬∫gT = 1 1 and fg2¬πpt ¬∫gT = 1 1, respectively. After Step 8, the same set of tt candidate splitting dimensions are selected. To show that Step 9 results in the same split in the two regions, consider a given split direction m and split point xj for j = 1, 2. If ¬π1¬∫¬π1¬∫¬π1¬∫¬π1¬∫¬π1¬∫¬π2¬∫¬π2¬∫¬π2¬∫¬π2¬∫¬π2¬∫¬ªl , u ¬º::. ¬ªl , x¬π1¬∫¬º¬ªl , u ¬ºand ¬ªl , u ¬º::. ¬ªl , x¬π2¬∫¬º¬ªl , u ¬ºm11 NN 11 m NN contain the same set of observations, i.e., for t = 1;:::;T1 ¬π1¬∫¬π1¬∫¬π1¬∫¬π1¬∫¬π1¬∫g1¬πpt ¬∫2¬ªl , u ¬º::. ¬ªl , x¬π1¬∫¬º¬ªl , u ¬º11 m NN ¬π2¬∫¬π2¬∫¬π2¬∫¬π2¬∫¬π2¬∫()g2¬πpt ¬∫2¬ªl , u ¬º::. ¬ªl , x¬π2¬∫¬º¬ªl , u ¬º;11 m NN then the Gini indices resulting from the splits are equal for the two link functions. This is because the Gini index only depends on the class composition in a region instead of the locations of the predictors, and the splits above lead to the same class composition in the sub-regions. This implies that in Step 8, both trees are going to nd the optimal splits that lead to the same division of training data in the sub-regions. By induction and the recursive nature of the tree construction, Algorithm 2 outputs the same partition in the bth tree for both link functions, i.e., the training data is partitioned equally. This completes the proof. Proof of Theorem 5: To simplify the notation, we use product N + 1 to denote the no-purchase option. Let nj denote the number of assortments in the training data where product j is in the assortment. Clearly nj has binomial distribution B¬πT , 1¬ù2¬∫. Let nj denote the number of assortments in the training set where product j is in the k 46  Electronic copy available at: https://ssrn.com/abstract=3430886 assortment and product k is chosen. Let n ÙÄÄÄj denote the number of assortments in thek training set where product j is not in the assortment and product k is chosen. The variable we dened above follows the following binomial distribution when 1 j N : 8 B¬πT , 1¬ù2k+1) 1 k < j8 B¬πT , 1¬ù2k+1) 1 k < j j >< ÙÄÄÄj >< = 0 k = j n B¬πT , 1¬ù2j) k = j , n :kk B¬πT , 1¬ù2k ) j < k ‚â§ N> = 0 j < k N + 1 >: B¬πT , 1¬ù2N ) k = N + 1: For example, a data point counts as nj when products 1 to k ÙÄÄÄ1 are not in the assortmentk while product k and j are in the assortment. So the probability is 1¬ù2k+1. Note that√çjj √çjÙÄÄÄ1 ÙÄÄÄj √çN +1 ÙÄÄÄjjwe have = nj and += T ÙÄÄÄn . Next we compute thek=1 nk k=1 nk k=j+1 nk Gini index Gj if the rst split is on product j. Recall the denition of the Gini index:√ç tj√çN =0 pÀÜjk ¬π1 ÙÄÄÄpÀÜjk ¬∫. If the split is on product j, then the left node (assortmentsRjTk without product j) has nj data points while the right node has T ÙÄÄÄnj . In the left node, the empirical frequency of label k, i.e., the fraction of assortments resulting in a purchase ÙÄÄÄjof product k, is n ¬ù¬πT ÙÄÄÄnj ¬∫. Similarly, in the right node, the empirical frequency isk jn ¬ùnj for label k ‚â§ j and 0 for label k > j. Therefore, we have thatk jj ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj nj √ïjn nT ÙÄÄÄnj √ïjÙÄÄÄ1 n nT ÙÄÄÄnj N√ï+1 nnkkkk kkGj = ¬π1 ÙÄÄÄ¬∫+ ¬π1 ÙÄÄÄ¬∫+ ¬π1 ÙÄÄÄ¬∫jjjj jjTnn T T ÙÄÄÄnT ÙÄÄÄnT T ÙÄÄÄnT ÙÄÄÄn k=1 k=1 k=j+1√çjj ¬∫2 √çjÙÄÄÄ1 ÙÄÄÄj ¬∫2 + √çN +1 ÙÄÄÄj ¬∫2 ! 1 j ‚àí k=1¬πnk j ‚àí k=1¬πnk k=j+1¬πnk = n + T ÙÄÄÄnjjTn T ÙÄÄÄn √çjj ¬∫2 √çj‚àí = 11¬πn ÙÄÄÄj¬∫2 + √çN =+ j 1 +1¬πn ÙÄÄÄj ¬∫2 k=1¬πnkkk kk = 1 ÙÄÄÄ‚àí (14)Tnj T ¬πT ÙÄÄÄnj ) Note that The second item of above equation only depends on assortments with product j, and the third item of above equation only depends on assortments without product j. Next we will show that G1 < Gj with high probability. We dene Hj for the quantity in 47  Electronic copy available at: https://ssrn.com/abstract=3430886 (14) for simplicity: √ç kj =1¬πnkj ¬∫2 √ç kj‚àí = 11¬πnk ÙÄÄÄj ¬∫2 + √ç kN =+ j 1 +1¬πnk ÙÄÄÄj ¬∫2 Hj , + :jjnT ÙÄÄÄn It‚Äôs easy to see that G1 < Gj if and only if H1 > Hj . To bound its probability, we introduce the Cherno inequality. The Cherno inequality: Let X1, X2, :::, Xn be independent Bernoulli random nvariables. Denote X = √ç with Œº = E¬ªX ¬º. For all 0 <  < 1, we have thati=1 Xi P¬πX < ¬π1 ÙÄÄÄ¬∫Œº¬∫< exp¬πÙÄÄÄŒº22 ¬∫. Applying the Cherno inequality to the Binomial random variable nj B¬πT , 1¬ù2¬∫jjfor 1 ‚â§ j ‚â§ N , we have P¬πn < ¬π1 ÙÄÄÄ¬∫T ¬ù2) = P¬πn > ¬π1 + ¬∫T ¬ù2) < exp¬πÙÄÄÄT2¬ù4¬∫. Therefore,  1 2TP nj ‚àí >< 2 exp ‚àí (15)2T 2T 4 Next we bound the probability that H1 is small compared to its mean. We can√çN+1 ÙÄÄÄ1¬∫2 rewrite H1 as H1 = n1 + k=2 ¬πn 1 k . Conditional on n1, nÙÄÄÄ1 ‚àº B¬πT ÙÄÄÄn1 , 1¬ù2) andT ÙÄÄÄn 2‚àö ÙÄÄÄ11n B¬πT ÙÄÄÄn , 1¬ù4¬∫. Dene 1 , / 1 ÙÄÄÄ. By the Cherno inequality we have: 3   1 ‚àö 1 ‚àö 12¬πT ÙÄÄÄn1¬∫ÙÄÄÄ11 ÙÄÄÄ11P¬πn2 ¬∫2 < 4¬π1 ‚àí 21¬∫2¬πT ÙÄÄÄn 1¬∫2 n = Pn < 2¬π1 ‚àí 21¬∫¬πT ÙÄÄÄn 1¬∫n < exp ÙÄÄÄ2 2   11¬∫21 12¬πT ÙÄÄÄn1¬∫ÙÄÄÄ11 ÙÄÄÄ11P¬πn3 ¬∫2 < n = Pn < 4¬π1 ÙÄÄÄ21¬∫¬πT ÙÄÄÄn 1¬∫n < exp ÙÄÄÄ316¬π1 ÙÄÄÄ21¬∫2¬πT ÙÄÄÄn 2 ÙÄÄÄ1Since ¬πn ¬∫2 0 for all k 4, we have: k !N√ï+1 1 ‚àö 1ÙÄÄÄ11P ¬πnk ¬∫2 < 4¬π1 ‚àí 21¬∫2 + ¬πT ÙÄÄÄn 1¬∫2 n16¬π1 ÙÄÄÄ21¬∫2 k=2 1 ‚àö 1ÙÄÄÄ1 ÙÄÄÄ11< P¬πn2 ¬∫2 + ¬πn3 ¬∫2 < 4¬π1 ‚àí 21¬∫2 + ¬πT ÙÄÄÄn 1¬∫2 n16¬π1 ÙÄÄÄ21¬∫2  1 ‚àö 1ÙÄÄÄ11 ÙÄÄÄ11< P¬πn2 ¬∫2 < 4¬π1 ‚àí 21¬∫2¬πT ÙÄÄÄn 1¬∫2 n + P¬πn3 ¬∫2 < 1¬∫2 n16¬π1 ÙÄÄÄ21¬∫2¬πT ÙÄÄÄn  12¬πT ÙÄÄÄn1¬∫< 2 exp ‚àí 2 48  Electronic copy available at: https://ssrn.com/abstract=3430886 Therefore,  1 ‚àö 111PH1 < n + 4¬π1 ‚àí 21¬∫2 + ¬πT ÙÄÄÄn 1¬∫n16¬π1 ÙÄÄÄ21¬∫2 !N√ï+1 1 ‚àö 1ÙÄÄÄ11 = P ¬πnk ¬∫2 < 4¬π1 ‚àí 21¬∫2 + ¬πT ÙÄÄÄn 1¬∫2 n16¬π1 ÙÄÄÄ21¬∫2 k=2 12¬πT ÙÄÄÄn1¬∫< 2 exp ‚àí (16)2 Combining with (15), we have that the unconditional probability can also be bounded:   ¬π1 ÙÄÄÄ¬∫T 1 ‚àö 1 ¬π1 + ¬∫TPH1 < + 4¬π1 ‚àí 21¬∫2 +2 16¬π1 ÙÄÄÄ21¬∫221  < P n 1 ‚àí >2T 2T  √ï ¬π1 ÙÄÄÄ¬∫T 1 ‚àö 1 ¬π1 + ¬∫T11+ P¬πn = k¬∫PH1 < + 4¬π1 ‚àí 21¬∫2 + n = k2 16¬π1 ÙÄÄÄ21¬∫22 k:jkÙÄÄÄT ¬ù2j<T ¬ù2  2T √ï 12¬πT ÙÄÄÄk¬∫1< 2 exp ‚àí + P¬πn = k¬∫2 exp ÙÄÄÄ42 k:jkÙÄÄÄT ¬ù2j<T ¬ù2  2T12¬π1 ÙÄÄÄ¬∫T2T < 2 exp ‚àí + 2 exp ‚àí = 4 exp ‚àí . (17)44 4 Next we will bound the probability that H2 is large compared to its mean. Recall that ¬πn21¬∫2 + ¬πn22¬∫2 ¬πn‚àí 12¬∫2 + √çN =+ 31¬πnÙÄÄÄ2¬∫2 kkH2 = 2 + 2 (18) nT ÙÄÄÄn 222 2For the rst item in (18), conditional on n2, both n1 and n = n2 ÙÄÄÄn1 follows B¬πn , 1¬ù2¬∫.2 Therefore, by the Cherno bound, we have !‚àö  22 212 222 1n P n1 ‚àí 2n > 2 1nn < 2 exp ‚àí 2 49  Electronic copy available at: https://ssrn.com/abstract=3430886 Moreover, we have 122 2P¬πn1¬∫2 + ¬πn 2 ÙÄÄÄn1¬∫2 > 2¬π1 + 212¬∫¬πn 2¬∫2 n 1 1122 = P 2¬πn 2¬∫2 + 2¬πn1 ‚àí 2n 2¬∫2 > 2¬π1 + 212¬∫¬πn 2¬∫2 n !‚àö  22 212 222 1n = P n1 ‚àí 2n > 2 1nn < 2 exp ‚àí . (19)2 where the rst equality above follows from ¬πn21¬∫2 + ¬πn2 ÙÄÄÄn21¬∫2 = 12 ¬πn2¬∫2 + 2¬πn21 ‚àí 12n2¬∫2. ÙÄÄÄ22For the second item in (18), conditional on n2, we have n B¬πT ÙÄÄÄn , 1¬ù2¬∫, T ÙÄÄÄ1 ÙÄÄÄ22 ÙÄÄÄ22 ÙÄÄÄ2 ÙÄÄÄ22n2 ÙÄÄÄn B¬πT ÙÄÄÄn , 1¬ù2¬∫, n B¬πT ÙÄÄÄn , 1¬ù4¬∫and T ÙÄÄÄn2 ÙÄÄÄn ÙÄÄÄn B¬πT ÙÄÄÄn , 1¬ù4¬∫.13 13 By the Cherno bound we have:  1 12¬πT ÙÄÄÄn2¬∫ÙÄÄÄ22Pn < 4¬π1 ÙÄÄÄ21¬∫¬πT ÙÄÄÄn 2¬∫n < exp ÙÄÄÄ3 2  1 12¬πT ÙÄÄÄn2¬∫ÙÄÄÄ2 ÙÄÄÄ22PT ÙÄÄÄn 2 ÙÄÄÄn ÙÄÄÄn < 4¬π1 ÙÄÄÄ21¬∫¬πT ÙÄÄÄn 2¬∫n < exp ÙÄÄÄ13 2 From the above two equations we have  1 12¬πT ÙÄÄÄn2¬∫ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22Pn3 ¬πT ÙÄÄÄn 2 ÙÄÄÄn ÙÄÄÄn3 ¬∫< 2¬∫2 n < 2 exp ÙÄÄÄ1 16¬π1 ÙÄÄÄ21¬∫2¬πT ÙÄÄÄn 2 Similar to (19) we also have  12¬∫2 12¬πT ÙÄÄÄn2¬∫ÙÄÄÄ2 ÙÄÄÄ22P¬πn1 ¬∫2 + ¬πT ÙÄÄÄn 2 ÙÄÄÄn1 ¬∫2 > 2¬π1 + 212¬∫¬πT ÙÄÄÄnn < 2 exp ‚àí 2 50  Electronic copy available at: https://ssrn.com/abstract=3430886 Combining the above two inequalities we have !N√ï+1 11ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22P¬πn1 ¬∫2 + ¬πn3 ¬∫2 + ¬πn ¬∫2 > 2¬π1 + 212¬∫‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬πT ÙÄÄÄn 2¬∫2 nk k=4 11ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22< P¬πn1 ¬∫2 + ¬πn3 ¬∫2 + ¬πT ÙÄÄÄn 2 ÙÄÄÄn ÙÄÄÄn3 ¬∫2 > 2¬π1 + 212¬∫‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬πT ÙÄÄÄn 2¬∫2 n1  11ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22 = P¬πn1 ¬∫2 + ¬πT ÙÄÄÄn 2 ÙÄÄÄn1 ¬∫2 ÙÄÄÄ2n3 ¬πT ÙÄÄÄn 2 ÙÄÄÄn ÙÄÄÄn3 ¬∫> 2¬π1 + 212¬∫‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬πT ÙÄÄÄn 2¬∫2 n1 1ÙÄÄÄ2 ÙÄÄÄ22< P¬πn1 ¬∫2 + ¬πT ÙÄÄÄn 2 ÙÄÄÄn1 ¬∫2 > 2¬π1 + 212¬∫¬πT ÙÄÄÄn 2¬∫2 n 1ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22+ Pn3 ¬πT ÙÄÄÄn 2 ÙÄÄÄn ÙÄÄÄn3 ¬∫< 2¬∫2 n1 16¬π1 ÙÄÄÄ21¬∫2¬πT ÙÄÄÄn    12¬πT ÙÄÄÄn2) 12¬πT ÙÄÄÄn2¬∫< 2 exp ‚àí + 2 exp ÙÄÄÄ22  12¬πT ÙÄÄÄn2¬∫= 4 exp ‚àí (20)2 The rst inequality follows from √ç kN =+ 41 nk ÙÄÄÄ2 = T ÙÄÄÄn2 ÙÄÄÄn1 ÙÄÄÄ2 ÙÄÄÄn‚àí 3 2 and thus √ç kN =+ 41¬πnk ÙÄÄÄ2¬∫2 ‚â§ ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ2¬πT ÙÄÄÄn2 ÙÄÄÄn ÙÄÄÄn3 ¬∫2. The rst equality follows from ¬πn3 ¬∫2 + ¬πT ÙÄÄÄn2 ÙÄÄÄn ÙÄÄÄn3 ¬∫2 =11 ¬πT ÙÄÄÄn2 ÙÄÄÄn‚àí 12¬∫2 ÙÄÄÄ2n‚àí 32¬πT ÙÄÄÄn2 ÙÄÄÄnÙÄÄÄ2 ÙÄÄÄn‚àí 32¬∫.1 Combine (19) and (20) we have 11 2PH2 > 2¬π1 + 212¬∫T ‚àí 8¬π1 ÙÄÄÄ21¬∫2¬πT ÙÄÄÄn 2¬∫n 22¬πn1¬∫2 + ¬πn2 ÙÄÄÄn1¬∫21 22< P > 2¬π1 + 212¬∫nn n2 ! ¬πn1 ÙÄÄÄ2¬∫2 + ¬πn‚àí 32¬∫2 + √ç kN =+ 41¬πnk ÙÄÄÄ2¬∫2 11  2+ P > 2¬π1 + 212¬∫‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬πT ÙÄÄÄn 2¬∫n2T ÙÄÄÄn 122 2 = P¬πn1¬∫2 + ¬πn 2 ÙÄÄÄn1¬∫2 > 2¬π1 + 212¬∫¬πn 2¬∫2 n !N√ï+1 11ÙÄÄÄ2 ÙÄÄÄ2 ÙÄÄÄ22+ P¬πn1 ¬∫2 + ¬πn3 ¬∫2 + ¬πnk ¬∫2 > 2¬π1 + 212¬∫‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬πT ÙÄÄÄn 2¬∫2 n k=4  22 22¬∫1n 1 ¬πT ÙÄÄÄn < 2 exp ‚àí + 4 exp ‚àí . (21)22 51  Electronic copy available at: https://ssrn.com/abstract=3430886 Next we bound the unconditional probability based on the conditional probability. We have 11PH2 > 2¬π1 + 212¬∫T ‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬π1 ÙÄÄÄ¬∫T 21  < P n 2 ‚àí >2T 2T √ï 11 + P¬πn 2 = k¬∫PH2 > 2¬π1 + 212¬∫T ‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬π1 ÙÄÄÄ¬∫Tn 2 = k2 k:jkÙÄÄÄT ¬ù2jT ¬ù2   2T √ï 12k12¬πT ÙÄÄÄk¬∫2< 2 exp ‚àí + P¬πn = k) 2 exp ‚àí + 4 exp ÙÄÄÄ4 22 k:jkÙÄÄÄT ¬ù2jT ¬ù2 √ï2T12¬π1 ÙÄÄÄ¬∫T22 exp ‚àí + P¬πn = k¬∫6 exp ÙÄÄÄ44 k:jkÙÄÄÄT ¬ù2jT ¬ù2  2T12¬π1 ÙÄÄÄ¬∫T2T = 2 exp ‚àí + 6 exp ‚àí = 8 exp ‚àí . (22)44 4 Next we choose a proper value for . By inequality (17) and (22), we want to nd  such that with high probability, we have p¬π1 ÙÄÄÄ¬∫T 11 ¬π1 + ¬∫T H1 ‚â• + 4¬π1 ‚àí 21¬∫2 +2 16¬π1 ÙÄÄÄ21¬∫22 11 > 2¬π1 + 212¬∫T ‚àí 8¬π1 ÙÄÄÄ21¬∫2 ¬π1 ÙÄÄÄ¬∫T H22 pwhere 1 = / 1 ÙÄÄÄ. We also have the constraint that 0 < 21 < 1, which is equivalent ‚àö to 0 <  < 17ÙÄÄÄ1 ‚âà 0:39. Solving the above inequality for 0 <  < 0:39 we have 2 0 <  0:166185. Let  = 0:166185. Then 4¬ù2 145. Plugging into (17) and (22), we have  TP¬πH1 < 0:512041T ¬∫< 4 exp ‚àí (23)145 TP¬πH2 > 0:512041T ¬∫< 8 exp ‚àí (24)145 52  Electronic copy available at: https://ssrn.com/abstract=3430886 Therefore TP¬πH1 < H2¬∫< 12 exp¬πÙÄÄÄ145¬∫. This implies that G1 < G2 with high probability. Note that the probability bound in above equation doesn‚Äôt depend on N . Next we consider j 3. Recall that √çjj √çjÙÄÄÄ1 ÙÄÄÄj √çN +1 ÙÄÄÄj k=1¬πnk ¬∫2 k=1¬πnk ¬∫2 + k=j+1¬πnk ¬∫2 Hj = j + j (25) nT ÙÄÄÄn Consider some 2 > 0. From (15) we have   21 22T P nj ‚àí >< 2 exp ‚àí (26)2T 2 T 4 ÙÄÄÄjjWe investigate the second item of (25). Conditional nj , we have n B¬πT ÙÄÄÄn , 1¬ù2¬∫,1 ÙÄÄÄjj ÙÄÄÄjj ÙÄÄÄj ÙÄÄÄjjT ÙÄÄÄnj ÙÄÄÄn B¬πT ÙÄÄÄn , 1¬ù2¬∫, n B¬πT ÙÄÄÄn , 1¬ù4¬∫and T ÙÄÄÄnj ÙÄÄÄn ÙÄÄÄn B¬πT ÙÄÄÄn , 1¬ù4¬∫.12 12pDene 3 , 2/ 1 ÙÄÄÄ2. Then similar to (20), we have jÙÄÄÄ1 N√ï+1 √ï¬© ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj 11 j ¬™ P¬≠¬πn1 ¬∫2 + ¬πn2 ¬∫2 + ¬πn ¬∫2 + ¬πn ¬∫2 > 2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2 ¬πT ÙÄÄÄnj ¬∫2 n ¬Ækk k=3 k=j+1¬´¬¨  ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj ÙÄÄÄj 11 j< P¬πn1 ¬∫2 + ¬πn2 ¬∫2 + ¬πT ÙÄÄÄnj ÙÄÄÄn ÙÄÄÄn2 ¬∫2 > 2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2 ¬πT ÙÄÄÄnj ¬∫2 n1  32¬πT ÙÄÄÄnj ¬∫< 4 exp ‚àí . (27)2 jj jThen similarly we can bound the rst term of (25) since n1 ‚àº B¬πn , 1¬ù2¬∫, nj ÙÄÄÄn1 ‚àº jjj jjjB¬πn , 1¬ù2¬∫, n2 B¬πn , 1¬ù4¬∫and nj ÙÄÄÄn1 ÙÄÄÄn2 B¬πT ÙÄÄÄn , 1¬ù4¬∫. j  !√ï j ¬∫21 1 j ¬∫2 jP ¬πn > 2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2 ¬πnnk k=1  jj jj 11 j< P¬πn1¬∫2 + ¬πn2¬∫2 + ¬πnj ÙÄÄÄn1 ÙÄÄÄn2¬∫2 > 2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2 ¬πnj¬∫2 n  2 j < 4 exp ‚àí 3n . (28)2 53  Electronic copy available at: https://ssrn.com/abstract=3430886 Combining (27) and (28), we have 11 jPHj > ¬ª2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2¬ºTn !j√ï j 11 j< P ¬πn ¬∫2 > ¬ª2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2¬º¬πnj ¬∫2 nk k=1 jÙÄÄÄ1√ï N√ï+1 + P¬©¬≠¬πn ÙÄÄÄj¬∫2 + ¬πn ÙÄÄÄj ¬∫2 > [ 12¬π1 + 232¬∫‚àí 18¬π1 ÙÄÄÄ23¬∫2¬º¬πT ÙÄÄÄnj¬∫2 nj ¬™¬Ækk k=1 k=j+1¬´¬¨   2 j2 j ¬∫3n 3 ¬πT ÙÄÄÄn < 4 exp ‚àí + 4 exp ‚àí (29)22 Using a similar argument, we can bound the unconditional probability:  11 22T PHj > ¬ª2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2¬ºT < 10 exp ‚àí . (30)4 Similarly we can calculate 2. By (30) and (23), we have the following condition for 2 11 2¬π1 + 232¬∫‚àí 8¬π1 ÙÄÄÄ23¬∫2 < 0:512041, (31) pwhere 3 = 2/ 1 ÙÄÄÄ2. Again when we consider 0 < 2 < 0:39, the above inequality is equivalent to 0 < 2 < 0:200261. Let 2 = 0:200261, then 4¬ù22 99:74, so we have for all j 3 ÙÄÄÄ TPHj > 0:512041T < 10 exp ‚àí (32)100 Now note that ÙÄÄÄ P¬πrst split not on product 1¬∫= PG1 > minfGj j2 ‚â§ j N g√ïÙÄÄÄ N ÙÄÄÄ = PH1 < maxfHj j2 ‚â§ j N } < P¬πH1 < 0:512041T ¬∫+ PHj > 0:512041T j=2  2T2T 2T < 4 exp ‚àí + 8 exp ‚àí + ¬πN ÙÄÄÄ2¬∫10 exp ‚àí 2 44 4  TT = 12 exp ‚àí + 10¬πN ÙÄÄÄ2¬∫exp ‚àí (33)145 100 54  Electronic copy available at: https://ssrn.com/abstract=3430886 Next we are going to show that the probability of the second split on product 2, third split on product 3 and so on, can be bounded by union bound. Let T denote the training set. Dene a sequence of subsets of T as follows: Ti , fS 2Tj1, 2, :::, i ÙÄÄÄ1 < Sg. That is, Ti only contains assortments that include a subset of fi, i + 1;:::, N g. Let Ti , jTi jdenote the cardinality of set Ti . Note that T1 = T and T1 = T . for 2 i N , since Ti B¬πT , 1¬ù2iÙÄÄÄ1¬∫, by the Cherno inequality we have  2 PTi < ¬π1 ÙÄÄÄ0¬∫2i 1 ÙÄÄÄ1T < exp¬π‚àí 0T ¬∫, (34)2i where 0 < 0 < 1. ¬ØDene the event Ai , fproduct i is the best split for training set Ti g, and let Ai denote the complement of event Ai . Then conditional onTi , we can bound the probability of event A ¬Ø i from (33) (here we only have N ÙÄÄÄi + 1 products):  ÙÄÄÄ ¬Ø Ti TiPAi Ti ¬∫< 12 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ‚àí (35)145 100 The unconditional probability can be bounded by combining (34) and (35):  √ï 1  PAi < PTi > ¬π1 ÙÄÄÄ0¬∫2iÙÄÄÄ1T + P¬πTi = k¬∫PAi jTi = k k:k¬π1ÙÄÄÄ0¬∫T ¬ù2iÙÄÄÄ1    2 √ï0T kk < exp¬π‚àí ¬∫+ P¬πTi = k) 12 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ÙÄÄÄ2i 145 100 k:k<¬π1ÙÄÄÄ0¬∫T ¬ù2iÙÄÄÄ1   02T ¬π1 ÙÄÄÄ0¬∫T ¬π1 ÙÄÄÄ0¬∫T ‚â§ exp¬π‚àí ¬∫+ 12 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ‚àí (36)2i 145 2iÙÄÄÄ1 100 2iÙÄÄÄ1 0T ¬π1ÙÄÄÄ0¬∫TSolving the equation  22 i = 1452iÙÄÄÄ1, we get 0 0:1107. Then we have  ÙÄÄÄ ¬Ø  TTPAi 13 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ÙÄÄÄ164 2iÙÄÄÄ1 113 2iÙÄÄÄ1 If all the events A1, A2, :::, Am happen, we can get right split for the rst m step. That is, the rst split is on product 1, the second split is on product 2, ..., the mth split is on 55  Electronic copy available at: https://ssrn.com/abstract=3430886 product m. We can bound the probability by the union bound: ÙÄÄÄ ÙÄÄÄ P\mi=1Ai = 1 ÙÄÄÄP[mi=1A ¬Ø i m√ï ¬Ø1 ‚àí P¬πAi ¬∫i=1   m√ï TT 1 ‚àí 13 exp ‚àí + 10¬πN ÙÄÄÄi ÙÄÄÄ1¬∫exp ‚àí (37)164 2iÙÄÄÄ1 113 2iÙÄÄÄ1 i=1 If the rst m splits match the products, then the assortments including at least one product among f1;:::, mgcan be correctly classied. Therefore, with probability at ÙÄÄÄleast P\mi=1Ai , we can accurately predict at least a fraction 1 ÙÄÄÄ1¬ù2m of assortments. Given  > 0, letting m = dlog2 1 ecompletes the proof.  B. SampleCode 56  Electronic copy available at: https://ssrn.com/abstract=3430886 